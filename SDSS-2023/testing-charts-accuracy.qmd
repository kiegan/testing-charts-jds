---
title: "Testing Charts: viewer's perceptual accuracy in surveys"
bibliography: "`r rbbt::bbt_write_bib('references.bib', overwrite = TRUE, library_id=rbbt::bbt_library_id('Graphics Research'))`"
abstract: "The use of visuals is a key component in scientific communication, and decisions about the design of a data visualization should be informed by what design elements best support the audience's ability to perceive and understand the components of the data visualization. We build on the foundations of Cleveland and McGill's work in graphical perception, employing a large, nationally-representative, probability-based panel of survey respondents to test perception in statistical charts. Our findings provide actionable guidance for data visualization practitioners to employ in their work."
keywords: 'list, keywords, here, 3-6, are, fine'
format: 
  pdf:
    include-in-header:
      - edits.tex
      - authors.tex
      # - refs.tex
    keep-tex: true
    # template: template.tex
documentclass: jds
---

<!-- Article Types -->
<!-- https://jds-online.org/journal/JDS/information/Article%20Types -->

<!-- Statistical Data Science -->

<!-- This section is the home base of the reformed journal covering statistical methods that are motivated by real-world applications. It is not for papers with technical proofs that push the frontiers of theoretical developments. In addition to classic topics in Statistics, cutting-edge works on big data, visualization, machine learning, and artificial intelligence are also welcome. -->

<!-- Computing in Data Science -->

<!-- Computing is an indispensable component of all data science and big data applications. As more journals on data science and big data emerge, it is of great interest for the data science community to have a highly regarded outlet with a specialization in computing, covering a wide spectrum from methods, algorithms, software implementations, to case studies. Existing journals on statistical computing have their own traditions and may not meet the increasing demands. Some research works on cutting-edge problems may not fit well in any existing journal. -->

<!-- This section covers the following types of articles. -->
<!-- 1. Software: Articles here are similar to those in the Journal of Statistical Software. They are not referencing manuals but vignettes that introduce the methods being implemented as well as the usage of the software with reproducible code chunks. The software implementation can be in any computer language with a sufficiently large user base. -->
<!-- 2. Algorithms: Articles here focus on the performance side of the computing needs arising from domain applications. For example, one can propose algorithms that make infeasible tasks feasible or speed up existing algorithms. -->
<!-- 3. Methods: Articles here are similar to those in Journal of Computational and Graphic Statistics or Statistics and Computing. The computing methods need to be motivated by a domain application with the properties carefully studied. -->

@clevelandGraphicalPerceptionTheory1984

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = F, 
                      eval = T, 
                      fig.path="./figures/", # figures are created on the fly, images are pictures like the stimuli
                      fig.env="figure*", 
                      fig.align = "center",
                      out.width = "50%",
                      dpi = 72, # we can set the resolution up to 300 later.
                      cache = TRUE) 
library(readr)
library(tidyverse)
library(knitr)
library(bibtex)
theme_set(theme_bw())

library(readxl)
library(lubridate)
library(patchwork)
```


```{r showplot, warning = F, message = F}
#| fig-cap: "quarto figure caption."
#| out-width: \columnwidth
#| label: fig-label-we-decide-on
#| fig-res: 300
#| fig-width: 5 # set only if different from the default
#| fig-height: 5
#| fig-env: figure
#| 

data.frame(x = runif(100), y = runif(100)) %>%
  ggplot(aes(x = x, y = y)) +
  geom_point()
```

\hh{This is Heike's color for making changes}
## Introduction

How do structural design choices in a data visualization impact viewers’ ability to identify the larger of two elements?  

How do aesthetic design choices in a data visualization impact viewers’ ability to identify the larger of two elements?  

How is viewer behavior (zooming, time spent on question, certainty of response) impacted by structural and aesthetic design choices in a data visualization? 


Structural design choices: 

Structural design choices 

Mapping, Stacked bar, Vertical, Horizontal, Horizontal wide 

Facetted bar 

Only have a split sample for this  

Pie, Alignment  

We have this for all above mappings, but the setup is a little different for facetted bar 


Aesthetic design choices (structural choices seems stronger/there could be a lot to talk about there… should we skip aesthetic on this one?) 

Colors 

Use of gridlines 

Outcomes/responses for modeling:  

Binary accuracy (correct/incorrect – ‘they are the same’ is incorrect here) 

Ordinal response (a/b/they are the same) 

Zooming behavior (zoomed/did not zoom) 

Time spent on question (continuous, in seconds) 


Certainty  

I’ve noted this below, but: how to model? Ordinal response? Binary (certain or very certain vs everybody else)? 

## Survey setup - Stimulus description

Asking perceptual questions in a survey is different from the controlled environment of a cognitive lab, where these kind of questions would usually be addressed. This means, that instructions to participants have to be delivered in a very short and easily understandable, because questions arising from the task can not be answered. Similarly, rather than asking the same (or similar) type of question multiple hundred times, in a survey we can ask only a few questions. In order to observe any effect, we need to ask questions that are perceptually hard, which means that we need to ask questions about stimuli that are close to our perceptual threshold. The **Just-Noticeable Difference** (JND) is defined as the smallest difference that will be detected 50% of the time. For the height difference of barcharts, @luModelingJustNoticeable2022 determined that the distance between bars is the main contributor to differentiating sizes. Based on their models, a tile difference of 155 pixels leads to a just noticeable difference of 3.5 pixels. \autoref{tbl-tasks-12} shows the two stacked barcharts shown to participants in the survey. The marked tiles in each plot are 155 pixels apart. Their heights are 205 (left) and 213 pixels (right), respectively, corresponding to about twice the JND. This difference should lead to a relatively high accuracy rate for participants and simultaneously limit the amount of frustration resulting from a task that is perceived as 'too hard'. 

<!-- log_{10}(JND) = -.4653 + .0065 distance -->

Both charts in \autoref{tbl-tasks-12} show the same data with slight modifications to the order of the levels – the first and second level in each of the bars are reversed between the left and the right chart. Participants were asked to compare the relative sizes of the tiles marked A and B (C and D, respectively) and select the correct response out of the possible choices:

1. A is bigger
2. B is bigger
3. They are the same

Answer 2 is the correct answer for both charts. Both charts are shown at the same size, i.e. in both cases the difference in size between the bars is exactly the same, the vertical distance between the bars is the exact same amount.
This leaves the vertical positioning of the bars as the only difference between the charts. Any differences in observed accuracy can therefore be attributed to this difference in presentation.

|                                   |                                   |
|-----------------------------------|-----------------------------------|
| **Unaligned positions**           | **Aligned positions**             |
| ![Unaligned positions marked A and B](../SDSS-2023/images/vertical-ab.PNG) | ![Aligned positions marked C and D](../SDSS-2023/images/vertical-cd.PNG) |

: The two stacked barcharts every participant got to see. In each barchart, the two marked tiles are to be compared for their size. in both instances, the tile on the right is (very slightly) larger. {#tbl-tasks-12 tbl-colwidths="\[50,50\]"}


DATA THAT MAY BE INCLUDED IN ANALYSIS:  

ROUNDS 1-2: Color variations on vertical stacked bar, aligned and unaligned 

ROUND 3: Horizontal and horizontal wide, aligned and unaligned 

ROUND 5: Horizontal wide gridlines (only dark grid split sample) 

ROUND 6: Facetted bar (split sample w/o forcing choice) 

ROUND 7: Aligned vs unaligned pie (full sample) 


## Data Analysis and Results


COMBINING SAMPLES AND WEIGHTING NOTES:  

\hh{We can combine responses across samples into one combined dataset, but we need to adjust weights accordingly so that each sample is weighted equally in the model [@omuircheartaighCombiningSamplesVs2002].}

Question for Ed: If we compare a full sample to a split sample, do we still want to weight these ‘equally’? 

Analysis should be done using the ‘survey’ package and weights should be taken into account. 


ANALYSIS PLAN:  

Models below structured as: 

Response 

Covariates to use in each model 

STRUCTURAL VARIATION – START HERE 

Binary accuracy across structural choices 

Model 1: 

Alignment only, just vertical stacked bar 

\hh{Using the AmeriSpeak survey tool, a total of 1902 participants were exposed to two barcharts each, as shown in \autoref{tbl-tasks-12}.}



Model 2:  

Alignment 

Bar vs pie (comparable question for pie is A vs B) 

Model 3:  

Alignment 

Vertical x horizontal x horizontal wide 

Model 4: 

Alignment 

Every structure (vertical bar, horizontal bar, horizontal wide bar, facet bar, pie) 

Visuals:  

% yes across each different structural condition 

Facet by aligned/unaligned? 

Model estimates + CIs 

Ordinal response 

Model 1: 

Alignment only, just vertical stacked bar 

Model 2:  

Alignment 

Bar vs pie (comparable question for pie is A vs B) 

Model 3:  

Alignment 

Vertical x horizontal x horizontal wide 

Model 4: 

Alignment 

Every structure (vertical bar, horizontal bar, horizontal wide bar, facet bar, pie) 

Visuals:  

All responses across each different structural condition 

Facet by aligned/unaligned? 

Model estimates + CIs 

Zooming behavior (zoomed/did not zoom) 

Model 1:  

Device type 

Alignment 

Vertical x horizontal x horizontal wide 

Visuals:  

% zoomed by device + alignment (already have this chart) 

Model estimates + CIs 

Time spent on question (in seconds) 

Model 1:  

Device type 

Zoom 

Alignment 

Vertical x horizontal x horizontal wide 

Model 2 (this may not be feasible for comparison depending on what level the ‘TOTALTIME’ is captured at):  

Device type 

Zoom 

Alignment 

Every structure (vertical bar, horizontal bar, horizontal wide bar, facet bar, pie) 

Visuals:  

Distribution of time spent variable 

Facet by device type, zoom, structural condition, alignment? Play around with it  

Average time spent by each of the conditions 

Certainty?  

Same models as above, but I’m not sure how we want to do the response. Ordinal response? Binary (certain or very certain vs everybody else)? 

AESTHETIC VARIATION – ONLY IF TIME 

Binary accuracy (correct/incorrect – ‘they are the same’ is incorrect here) across structural choices 

Model 1: 

Dark grid vs no grid (only have for horizontal wide) 

Response choice (ordinal response) 

Model 1: 

Dark grid vs no grid (only have for horizontal wide) 

Zooming behavior (zoomed/did not zoom) 

Model 1:  

Device type 

Dark grid vs no grid 

Time spent on question (in seconds) 

Model 1:  

Device type 

Zoom 

Dark grid vs no grid 

Certainty?  

Same models as above, but I’m not sure how we want to do the response. Ordinal response? Binary (certain or very certain vs everybody else)? 

 

# Conclusion



# Supplementary Material {-}

+ **Participant Data (Linear):** Link to csv file with the data. 
+ **Data Analysis Code:** Link to an html document with annotated code chunks.

# References