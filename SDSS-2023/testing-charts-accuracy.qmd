---
title: "Testing Charts: viewers' perceptual accuracy in surveys"
short-title: Testing charts in surveys # running title
bibliography: references.bib
abstract: "The use of visuals is a key component in scientific communication, and decisions about the design of a data visualization should be informed by what design elements best support the audience's ability to perceive and understand the components of the data visualization. We build on the foundations of Cleveland and McGill's work in graphical perception, employing a large, nationally-representative, probability-based panel of survey respondents to test perception in statistical charts. Our findings provide actionable guidance for data visualization practitioners to employ in their work."
keywords: 'list, keywords, here, 3-6, are, fine'
preamble: >
  \usepackage{amsfonts,amsmath,amssymb,amsthm}
  \usepackage{booktabs}
  \usepackage{graphicx}
  \usepackage{hyperref}
  \usepackage{setspace}
  \usepackage{lscape}
documentclass: jds
format:
  pdf: 
    includes:
      in_header: 'gt_packages.sty'
    include-in-header: 
      text: |
        \usepackage{lscape}
        \author[1]{Kiegan Rice\thanks{Corresponding author. Email: rice-kiegan@norc.org}}
        \author[2]{Heike Hofmann\footnote{Email: hofmann@iastate.edu}}
        \author[1]{Nola du Toit\footnote{Email: dutoit-nola@norc.org}} 
        \author[1]{Edward Mulrow\footnote{Email: mulrow-edward@norc.org}}
        \affil[1]{NORC at the University of Chicago}
        \affil[2]{Department of Statistics, Iowa State University}
    keep-tex: true
  html: default
editor: 
  markdown: 
    wrap: sentence
---

```{=latex}
  \newcommand{\hh}[1]{{\textcolor{orange}{#1}}} 
  \newcommand{\kr}[1]{{\textcolor{teal}{#1}}}.  

% take out everything above later
% keep everything below

  \newcommand{\blandscape}{\begin{landscape}} 
  \newcommand{\elandscape}{\end{landscape}}

  \setlength{\parindent}{0pt}
  \singlespacing
```
<!-- fig-width: 6.5 -->

<!-- fig-height: 4.5 -->

<!-- template: template-jdsart.cls.tex -->

```{r run-to-update-bibfile, eval=FALSE, include=FALSE}
keys <- rbbt::bbt_detect_citations("testing-charts-accuracy.qmd")
citations_only <- grep(pattern="[fig|tbl|eq][-|:].*", keys, value=TRUE, invert=TRUE)
rbbt::bbt_write_bib(keys=citations_only, 'references.bib', overwrite = TRUE,
                    library_id=rbbt::bbt_library_id('Graphics Research'),
                    translator='bibtex')
```

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = F, 
                      eval = T, 
                      fig.path="./figures/", # figures are created on the fly, images are pictures like the stimuli
                      fig.env="figure*", 
                      fig.pos = "hbt", # preference of figure placement: here, bottom, top 
                 #     fig.align = "center",
                      out.width = "\\textwidth",
                #      fig.height = 5,
                 #     fig.width = 5,
                      dev = "png", dev.args = list(type = "cairo-png")
#                      dpi = 72#, # we can set the resolution up to 300 later.
                      #cache = TRUE) 
)



library(readr)
library(tidyverse)
library(knitr)
library(bibtex)
theme_set(theme_bw())

library(readxl)
library(lubridate)
library(patchwork)
library(survey)
#library(ggmosaic)
library(ggpcp)
library(broom)
library(latex2exp)
library(gt) # grammar of tables
library(here)
library(multcomp)

# Kish's Effective Sample Size
neff <- function(weight) {
  n <- length(weight)
  L <- var(weight)/mean(weight)^2
  n/(1+L)
}

# inverse of the logistic function
inv_logit <- function(x) {exp(x)/(1+ exp(x))}

# calculate lambda for combining weights
calculate_lambda <- function(weights1, weights2) {
  d1 <- 1+var(weights1)/mean(weights1)^2
  d2 <- 1+var(weights2)/mean(weights2)^2

  n1 <- length(weights1)
  n2 <- length(weights2)

  denom <- n1/d1 + n2/d2

  lambda <- n1/(d1*denom) 
  lambda
}

vs_repo_local <- here::here("data") # link to data folder
```

```{r data}
# other github repo. 
round1 <- readRDS(file.path(vs_repo_local, "round1.rds"))

round1_adj <- round1 %>% 
            filter(ab != "SKIPPED ON WEB", cd != "SKIPPED ON WEB") %>%
            mutate(CorrectAB = ab=="B is bigger", CorrectCD = cd=="D is bigger",
                   CorrectABCD = paste(CorrectAB, CorrectCD),
                   one = 1)

round2 <- readRDS(file.path(vs_repo_local, "round2.rds"))

round2_adj <- round2 %>% 
            filter(ab != "SKIPPED ON WEB", cd != "SKIPPED ON WEB") %>%
            mutate(CorrectAB = ab=="B is bigger", CorrectCD = cd=="D is bigger",
                   CorrectABCD = paste(CorrectAB, CorrectCD),
                   one = 1)

round3 <- readRDS(file.path(vs_repo_local, "round3.rds"))

# filter out all of the 'skipped on web':
round3_adj <- round3 %>% mutate(
  weight = WEIGHT_NOLA
) %>% filter(
  ab != "SKIPPED ON WEB",
  cd != "SKIPPED ON WEB"
)

round5 <- readRDS(file.path(vs_repo_local, "round5.rds"))

# floating bars (unaligned) are shown to all participants of round 5
round5_adj <- round5 %>% mutate(
  ab = ab1,
  ab_zoom = ab1_zoom,
  ab_certain = ab1_certain,
  weight = WEIGHT_NOLA
) %>% filter(
  ab != "SKIPPED ON WEB"
)


round6 <- readRDS(file.path(vs_repo_local, "round6.rds"))

round6_adj <- round6 %>% mutate(
  weight = WEIGHT_NOLA
) %>% filter(RND_01 == 0) # same type of questions as round 7

round7 <- readRDS(file.path(vs_repo_local, "round7.rds"))

# filter out all of the 'skipped on web':
round7_adj <- round7 %>% mutate(
  weight = WEIGHT_NOLA
)
```


```{r combine-123}
lambda_1_2 <- calculate_lambda(round1_adj$weight, round2_adj$weight)

round_12 <- round1_adj %>% 
            mutate(weight = lambda_1_2*weight/sum(weight),
                   round = "Round 1") %>% 
            dplyr::select(CaseId, ab, cd, CorrectAB, CorrectCD, CorrectABCD,
                   ab_certain, cd_certain, ab_zoom, cd_zoom, round, weight,
                   age4, gender, income4, racethnicity, educ5) %>%
    rbind(round2_adj %>% 
            mutate(weight = (1-lambda_1_2)*weight/sum(weight),
                   round = "Round 2") %>% 
            dplyr::select(CaseId, ab, cd, CorrectAB, CorrectCD, CorrectABCD, 
                   ab_certain, cd_certain, ab_zoom, cd_zoom, round, weight,
                   age4, gender, income4, racethnicity, educ5))


lambda_12_3 <- calculate_lambda(round_12$weight, round3_adj$weight)

lambdas <- c(lambda_1_2, 1-lambda_1_2)*lambda_12_3
lambdas <- c(lambdas, 1-sum(lambdas))

round_123 <- round_12 %>% 
              mutate(weight = lambda_12_3*weight/sum(weight),
                     chart = "Vertical") %>% 
  rbind(
    round3_adj %>% 
      mutate(
        weight = (1-lambda_12_3)*weight/sum(weight),
        round = "Round 3",
        CorrectAB = ab == "B is bigger",
        CorrectCD = cd == "D is bigger",
        CorrectABCD = paste(CorrectAB, CorrectCD),
        chart = ifelse(P_EXP==1, "Horizontal", "Horizontal wide")
       ) %>% 
        dplyr::select(CaseId, ab, cd, CorrectAB, CorrectCD, CorrectABCD,
                   ab_certain, cd_certain, ab_zoom, cd_zoom, round, chart, weight,
                   age4, gender, income4, racethnicity, educ5) 
  ) 

lambda_1235 <- calculate_lambda(round_123$weight, round5_adj$weight)
round_123b <- round_123 %>% 
              mutate(weight = lambda_1235*weight/sum(weight)) %>% 
  rbind(
    round5_adj %>% 
      mutate(
        weight = (1-lambda_1235)*weight/sum(weight),
        round = "Round 5",
        cd = NA,
        cd_certain=NA,
        cd_zoom=NA,
        CorrectAB = ab == "B is bigger",
        CorrectCD = NA,
        CorrectABCD = NA,
        chart = "Floating bars"
       ) %>% 
        dplyr::select(CaseId, ab, cd, CorrectAB, CorrectCD, CorrectABCD,
                   ab_certain, cd_certain, ab_zoom, cd_zoom, round, chart, weight,
                   age4, gender, income4, racethnicity, educ5) 
  ) 

levels(round_123$educ5) <- c("Less than HS", "HS or equivalent", "Some college", "Bachelor", "Post graduate")

levels(round_123$income4) <- c("< $30k", "$30k - $60k", "$60k - $100k", "more than $100k")
  
round_123$weight <- neff(round_123$weight)/sum(round_123$weight)*round_123$weight 
# does not affect the further analysis, but helps with the interpretation of results

round_123_long <- round_123 %>% 
  pivot_longer(ab:cd, names_to = "task", values_to = "Response") 

round_123_long <- round_123_long %>% 
  mutate(
    correct = Response %in% c("B is bigger", "D is bigger"),
    correct3 = correct | (Response == "They are the same"),
    task_chart = factor(paste(task, chart, sep="-")),
    zoom = ifelse(task=="ab", ab_zoom, cd_zoom),
    response3 = ifelse(correct, "Correct", ifelse(correct3, "They are the same", "Wrong"))
  )

survey_123 <- 
  survey::svydesign(
  data = round_123_long, 
  ids = ~CaseId, weights = ~weight)

round_123b_long <- round_123b %>% 
  pivot_longer(ab:cd, names_to = "task", values_to = "Response") %>%
  mutate(
    correct = Response %in% c("B is bigger", "D is bigger"),
    correct3 = correct | (Response == "They are the same"),
    task_chart = factor(paste(task, chart, sep="-")),
    zoom = ifelse(task=="ab", ab_zoom, cd_zoom),
    response3 = ifelse(correct, "Correct", ifelse(correct3, "They are the same", "Wrong"))
  ) %>% filter(
    !is.na(Response)
  )

survey_123b <- 
  survey::svydesign(
  data = round_123b_long, 
  ids = ~CaseId, weights = ~weight)

```


## Introduction

<!-- What is our main objective? -->

\kr{Should the abstract match -- or be close to -- our SDSS short abstract?}
\hh{I don't think it has to}


What do viewers see when we show them a data chart?
A data chart -- at its core -- maps quantitative values to graphical elements representing their relative values.
Modern data visualizations are much more than a simple, objective mapping of values to a plane; they contain contextual and design elements, and are often structured to support the viewer in understanding a particular view of a set of data or specific pattern underlying the values.
The design of a data visualization impacts a viewer's ability to achieve that understanding; a poorly designed data visualization may leave viewers struggling to understand the content or context, or make it difficult to complete accurate and useful comparison of values across groups or time points.
More broadly, the design of a data visualization can change how viewers interact with the chart.

A crucial step in the process of interacting with and understanding a chart is the viewer's employment of comparisons of the parts within.
@clevelandGraphicalPerceptionTheory1984 observed as such, and in their seminal study defined the better visual among a pair as the one that allows viewers to make more accurate comparisons.
Based on mappings of quantitative variables to different graphical elements, Cleveland and McGill's study resulted in a ranking of perceptual tasks from most accurate to least accurate, which was then extended by @mackinlayAutomatingDesignGraphical1986 to a theoretical framework ranking tasks' order along their ordinal and nominal scales.

Cleveland and McGill's work -- while a foundational user study in graphical perception -- utilized a small convenience sample, consisting of only a few individuals recruited from among the authors' coworkers and their spouses.
@heerCrowdsourcingGraphicalPerception2010 reproduced Cleveland and McGill's rankings using a larger sample from a crowd sourcing platform, employing a total of XXX Amazon Mechanical Turk (mTurk) workers for the study.
Crowd sourced samples were shown by @borgoCrowdsourcingInformationVisualization2017 to be biased towards more male, younger, and relatively higher education relative to the adult U.S. population as a whole; generally, convenience and opt-in samples are not representative of the general population, a common target audience for data visualization and scientific communication work.
Further, the populations' emphasis on higher education individuals also leads to results which hold for groups of individuals who may be more likely to have prior exposure to data visualization in the context of scientific communication, or more exposure to data topics in higher education, but may not hold across other groups within the population.
<!-- Bias from crowd-sourcing? sample size?  -->

\kr{The mackinlay thing feels like maybe we are overemphasizing the ranking done by cleveland/mcgill and then mackinlay - I think we could emphasize less if we want}
\hh{my point was to emphasize that Mackinlay is a foundational paper (rightfully so), but not based on any data beyond Cleveland}

<!--Our work -- as that of Cleveland and McGill and Heer and Bostock -- -->

Our work also centers on studying data visualization design choices and their impact on viewer behavior and accuracy of viewers' responses.
We seek to answer whether it is possible to reproduce some of their findings in the context of a survey with a large, nationally-representative set of respondents, and within that context we focus on the following research questions:

1.  How do structural design choices in a data visualization impact viewers' ability to identify the larger of two elements?

2.  How is viewer interaction with the task impacted by structural design choices in a data visualization?

3.  Are there differences in perception and interaction with the tasks across demographic groups?  

We employ a probability-based survey panel and run a series of perception tests with nationally-representative samples of respondents from that panel.
The advantage of using a probability-based approach is two-fold.
First, we have access to a large sample of survey participants and thus have greater power in making inference about graphical perceptional abilities.
Second, the sample is representative of the general adult public in the U.S., and this allows us to test whether prior results from convenience samples hold with a nationally representative sample and whether there are differences in those results across demographic subgroups.

We present viewers with structural variations of bar charts and ask them to answer questions comparing the size of elements within those charts.
In this work, we present the series of tests we completed and the resulting findings.
The remainder of the paper is organized as follows: first, we describe the design of the visual stimuli used in our perception tests.
We then describe the population of study respondents and obtained survey sample.
Subsequently, we share our analyses of the resulting survey responses across each of our tests, including analyses on accuracy of responses and response behavior.
Finally, we discuss implications of this work and next steps.

<!-- # ```{r} -->
<!-- # #| label: fig-perceptual-tasks -->
<!-- # #| fig-cap: Ranking of perceptual tasks, as given by @mackinlayAutomatingDesignGraphical1986. The ranking of tasks on the quantitative scale are empirically verified by @clevelandGraphicalPerceptionTheory1984. -->
<!-- # knitr::include_graphics("images/mackinlay-tasks.png") -->
<!-- # ``` -->

<!-- \kr{The figure below is still a bit of an orphan, but I think we can revisit the intro ad study setup after we get more of the results in the paper} -->

<!-- ```{r} -->
<!-- #| label: fig-aligned-unaligned -->
<!-- #| fig-cap: "The only difference between the two pairs of rectangles A, B and C, D is their alignment, i.e. A and C are of identical size, as are B and D. When participants are asked to compare the size of these tiles in barcharts, the predominant response for the unaligned pair on the left is 'they are of the same size'. In contrast, more than half of the viewers respond with 'D is bigger' to the aligned pair of bars on the right." -->
<!-- #| fig-subcap: -->
<!-- #| - "**Unaligned bars**" -->
<!-- #| - "**Aligned bars**" -->
<!-- #| layout-nrow: 1 -->
<!-- #| fig-align: "center" -->
<!-- #| out-width: 50% -->
<!-- knitr::include_graphics("images/round5-modified.PNG") -->
<!-- knitr::include_graphics("images/round3-modified.PNG") -->
<!-- ``` -->

## Study design -- stimulus

Each task is made up of two elements: a visual stimulus and a question about that stimulus that the viewer is asked to respond to.
In our study, each visual stimulus is an image of a data visualization, while each question prompts viewers to identify which of two marked pieces in the data visualization is larger.

The comparison between sets of marked pairs is designed to be a difficult task, with the difference between the values represented in the two marked pieces being a just-noticeable difference.
The **Just-Noticeable Difference** (JND) is defined as the smallest difference that will be detected 50% of the time.
Prior results from studies on bar charts and pie charts [@luModelingJustNoticeable2022] inform the differences in charts shown to our survey panelists.

We employ comparisons at the JND in our tasks in order to maximize our ability to identify the impact of design changes on viewer accuracy and behavior.
Asking perception tasks in a survey differs from the controlled environment of a cognitive lab, where these kind of questions may usually be assessed.
Rather than asking the same (or similar) type of question with varied signal strength dozens or hundreds of times, we are limited to only a few questions at a time.
With a small set of tasks, we need to present tasks that are perceptually hard, and thus ask questions about stimuli that are close to our perceptual threshold.
Therefore, we focus on questions which vary the structure of the presented image, but ask viewers to compare the same underlying data values across those varied images.

We ask participants to determine which of two just-noticeably different marked pieces is larger within each data visualization image, and focus on three main sets of structural variation in the design of that data visualization image.
First, we vary the alignment of the pieces in question.
Viewers are presented with two marked pieces in a chart that do not share a common baseline, then two pieces that do share a common baseline.
Second, we vary the orientation of the chart -- we rotate the vertical stacked bar chart, and present a horizontally oriented version of the same chart, with identically sized marked pieces.
Finally, we change the aspect ratio of the chart and present a wider version of the horizontally oriented chart which has longer, but thinner, marked pieces.

```{=tex}
\hh{Not sure where should put this, parking it here for right now:}
\hh{expectations - start}
```
What we call 'aligned' and 'unaligned', here, is similar to Cleveland and McGill's set of rankings, but with some modifications: both 'aligned' and 'unaligned' bars share the same axis.
Aligned tiles are additionally anchored in the same position in one dimension, i.e. the difference between their sizes can be reduced to a positional assessment.
Unaligned tiles do not share this anchor, however, the context of the other tiles in the chart provide a frame, which *should* help with an assessment of the tiles' sizes beyond a comparison of (arc) lengths or areas.

We would expect that comparing unaligned tiles is a harder task (with correspondingly lower levels of accuracy) than a comparison of aligned tiles, with the framing given by the context of the other tiles in the same column mitigating some of this difficulty.
@fig-tasks-explain gives an overview of the comparisons of tasks 1 through 3 and the closest corresponding tasks in Cleveland and McGill.
\kr{The updated sketch is great here!! One note -- can we move the label for the "positions along the same, but unaligned scale", it's not aligned with the others}
\hh{that was on purpose :(  that goes beyond Cleveland and McGill ... I'll see what I can do about that}


```{=tex}
\hh{I think we should include the floating bars here to demonstrate that they are worse than the wide horizontal bars. }
```

```{r}
#| label: fig-tasks-explain
#| fig-cap: Comparisons made in charts within the Cleveland and McGill ranking 
#| out-width: 60%

include_graphics("images/sketch-tasks.png")
```

\hh{expectations - end}

Our use of a survey format guides the format and design of the questions asked and how they are presented to respondents.
First, participant instructions must be delivered in a very short and easily understandable format, because participants cannot ask clarifying questions about the task as they might be able to in a cognitive lab setting.
Second, we want to utilize content within the data visualization image which is not socially or politically charged for the average U.S. adult; this risks participants reacting to the subject matter within the chart rather than focusing on the presented task.
For this reason, we utilize data on living arrangements of older U.S. adults -- a topic which most U.S. adults will have some familiarity with, but is not inherently polarizing.
Finally, to prevent viewers from being exposed to slight variations of the same stimulus in a row (and risk unforeseen order effects or respondents using prior questions to inform their responses), we either split a survey sample in two and show each subsample a distinct structural version of the chart or test variations of a chart across distinct rounds of the survey. 

#### Question text  
When viewing each chart, participants were asked to compare the relative sizes of marked elements within the chart:

"There are many charts used in the news media to portray data visually. Looking at the chart below, which of the marked dark blue pieces is bigger, A or B? Just your best guess is fine.

-   A is bigger

-   B is bigger

-   They are the same

When presented with the aligned version of each chart, pieces were marked with a C and D and the question text is updated accordingly.
In all scenarios, the second option (B \[D\] is bigger) is the correct response.  

For a given task, viewers are first presented with the unaligned version of the task, followed by the aligned version of the task. The time in seconds that each respondent spent on each task was recorded, as well as whether the participant zoomed in on each chart while answering the question.
Respondents were also asked to rate their certainty in their response to each question on a five-point scale.

<!--
```{r eval=FALSE}
#| label: fig-tasks-old
#| fig-cap: "Each of the visual stimuli presented to viewers. In each bar chart, two pieces are marked. The larger piece in each chart are pieces B and D. "
#| fig-subcap:
#| - "Vertical, unaligned"
#| - "Horizontal, unaligned"
#| - "Wide horizontal, unaligned"
#| - "Vertical, aligned"
#| - "Horizontal, aligned"
#| - "Wide horizontal, aligned"
#| layout-nrow: 3
#| out-width: 90%
cat("Task 1")
cat("Task 2")
cat("Task 3")
knitr::include_graphics("images/vertical-ab.PNG")
knitr::include_graphics("images/round3/horizontal-ab.PNG")
knitr::include_graphics("images/round3/hwide-ab.PNG")
knitr::include_graphics("images/vertical-cd.PNG")
knitr::include_graphics("images/round3/horizontal-cd.PNG")
knitr::include_graphics("images/round3/hwide-cd.PNG")
```
-->



```{r}
#| label: fig-tasks
#| fig-cap: "Each of the visual stimuli presented to viewers. In each bar chart, two pieces are marked. The larger piece in each chart are pieces B and D. "
#| out-width: 90%
knitr::include_graphics("images/stimuli-overview.png")
```

### Task 1: Vertical stacked bar

\autoref{fig-tasks}(a,d) shows the two stacked bar charts shown to participants in the first task.
The marked tiles in each plot are 155 pixels apart, which leads to a JND of 3.5 pixels based on @luModelingJustNoticeable2022's model.
The heights of the bars are 205 (left) and 213 pixels (right), corresponding to about twice the JND.
This difference should lead to a relatively high accuracy rate for participants and simultaneously limit the amount of frustration resulting from a task that is perceived as 'too hard'.

<!-- Lu et al calculation: log_{10}(JND) = -.4653 + .0065 distance -->

<!-- describe stimulus -->

Both charts show the same data with slight modifications to the order of the levels -- the first and second level in each of the bars are reversed between the left and the right chart.
Both charts are displayed at the same size, i.e. in both cases both the difference in size between the bars and the horizontal distance between the bars is the exact same amount.
This leaves the vertical positioning of the bars as the only difference between the charts.
Any differences in observed responses can therefore be attributed to this difference in presentation.

Task 1 was presented to viewers in two rounds (Rounds 1-2), with four color scheme variations each presented to 50% of respondents in each round. 
There were no significant differences in respondent accuracy or behavior across color schemes, and we combine them here into one set of stimulus responses to focus on the comparison of structural differences.
\kr{Should we put the color variations and a test for significance in accuracy/a visual in the appendix/supplementary materials? } \hh{we could do that, but I'm not sure that we need to mention color at all.}


### Task 2: Horizontal stacked bar

The visual mappings in the second task, shown in \autoref{fig-tasks}(b, e), are identical to the first task, but the axes of the chart are rotated so that the stacked bars are represented in a horizontal format.
This represents a structural change in how the data are presented to the viewer while preserving the pixel size of the elements viewers must compare.
Tasks 2 and 3 were asked within the same survey round (Round 3) and the full sample was randomly split among respondents, with 50% of participants seeing the Task 2 stimuli and 50% of participants seeing the Task 3 stimuli.

### Task 3: Horizontal wide stacked bar

The images utilized in the third task again represent the same data as in the first two tasks, and the overall image has dimensions which are identical to the images in the first two tasks, displaying at the same size during the survey.
\hh{The length of bars is increased to fit the new dimensions. This increases the difference in the length of bars from previously 8 pixels to 13 pixels. The widths of the tiles are adjusted accordingly (from 50 to 30 pixels) to keep the overall area of the tiles approximately constant. }
<!--However, the aspect ratio of the plotting area is adjusted to increase the length in pixels of the stacked bar elements, and  decrease the relative thickness \kr{wording?} of each bar. -->
<!--kr{Do we have a measurement of the pixels for this? I think a sentence on that would close out this section nicely.}\hh{like that?}\kr{I was thinking the length pixels, since we talk about how many they are in Task 1, so we could talk about how many JND the difference is. I like the height thing though too, so maybe both? }-->



## Study design -- participants

Participants were recruited as part of NORC's AmeriSpeak panel, which utilizes a probability-based sampling methodology and samples U.S. households from NORC's National Sample Frame that provides coverage of over 97% of U.S. households.
The current panel size is 54,001 panel members aged 13 and over residing in over 43,000 households [@dennis2019technical].
\hh{I don't think I've got the right citation} Each test was conducted using the AmeriSpeak Omnibus survey, which runs biweekly and samples around 1,000 U.S. adults to answer questions on a variety of topics. Our tests require visual inspection of an image; for this reason, our survey questions were only presented to web-based panelists and not panelists who respond via phone interviews. 

<!--\hh{XXX I think what you are trying to say is that we do not know, if participants have seen the stimuli multiple times, but if they did, there was at least a month in between?} \kr{Yes that's what I was trying to say here!!}-->

The Omnibus sample is not longitudinal in nature, i.e.\ we do not have any information about whether the same panelists were included in multiple rounds. While there is a (small) chance that panelists are included in multiple rounds of our data, there was at least a one month gap between viewing each visual stimulus, and for the purposes of the analysis we will consider data from these viewings as independent.

<!-- Given the nature of pulling a sample from the panel for each Omnibus round, there is a possibility that some participants may have been included in multiple rounds; however, our collected data is not a longitudinal or panel study, so we do not have repeated responses from the same participants across each of our tests.
Each of the tests was run at least a month apart, so if respondents did participate in multiple rounds, there was at least a one month gap between viewing each visual stimulus.
-->

## Study design -- survey weighting

Paragraph on how survey weights are derived?
-- **Question for Ed -- is this something AmeriSpeak can provide boilerplate language for?**

All calculations in this paper are done in R [@RLanguage], and weights are applied in analyses using the `survey` package [@lumleyAnalysisComplexSurvey2004] version 4.0 [@survey] based on @lumleyComplexSurveysGuide2010.

When combining responses from different surveys, weights need to be calibrated to the total population, so that their order after combining still reflects their relative importance. 

<!--
\kr{Do we need to update this description to just say how we did it for combining across all the rounds represented?}\hh{done} -->

We *combine* (rather than cumulate) a set of $\ell$ surveys $S_1$, $S_2$, ... $S_\ell$ (with $ \ell \ge 2$ ) as described in @omuircheartaighCombiningSamplesVs2002, by multiplying weights in $S_i$ by $\lambda_i$, for $1 \le i \le \ell$. $\lambda_i \in [0,1]$ with $\sum_i \lambda_i = 1$ is given as
$$
\lambda_i = \frac{n_i/d_i}{\sum_{j=1}^{\ell}n_j/d_j},
$$ where $n_i$ is the nominal sample of survey $S_i$ and $d_i$ are the design effects for the estimators.
Here, $d_i$ are estimated as 
$$
d_i = 1 + CV(w \in S_i)^2
$$ 
where $CV$ is the coefficient of variation of the weights $w$ within each sample, and is estimated as in @kishSurveySampling1965 :
$$ 
CV(w \in S) = \frac{\widehat{Var(w)}}{\bar{w}^2}.
$$

<!--Note that @omuircheartaighCombiningSamplesVs2002 estimate $\lambda$ separately for any combination of race/ethnicity by sex.
We employ that strategy whenever we include demographic variables in the analysis, otherwise we use a single adjustment for the weights. -->


The data for this paper were collected in several rounds as part of the NORC Omnibus. The resulting number of participants, effective sample sizes, and $\lambda$ values are shown in \autoref{tbl-rounds}.

\hh{we might be able to include the lambda values for combining the tables here as well}

<!-- | Name    | Date       |                \# Participants |                effective sample size |         Sum of weights $\sum_i w_i$ | -->

<!-- |---------------|---------------|--------------:|--------------:|--------------:| -->

<!-- | Round 1 | April 2022 |           `r nrow(round1_adj)` | `r round(neff(round1_adj$weight),1)` | `r round(sum(round1_adj$weight),1)` | -->

<!-- | Round 2 | May 2022   |           `r nrow(round2_adj)` | `r round(neff(round2_adj$weight),1)` | `r round(sum(round2_adj$weight),1)` | -->

<!-- | Round 3 | Jun 2022   |           `r nrow(round3_adj)` | `r round(neff(round3_adj$weight),1)` | `r round(sum(round3_adj$weight),1)` | -->

<!-- | Round 6 | Sep 2022   | `r nrow(round6_adj)` \[Split\] | `r round(neff(round6_adj$weight),1)` | `r round(sum(round6_adj$weight),1)` | -->

<!-- | Round 7 | Oct 2022   |           `r nrow(round7_adj)` | `r round(neff(round7_adj$weight),1)` | `r round(sum(round7_adj$weight),1)` | -->

<!-- |         |            |                                |                                      |                                     | -->

<!-- : Survey rounds: dates, number of participants (nominal sample size), effective sample size, and sum of weights. {#tbl-rounds} -->



| Name    | Date       |      \# Participants |                effective sample size |         Sum of weights $\sum_i w_i$ | $\lambda_i$ |
|---------------|---------------|--------------:|--------------:|--------------:|--------------:|
| Round 1 | April 2022 | `r nrow(round1_adj)` | `r round(neff(round1_adj$weight),1)` | `r round(sum(round1_adj$weight),1)` | $\lambda_1$  =  `r round(lambdas[1],3)` |
| Round 2 | May 2022   | `r nrow(round2_adj)` | `r round(neff(round2_adj$weight),1)` | `r round(sum(round2_adj$weight),1)` | $\lambda_2$  = `r sprintf("%.3f",lambdas[2])` |
| Round 3 | June 2022   | `r nrow(round3_adj)` | `r round(neff(round3_adj$weight),1)` | `r round(sum(round3_adj$weight),1)` | $\lambda_3$  = `r round(lambdas[3],3)` |
| **Total**        |    ---        |    `r nrow(round_123)`   |  `r round(neff(round_123$weight),1)`   |    |   |

: Survey rounds: dates, number of participants (nominal sample size), effective sample size, sum of weights, and factors for the convex combination as discussed above. {#tbl-rounds}



# Results


### Respondents

A total of `r sum(c(nrow(round1_adj), nrow(round2_adj), nrow(round3_adj)))` respondents participated across the three rounds. The number of responses and corresponding effective sample sizes in each round are shown in \autoref{tbl-rounds}. All responses were combined into one set of survey responses, with adjusted combined sample weights and indicators for which task each respondent was exposed to. The resulting sample sizes corresponding to each task are shown in \autoref{tbl-tasks}. 

| Name    | Description    |      \# Participants |                effective sample size |
|---------------|---------------|--------------:|--------------:|
| Task 1 | Vertical| `r nrow(round1_adj) + nrow(round2_adj)` | `r round(neff(round_12$weight),1)` |
| Task 2 | Horizontal |`r nrow(round3_adj[round3_adj$P_EXP == 1,])` | `r round(neff(round3_adj[round3_adj$P_EXP == 1,]$weight),1)` |
| Task 3 | Horizontal wide |`r nrow(round3_adj[round3_adj$P_EXP == 2,])` | `r round(neff(round3_adj[round3_adj$P_EXP == 2,]$weight),1)` |

: Survey tasks: number of participants (nominal sample size) and effective sample size for each task. {#tbl-tasks}

Distributions of demographic characteristics across each task are shown in \autoref{fig-rounds-demographics}, the dark points show percentages for US adults based on the American Community Survey 2021. The distribution of demographics are all quite similar across tasks and their 90% margin of errors (hashed lines) all cover the Census estimates. 
<!--\kr{Split by rounds or by tasks? I feel like since we later split by task, maybe we should show the demos by task? Or we can just talk about how Round 3 was split randomly along demos for Tasks 2/3?}\hh{changed the figure to show demos by task -- do we need confidence intervals? -- or a comparison against census data?}-->

```{r}
#| label: fig-rounds-demographics
#| fig-cap: "Demographics of respondents in each of the tasks. The dots show percentages of the national demographics as estimated by the American Community Survery (ACS) 2021."
#| fig-height: 7
#| fig-width: 12

round_123 <- round_123 %>% group_by(chart) %>% 
  mutate(
    weight_pct_round = weight/sum(weight)*100
  )

# from ACS 2001
# https://data.census.gov/table?q=sex+of+adults+age+greater+than+18+across+the+US&tid=ACSDP1Y2021.DP05	
acs_gender <- data.frame(gender = c("Male", "Female"), 
                         estimate = c(126690177, 131728290), moe=c(22354, 18838),
                         percent = c(49, 51), moe_percent = 0.1)

omni_gender <- data.frame(svyby(~gender, by=~chart, design=survey_123, svymean))
omni_gender <- rbind(
  omni_gender %>% dplyr::select(chart, ends_with("Male")) %>% mutate(sex="Male") %>%
    rename(mean = "genderMale", SE = "se.genderMale") %>% 
    dplyr::select(chart, mean, SE, sex),
  omni_gender %>% dplyr::select(chart, ends_with("Female")) %>% mutate(sex="Female") %>%
    rename(mean = "genderFemale", SE = "se.genderFemale")
  ) %>%
  mutate(
    sex = factor(sex, levels=c("Male", "Female")),
    mean = 100*mean,
    SE = 100*SE
  )
omni_gender <- omni_gender %>% mutate(
  low = mean - 1.6445*SE, # for 90% MoE 
  high = mean + 1.6445*SE
)
omni_gender <- omni_gender %>%
  dplyr::select(-SE) %>%
  pivot_longer(cols =c(mean, low, high), names_to="group", values_to="value") 


gender <- round_123 %>% 
  ggplot() + 
  geom_bar(aes(x = gender, weight = weight_pct_round), fill="grey", colour = "grey30", linewidth=0.5) + 
  # geom_rect(
  #   aes(
  #     xmin = as.numeric(sex) - 0.45,
  #     ymin = mean - 1.96* SE,
  #     xmax = as.numeric(sex) + 0.45,
  #     ymax = mean + 1.96* SE,
  #   ), 
  #   fill = "black", alpha = 0.25, #colour = "white", linewidth = 0.25,
  #   data = omni_gender
  # ) +
  geom_segment(
    aes(
      x = as.numeric(sex) - 0.45,
      y = value,
      xend = as.numeric(sex) + 0.45,
      yend = value,
      linetype=group
    ), 
    alpha = 1, colour = "black", linewidth = 0.25,
    data = omni_gender
  )  +
  ylab("Percent") + 
  ggtitle("Gender") +
  theme(axis.text.x = element_text(angle=30, hjust = 1)) +
  xlab("") +
  facet_grid(.~chart) + 
  geom_point(aes(x = gender, y = percent), data = acs_gender) +
  theme(legend.position ="none") +
  scale_linetype_manual(values=c(2,2,1))


acs_age <- data.frame(age4 = c("18-29", "30-44", "45-59", "60+"), 
                      estimate = c(52325456, 66712246, 61814869, 77565896))
acs_age <- acs_age %>% mutate(percent = estimate/sum(estimate)*100)

age <- round_123 %>% 
  ggplot(aes(x = age4)) + 
  geom_bar(aes(weight = weight_pct_round), fill="grey", colour = "grey30", linewidth=0.5) + 
  ylab("Percent") + 
  ggtitle("Age ") +
  theme(axis.text.x = element_text(angle=30, hjust = 1)) +
  xlab("") + ylim(c(0,35)) +
  facet_grid(.~chart) +
  geom_point(aes(x = age4, y = percent), data = acs_age)


acs_educ5 <- data.frame(educ5 = c("Less than HS", "HS or equivalent", "Some college",  "Bachelor", "Post graduate"), 
                      estimate = c(28687047, 69610758, 77707408, 50031690, 29454503))
acs_educ5 <- acs_educ5 %>% mutate(percent = estimate/sum(estimate)*100)

education <- round_123 %>% 
  ggplot(aes(x = educ5)) + 
  geom_bar(aes(weight = weight_pct_round), fill="grey", colour = "grey30", linewidth=0.5) + 
  ylab("Percent") + 
  ggtitle("Education levels") +
  theme(axis.text.x = element_text(angle=30, hjust = 1)) +
  xlab("") + ylim(c(0,35)) +
  facet_grid(.~chart) +
  geom_point(aes(y = percent), data = acs_educ5)

income <- round_123 %>% 
  ggplot(aes(x = income4, weight = weight_pct_round)) + geom_bar() + 
  ylab("Percent") + 
  ggtitle("Income levels") +
  theme(axis.text.x = element_text(angle=30, hjust = 1)) +
  xlab("") + ylim(c(0,35)) +
  facet_grid(.~chart)

(gender + education + plot_layout(nrow=1, widths=c(2,5))) / 
  (age + income + plot_layout(nrow=1, widths = c(4,4)))

```


### Accuracy of responses  

We first investigate respondent accuracy in selecting the correct response. As participants were able to select the option 'They are the same', there are several ways to model accuracy and response. The argument could be made that for the purposes of practical interpretation, 'they are the same' is a correct choice, as the options are visually very similar and not substantially different values within the context of the data shown in the chart. However, we are interested in understanding whether viewers *can* perceive the difference and correctly identify which piece is larger and thus consider multiple ways of modeling response to investigate participant response patterns. We begin by defining a measure of binary 'correctness', for which all answers that are not the correct option (B [D] is bigger) are 'incorrect', including the selection of 'they are the same'. 

\autoref{fig-alignment}(a) displays all responses along the binary correctness measure, separated by whether the stimulus was an aligned task or unaligned task. We can see that levels of accuracy for all responses are significantly higher for the 'easier' (aligned) task, with about twice as many respondents correctly selecting the larger of the two marked elements. 


```{r ttest}
paired <- svyttest(correct~task, survey_123)
abcd <- svyby(~correct, by=~task, survey_123, svytotal)

marginal_results <- data.frame(abcd)
```

<!--Rounds 1 and 2 are combined by adjusting the weights with $\lambda$ = `r round(lambdas[1],3)` for an effective sample size of `r round(neff(round_12$weight),1)`.-->


Because each participant was shown both the aligned and unaligned versions of the chart, we can use a paired $t$-test to compare mean accuracy between the two charts.
The resulting $t$-statistic is highly significant ($t$ statistic: `r round(paired$statistic,1)`, df: `r paired$parameter`, $p$-value: $\ll 0.0001$).


```{r accuracy,   warning=FALSE}
#| fig-cap: "On the left (a), a stacked barchart shows the number of respondents with correct (green) and incorrect (grey) responses to the two comparison questions. When tiles are aligned along the same axis, more than twice the number of responses are correct. The shaded area along the top of the green tiles corresponds to  95\\% confidence intervals around (marginal) correct responses. On the right (b), a parallel coordinate plot shows all combinations of responses. There's a huge asymmetry in the number of responses where participants answered only one of the questions correctly. A lot more responses are correct when comparing aligned tiles than unaligned tiles."
#| label: fig-alignment
#| out-width: 80%
#| fig-height: 5
#| fig-width: 10

gg1 <- round_123_long %>%
  mutate(
    Response = ifelse(correct, "Correct", "Not correct"),
    Response = factor(Response, levels=c("Not correct", "Correct")),
    Question = ifelse(task=="ab",
                      "Unaligned: A or B?", "Aligned: C or D?")
  ) %>%
#  filter(Response == "Correct") %>%
  ggplot(aes(x = Question, fill = Response, weight = weight)) +
  geom_bar() +
  ylab("Number of (effective) Respondents") +
  xlab("") +
  scale_fill_manual(values=c("grey", "forestgreen")) +
  scale_y_continuous(
      sec.axis = sec_axis(~./sum(round_123$weight)*100,
                          name="Percent responses")
  ) +
  theme(legend.position = "bottom") +
  ggtitle("(a) Accuracy of survey responses")  +
  geom_rect(
    aes(
      x = rev(c("Aligned: C or D?", "Unaligned: A or B?")),
      xmin = 2:1 - 0.45,
      ymin = correctTRUE - 1.96* se.correctTRUE,
      xmax = 2:1 + 0.45,
      ymax = correctTRUE + 1.96* se.correctTRUE,
      weight = 1),
    fill = "black", alpha = 0.25, #colour = "white", linewidth = 0.25,
    data = marginal_results
  )

gg2 <- round_123 %>%
  mutate(
    `Unaligned: A or B?` = ifelse(CorrectAB, "Correct", "Not correct"),
    `Aligned: C or D?` = ifelse(CorrectCD, "Correct", "Not correct")
    ) %>%
  pcp_select(`Aligned: C or D?`, `Unaligned: A or B?`) %>%
  pcp_scale() %>%
  pcp_arrange() %>%
  ggplot(aes_pcp()) +
    geom_pcp_axes() +
    geom_pcp(aes(colour = CorrectABCD), alpha = 0.2)+
    geom_pcp_boxes(fill=NA, colour = "black", linewidth=0.25) +
    geom_pcp_labels(colour = "black") +
  scale_y_continuous("Number of (effective) Respondents", breaks=seq(0,1,by=0.25),
                     labels = round(sum(round_123$weight)*seq(0,1,by=0.25),0),
                     sec.axis = sec_axis(~.*100,
                          name="Percent responses")) +
  xlab("") +
  scale_colour_manual(values = c("grey", "#84AC84", "#70A570", "forestgreen")) +
  ggtitle("(b) Combinations of responses") +
  theme(legend.position = "none")

gg1 + gg2
```

Next, we consider an ordinal model to investigate response behavior across all three options -- 'A [C] is bigger', 'B [D] is bigger', and 'They are the same', and consider these response patterns across each of the stimuli. 



```{r model-123, warning = FALSE, message = FALSE}

logit_123_correct <- svyglm(correct~task_chart-1, family=quasibinomial(), design=survey_123b)
logit_123_correct3 <- svyglm(correct3~task_chart-1, family=quasibinomial(), design=survey_123b)


ci_confint <- svyby(~correct, by=~task_chart, design=survey_123b, svyciprop)
ci_confint3 <- svyby(~correct3, by=~task_chart, design=survey_123b, svyciprop)


coefs_123 <- broom::tidy(logit_123_correct)
coefs_123_relaxed <- broom::tidy(logit_123_correct3)

# create all pairwise comparisons for tasks and questions.
cross_compare <- glht(logit_123_correct, mcp(task_chart="Tukey"))
cross_compare3 <- glht(logit_123_correct3, mcp(task_chart="Tukey"))

# get letters for each confidence 
ci_confint$letters <- cld(cross_compare)$mcletters$Letters
ci_confint3$letters <- cld(cross_compare3)$mcletters$Letters

coefs_123 <- coefs_123 %>% left_join(
  ci_confint %>%
    mutate(
      task_chart = paste0("task_chart",task_chart, sep="")
    ) %>% dplyr::select(task_chart, letters), by=c("term"= "task_chart"))

coefs_123_relaxed <- coefs_123_relaxed %>% left_join(
  ci_confint3 %>% 
    mutate(
      task_chart = paste0("task_chart",task_chart, sep="")
    ) %>% dplyr::select(task_chart, letters), by=c("term"= "task_chart"))


ci_confint <- ci_confint %>% 
  separate_wider_delim(
    task_chart, delim = "-", names =c("task", "chart")
    )  %>% mutate(
  task = factor(task, levels=c("cd", "ab"), labels=c("Aligned", "Unaligned")),
  chart = factor(chart, levels =c("Vertical", "Horizontal", "Horizontal wide", "Floating bars"))
)

ci_confint3 <- ci_confint3 %>% 
  separate_wider_delim(
    task_chart, delim = "-", names =c("task", "chart")
    ) %>% mutate(
  task = factor(task, levels=c("cd", "ab"), labels=c("Aligned", "Unaligned")),
  chart = factor(chart, levels =c("Vertical", "Horizontal", "Horizontal wide", "Floating bars"))
) 
```

```{r}
#| label: fig-response-123
#| fig-cap: "Responses for accuracy in the three designs. Responses to the same task are shown side-by-side for the three designs.  The overlaid rectangles represent 95% confidence intervals. The letters in blue and orange encode significances between pairwise proportions: two bars have a significantly different proportion (at a 5% significance level) if they do not share a letter. There is no significant difference between the three designs for wrong responses. When tiles are unaligned, the horizontal wide barchart is showing the highest accuracy. For aligned tile, the horizontal wide design and the vertical design do not show a significant difference in accuracy."
#| fig-height: 4.25
#| fig-width: 7
#| out-width: 80%

round_123b_long %>% 
  mutate(
    response3 = c("Wrong", "They are the same", "Correct")[correct+correct3+1],
    response3 = factor(response3, levels=rev(c("Correct", "They are the same", "Wrong"))),
    task = factor(task, levels=c("cd", "ab"), labels=c("Aligned", "Unaligned")),
    chart = factor(chart, levels=c("Vertical", "Horizontal", "Horizontal wide", "Floating bars"))
  ) %>% 
  ggplot(aes(x = chart)) + 
  geom_bar(aes(fill = response3, weight = weight), position="fill") +
  facet_grid(.~task, scales = "free_x", space="free_x") + 
  theme(legend.position="bottom") + 
  scale_fill_manual("Response", values=c("orange", "grey", "steelblue")) +
  xlab("") + 
  geom_rect(
    aes(
      xmin = as.numeric(chart) - 0.45,
      ymin = correct - 1.96* `se.as.numeric(correct)`,
      xmax = as.numeric(chart) + 0.45,
      ymax = correct + 1.96* `se.as.numeric(correct)`,
    ), 
    fill = "black", alpha = 0.25, #colour = "white", linewidth = 0.25,
    data = ci_confint
  ) +
  geom_segment(
    aes(
      x = as.numeric(chart) - 0.45,
      y = correct,
      xend = as.numeric(chart) + 0.45,
      yend = correct
    ), 
    alpha = 1, colour = "black", linewidth = 0.25,
    data = ci_confint
  ) +
  geom_rect(
    aes(
      xmin = as.numeric(chart) - 0.45,
      ymin = correct3 - 1.96* `se.as.numeric(correct3)`,
      xmax = as.numeric(chart) + 0.45,
      ymax = correct3 + 1.96* `se.as.numeric(correct3)`,
    ), 
    fill = "black", alpha = 0.25, #colour = "white", linewidth = 0.25,
    data = ci_confint3
  ) +
  geom_segment(
    aes(
      x = as.numeric(chart) - 0.45,
      y = correct3,
      xend = as.numeric(chart) + 0.45,
      yend = correct3
    ), 
    alpha = 1, colour = "black", linewidth = 0.25,
    data = ci_confint3
  ) +
  scale_y_continuous("Percent responses", breaks=seq(0,1,by=0.25), labels = 100*seq(0,1,by=0.25), limits = c(-0.1, 1.1)) + 
  geom_text(aes(label=letters), y = -0.05, data = ci_confint, colour = "steelblue") +
  geom_text(aes(label=letters), y = 1.05, data = ci_confint3, colour = "darkorange") 
```

@fig-response-123 shows the results of a cell-means model with ordinal response $Y_k$, where $Y_k$ is the $k$th participant's response, $Y_k \in \{1, 2, 3\}$, where 'correct' is encoded as 1, 'they are the same' is encoded as 2, and 'wrong' is encoded as 3:

$$
\text{logit }P(Y_k \le \ell) = \mu_{ij\ell(k)},
$$

where $\ell \in \{1, 2\}$; $i \in \{1, 2\}$ is the comparison type (1 = Aligned, 2 = Unaligned), and $j \in \{1, 2, 3\}$ is the chart design, with 1 = Vertical, 2 = Horizontal, and 3 = Horizontal wide.
The estimated values and 95% confidence intervals are shown in @tbl-rounds-123. \hh{The lowercase letters indicate significances between pairs of estimates: two estimates (in the same column) are significantly different at 5% level, if they do not have any letter in common [@piephoAlgorithmLetterBasedRepresentation2004].

```{r}
#| label: tbl-rounds-123
#| tbl-cap: Log-odds for the cell-means model, letters behind numbers indicate pairwise significances. Within the same **column** values are significantly different (at 5%) if they do not share the same letter.

dframe <- rbind(
  coefs_123 %>% mutate(level = c("correct | same or wrong")),
  coefs_123_relaxed %>% mutate(level = c("correct or same | wrong"))
) %>% mutate(
  term = gsub("task_chart", "", term),
) %>% separate(term, into=c("task", "chart"), sep="-")

dframe %>% 
  dplyr::select(level, task, chart, estimate, std.error, letters) %>%
  mutate(
    task = ifelse(task=="ab", "Unaligned", "Aligned")
  ) %>%
  group_by(task) %>%
  mutate(
    ci_low = exp(estimate-1.96*std.error),
    ci_high = exp(estimate+1.96*std.error),
    estimate = exp(estimate)
  ) %>%
  tidyr::pivot_wider(
        id_cols = c(task, chart),
        names_from = level,
        values_from = c(estimate, ci_low, ci_high, letters)
      ) %>%
  gt() %>%
  tab_header(title = "Odds of accuracy by task and chart type") %>%
  fmt_number(
    everything(),
    decimals=2
  ) %>%
  cols_merge(
    columns = contains("ci") & contains("same or wrong"),
    pattern = "<< [{1}, {2}]>>"
  ) %>%
  tab_spanner(
    label = "correct | same or wrong",
    columns =  c("estimate_correct | same or wrong", "ci_low_correct | same or wrong",
                 "letters_correct | same or wrong")
  ) %>%
  cols_merge(
    columns = contains("ci") & contains("correct or same"),
    pattern = "<< [{1}, {2}]>>"
  ) %>%
  tab_spanner(
    label = "correct or same | wrong",
    columns =  c("estimate_correct or same | wrong", "ci_low_correct or same | wrong",
                "letters_correct or same | wrong")
  ) %>%
  tab_options(
    column_labels.font.weight = "bold",
    row_group.font.weight = "bold",
    footnotes.font.size = 8
  ) %>%
  cols_label(
    .list = c(
      "chart" = "",
      "estimate_correct | same or wrong" = "correct | same or wrong",
      "estimate_correct or same | wrong" = "correct or same | wrong",
      "estimate_correct or same | wrong" = "Est.",
      "ci_low_correct or same | wrong" = "          [95% CI]",
      "letters_correct or same | wrong" = " ",
      "estimate_correct | same or wrong" = "Est.",
      "ci_low_correct | same or wrong" = "          [95% CI]",
      "letters_correct | same or wrong" = " "
    )
  ) %>% tab_footnote(
    footnote = "We are including here, responses on an A vs B comparison for floating bars, corresponding to a length assessment, see bottom right of Figure 1.",
    locations = cells_body(
      rows = 1,
      columns = "chart"
    )
  ) 

```

One pattern in accuracy holds across each of the three structural variations: the aligned task has a higher level of accuracy than its unaligned counterpart. Interestingly, while we expect an improvement in accuracy when shifting from the horizontal to the horizontal wide design given the larger difference in pixel length between the two pieces, the resulting effects on the accuracy of the responses are not completely straightforward: the shift from a vertical to the (tall) horizontal design is detrimental to an accurate perception for both aligned and unaligned comparisons. The re-scaled design of the wide horizontal bars reclaims some of the loss for unaligned bars and outperforms the vertical design by a similar margin in aligned bars, but does not out-perform the vertical design when comparing unaligned tiles. 

Rates of selecting the incorrect response are low across all three structural variations and the aligned and unaligned tasks; while viewers select the incorrect response at significantly higher rates for all three unaligned tasks relative to the aligned tasks, those rates do not differ significantly across the three structural variations. Most of the observed differences in response patterns across structural variations are attributed to respondents' selection between the correct option or the 'they are the same' option.  

To better comprehend these observed patterns, we investigate viewer interaction with the tasks more broadly as well as differences in response selection across demographic groups.  

### Respondent behavior  


One contributing factor to the observed patterns in response accuracy might be the way that participants interact with the different designs.
Across all tasks, about half of all participants make use of the option to zoom into charts. We observe that while zooming does help with the overall accuracy (which is in agreement with the findings by @luModelingJustNoticeable2022 about the physical size of stimuli), the increase is not significant.
However, different designs lead to different rates of zooming: we observe in \autoref{fig-zoom-123} that when dealing with the vertical design, the rate of zooming is significantly higher than for the two horizontal designs.

<!-- The rate of zooming into charts is about twice as high for participants filling out the survey on their smart phones than on other, usually bigger devices such as tablets or desktops. -->

```{r zoom-123}
logit_zoom <- 
  svyglm(I(zoom=="yes")~response3+task+chart, family=quasibinomial(), design=survey_123)

coefs_zoom <- broom::tidy(logit_zoom)
```

```{r}
#| label: fig-zoom-123
#| fig-cap: "Zooming - not significant for accuracy or task, but changes by the type of chart."
#| fig-height: 4.25
#| fig-width: 7
#| out-width: 80%

zt <- round_123_long %>% 
  mutate(
    response3 = c("Wrong", "They are the same", "Correct")[correct+correct3+1],
    response3 = factor(response3, levels=rev(c("Correct", "They are the same", "Wrong"))),
    task = factor(task, levels=c("cd", "ab"), labels=c("Aligned", "Unaligned")),
    chart = factor(chart, levels=c("Vertical", "Horizontal", "Horizontal wide")),
    zoom = factor(zoom, levels=c("no", "yes"))
  ) %>% 
  ggplot(aes(x = response3)) + 
  geom_bar(aes(fill = zoom, weight = weight), position="fill") +
  facet_grid(.~task) + 
  theme(axis.text.x = element_text(angle=30, hjust = 1)) + 
  scale_fill_manual("Zoom", values=c("grey", "#1274F3")) +
  xlab("") + 
  ggtitle("Zooming behavior\nby task") +
  scale_y_continuous("Percentage", breaks=seq(0,1, by=0.25), 
                     labels = seq(0,1, by=0.25)*100)

zd <- round_123_long %>% 
  mutate(
    response3 = c("Wrong", "They are the same", "Correct")[correct+correct3+1],
    response3 = factor(response3, levels=rev(c("Correct", "They are the same", "Wrong"))),
    task = factor(task, levels=c("cd", "ab"), labels=c("Aligned", "Unaligned")),
    chart = factor(chart, levels=c("Vertical", "Horizontal", "Horizontal wide")),
    zoom = factor(zoom, levels=c("no", "yes"))
  ) %>% 
  ggplot(aes(x = response3)) + 
  geom_bar(aes(fill = zoom, weight = weight), position="fill") +
  facet_grid(.~chart) + 
  scale_fill_manual("Zoom", values=c("grey", "#1274F3")) +
  xlab("") + 
  ggtitle("\nby chart") +
  theme(axis.text.x = element_text(angle=30, hjust = 1)) +
  scale_y_continuous("Percentage", breaks=seq(0,1, by=0.25), 
                     labels = seq(0,1, by=0.25)*100)

zt + zd + plot_layout(widths=c(5, 7.5), guides='collect') 
  
```

To formalize this pattern in a model, let $Y_{jk}$ describe the zooming behavior of panelist $k$ on task $j$.
We model zooming behavior (no = 0, yes = 1) as a logistic regression by correctness of response ($\rho$), task ($\tau$), and design ($\delta$) of the chart: 

$$
\text{logit } P(Y_k \le 1) = \mu + \rho_{i(k)} + \tau_{j(k)} + \delta_{\ell(k)},
$$

The resulting model estimates and confidence intervals are displayed in \autoref{tbl-zoom-123}. We can observe that zooming behavior *only* differs significantly by structural design; rates do not differ significantly between the aligned and unaligned tasks, nor do they differ significantly by the chosen response. This higher rate of zooming does not necessarily lead to higher accuracy; although rates of accuracy are significantly higher for the vertical orientation relative to the horizontal orientation, they are not significantly higher -- and in fact, are significantly lower on the aligned task -- than the accuracy for the horizontal wide orientation.  \hh{I'll pretty up @tbl-zoom-123 if we are going to keep it.} \kr{I think we should! it's nice to have a model for this}

```{r}
#| label: tbl-zoom-123
#| tbl-cap: Coefficients for logistic regression of zooming by task

#    term = TeX(c("$\\mu$", "They are the same $\\rho_2$", "wrong $\\rho_3$", "Aligned $\\tau_2$", "Horizontal wide $\\delta_2$", "Vertical $\\delta_3$"))

coefs_zoom$term <- c("$$\\hat{\\mu}$$", "$$\\text{They are the same } \\widehat{\\rho}_{2}$$", 
                     "Wrong $$\\widehat{\\rho}_{3}$$", "Aligned $$\\widehat{\\tau}_{2}$$", 
                     "Horizontal wide $$\\widehat{\\delta}_{2}$$", "Vertical $$\\widehat{\\delta}_{3}$$")

coefs_zoom %>%
  mutate(
    term = ifelse(term=="(Intercept)", "Intercept", term),
    p.value = ifelse(p.value<0.0001, "< 0.0001", sprintf("%.4f", p.value)),
    variable = c("", "Response", "Response", "Task", "Chart design", "Chart design")
  ) %>%
  group_by(variable) %>%
  gt() %>%
  tab_header("Estimates for logistic regression on zooming behavior") %>%
  tab_options(
    column_labels.font.weight = "bold",
    row_group.font.weight = "bold"
  ) %>%
  cols_label(
    std.error = "SE",
    statistic = "t-statistic",
    p.value = "p-value"
  ) %>%
  fmt_number(
    columns = c(estimate, std.error),
    decimals = 2
  ) %>%
  fmt_number(
    columns = statistic,
    decimals = 1
  ) %>%
  cols_align(
    columns = p.value,
    align = "right"
  ) 

```

 

### How certain are participants?

<!--
\kr{Do we have anything about total time spent on tasks across the 3 structures? We have time on each question for rounds 2-3. And can we pull in some of the certainty stuff?}

\hh{we could include certainty - I'll look into it}
-->

```{r certainty-data}
round_123_certainty_long <- round_123 %>% 
  pivot_longer(ab_certain:cd_certain, names_to = "Question", values_to = "Response") %>%
  filter(Response != "SKIPPED ON WEB") %>%  # 20 participants
  mutate(
    Correct = ifelse(Question == "ab_certain", CorrectAB, CorrectCD),
    Question = ifelse(Question=="ab_certain", "Unaligned", "Aligned"),
# ordering of levels in Response is from extremely=6 to not at all=2 - not very intuitive
    Response = factor(Response, levels = c("not at all", "slightly", "moderately", "very", "extremely")) # gets rid of the 'SKIPPED ON WEB' and makes the ordering go from 1 to 5
  )

round_123_certainty_long <- round_123_certainty_long %>% mutate(
  correct_question_chart = factor(paste(Correct, Question, chart, sep="-"))
)
survey_123_crt <- 
  survey::svydesign(
  data = round_123_certainty_long, 
  ids = ~CaseId, weights = ~weight)
```

Using linear scores for the response of `Certainty`, with `not certain at all` assigned a score of 1 and `extremely certain` assigned a score of 5, we can estimate the effects of task, chart design, and correctness on certainty  by using  a cell-means model of the form:
$$
Y_{k} = \mu_{ij\ell(k)} + \epsilon_{k},
$$
where $k = 1, ..., N$, $\mu_{ij\ell(k)}$ is average certainty (measured on a scale from 1 to 5) of the four combinations of task and correctness by each of the three designs, where $i = 1, 2$ encodes unaligned/aligned, and $j=1,2$ encodes wrong, correct, and $\ell = 1, 2, 3$ encodes horizontal, wide horizontal, and vertical stacked bar charts, respectively. We also assume that errors are normally distributed, i.e. $\epsilon_k \stackrel{i.i.d}{\sim} N(0, \sigma^2)$ for all $k = 1, ..., N$. 
The results are shown in @fig-certainty. 
What we find, is that the highest scores for certainty are associated with comparing aligned tiles. Certainty scores are not significantly different between correct and wrong responses. Scores are (mostly) significantly lower for the unaligned task. Interestingly, the lowest scores of certainty are associated with correct responses, here. 

```{r model-certainty}
model_certain <- svyglm(as.numeric(Response)~ correct_question_chart-1, design = survey_123_crt)

coefs_certainty <- broom::tidy(model_certain)
coefs_certainty <- coefs_certainty %>% 
  mutate(
    term = gsub("correct_question_chart","", term)
  ) %>%
  separate(term, into=c("Correct", "Question", "chart"), sep="-", remove = FALSE) 

coefs_certainty <- coefs_certainty %>% 
  mutate(
    Question = factor(Question, 
                      levels = c("Aligned", "Unaligned")),
    Correct = factor(Correct, levels=c("FALSE", "TRUE"), 
                     labels = c("Wrong", "Correct")),
    chart = factor(chart, levels=c("Horizontal wide", "Horizontal", "Vertical"))
  )

scale_x <- function(value, from, to) {
  (value-from[1])/diff(from)*diff(to)
}

coefs_certainty <- coefs_certainty %>% 
  mutate(
    estimate = scale_x(estimate, from=c(1,5), to=c(0,1)),
    std.error = std.error/4
  )

round_123_certainty_long <- round_123_certainty_long %>%
  mutate(
    Question = factor(Question, 
                      levels =c("Aligned", "Unaligned")),
    Correct = factor(Correct, levels=c("FALSE", "TRUE"), 
                     labels = c("Wrong", "Correct")),
    chart = factor(chart, levels=c("Horizontal wide", "Horizontal", "Vertical"))
  )

```

```{r model-letters, warning=FALSE}
# create all pairwise comparisons for tasks and questions.
cross_compare_certainty <- glht(model_certain, mcp(correct_question_chart="Tukey"))

foo <- cld(cross_compare_certainty)

# get letters for each confidence 
coefs_certainty$letters <- cld(cross_compare_certainty)$mcletters$Letters
```


```{r}
#| label: fig-certainty
#| out-width: 100%
#| fig-cap: "Certainty by task, chart design, and correctness. Correct answers in the unaligned task have the lowest certainty scores associated with them. Incorrect responses on the unaligned task show a significant boost in certainty in the vertical stacked bar. For horizontal stacked bars the difference between aligned and unaligned charts matters significantly. Errorbars around the points show estimated scores (on a scale of 1 to 5) with corresponding 95% CI. The letters underneath the bars indicate significances. The scores for two bars are significantly different if they do not share a letter. "
#| fig-width: 10
#| fig-height: 4.5

round_123_certainty_long %>%
  ggplot(aes(x = chart)) +
  facet_grid(.~Question+Correct) +
  geom_bar(aes(fill=Response, weight=weight), 
           position=position_fill(reverse=TRUE), 
           colour="grey40", linewidth=0.25) +
  theme(
    axis.text.x = element_text(angle=30, hjust=1)
  ) + 
  scale_y_continuous("Percentage", breaks=seq(0,1, by=0.25), 
                     labels = seq(0,1, by=0.25)*100, limits = c(-0.1, 1),
                     sec.axis = sec_axis( trans=~(.*4)+1, name="Certainty scores")) +
  xlab("") + 
  scale_fill_brewer(palette = "PRGn", guide = guide_legend(reverse = TRUE)) + 
  geom_point(
    aes(y = estimate),
    size = 3,
    data = coefs_certainty
  ) + 
  geom_errorbar(
    aes(ymin = (estimate-1.96*std.error),
        ymax = (estimate+1.96*std.error)),
    width=0.3,
    data = coefs_certainty
  ) + 
  geom_text(aes(label=letters), y = -0.05, data = coefs_certainty) 
```



### Differences across demographic groups

Finally, we turn to investigating responses across demographic groups, a core benefit provided by the large and representative nature of our survey samples. \autoref{fig-demographics} displays response patterns across our three structural variations by gender, age (4 groups), educational attainment (5 groups), and income level (4 groups). Our pattern of higher accuracy in selecting the correct response on the aligned task relative to the unaligned task holds across all demographic groups and structural variations. However, we do observe different response patterns, particularly on the unaligned task, across demographic groups. 

Let $Y_k$ be the response of participant $k$, on a scale from 1 = 'wrong', 2 = 'they are the same' to 3 = 'correct'.
We use a generalized cumulative logistic regression, where $\mu_i$ are intercepts $1 \le i < 3$, $X_k$ are demographics of the $k$th participant (in form of the model matrix), and $\beta_{i}$ are the coefficients. 

$$
\text{ logit } P(Y_k \le i \mid X_k) = \mu_i + X_{k}'\beta_{i}
$$
The resulting model estimates and confidence intervals are shown in \autoref{tbl-demographics}. When considering the easier (aligned) task, response patterns do not differ significantly by age, gender, or education level. We do observe, however, a significantly higher log odds of selecting the 'correct' or 'they are the same' response (or significantly lower log odds of selecting 'incorrect') among those with an income between \$60,000 and \$100,000. When we consider the more difficult (unaligned) task, we see significant separation in response patterns across gender, educational attainment, and income groups. We do not see significant differences in responses by age for either task. Interestingly, we see lower log odds of selecting the 'correct' response among those with higher educational attainment (those with a bachelor's degree or post graduate study) *and* lower log odds of selecting the 'incorrect' response; as income and educational attainment increase, respondents are more likely to select the 'they are the same' option during the difficult task. This speaks to the difficulty of the task, and respondents' interpretation of the task when it is more difficult to perceive small differences. 

<!--\hh{XXX the formatting isn't right yet here ... latex problem with large floats ... will fix it.}-->

```{r}
#| label: fig-demographics
#| fig-cap: "Panelists' demographics matter, particularly, when the task difficulty increases. For aligned tiles, gender, age, and education are not significant factors. However, income levels do have a (small) effect. When income levels increase, the percentage of wrong answers (orange) decreases, while the percentage of correct answers (blue) increases slightly (significant at below 0.05). For the  more difficult task of comparing unaligned tiles, demographics are more significant. Higher levels of education and higher levels of income are associated with a significant increase of panelists choosing a response of 'they are the same', resulting in significant decreases of both correct answers and wrong answers."
#| fig-subcap: 
#|   - Comparisons of aligned tiles
#|   - Comparisons of unaligned tiles
#| fig-height: 4
#| fig-width: 10
#| layout-nrow: 2
#| out-width: 100%

educ_cd <- round_123_long %>% 
  filter(task=="cd") %>%
  mutate(
    response3 = c("Wrong", "They are the same", "Correct")[correct+correct3+1],
    response3 = factor(response3, levels=rev(c("Correct", "They are the same", "Wrong"))),
    task = factor(task, levels=c("cd", "ab"), labels=c("Aligned", "Unaligned")),
    chart = factor(chart, levels=c("Vertical", "Horizontal", "Horizontal wide"))
  ) %>% 
  ggplot(aes(x = educ5)) + 
  facet_grid(~task) + 
  geom_bar(aes(fill = response3, weight = weight), position="fill") +
  theme(legend.position="none",
        axis.text.x = element_text(angle=30, hjust=1)) + 
  scale_fill_manual("Response", values=c("orange", "grey", "steelblue")) +
  xlab("") +
    ggtitle("\nby education") +
  scale_y_continuous("Percentage", breaks=seq(0,1, by=0.25), labels = seq(0,1, by=0.25)*100)

educ_ab <- round_123_long %>% 
  filter(task=="ab") %>%
  mutate(
    response3 = c("Wrong", "They are the same", "Correct")[correct+correct3+1],
    response3 = factor(response3, levels=rev(c("Correct", "They are the same", "Wrong"))),
    task = factor(task, levels=c("cd", "ab"), labels=c("Aligned", "Unaligned")),
    chart = factor(chart, levels=c("Vertical", "Horizontal", "Horizontal wide"))
  ) %>% 
  ggplot(aes(x = educ5)) + 
  facet_grid(.~task) + 
  geom_bar(aes(fill = response3, weight = weight), position="fill") +
  theme(legend.position="none",
        axis.text.x = element_text(angle=30, hjust=1)) + 
  scale_fill_manual("Response", values=c("orange", "grey", "steelblue")) +
  xlab("") +
    ggtitle("\nby education") +
  scale_y_continuous("Percentage", breaks=seq(0,1, by=0.25), labels = seq(0,1, by=0.25)*100)

gender_cd <- round_123_long %>% 
  filter(task=="cd") %>%
  mutate(
    response3 = c("Wrong", "They are the same", "Correct")[correct+correct3+1],
    response3 = factor(response3, levels=rev(c("Correct", "They are the same", "Wrong"))),
    task = factor(task, levels=c("cd", "ab"), labels=c("Aligned", "Unaligned")),
    chart = factor(chart, levels=c("Vertical", "Horizontal", "Horizontal wide"))
  ) %>% 
  ggplot(aes(x = gender)) + 
  facet_grid(.~task) + 
  geom_bar(aes(fill = response3, weight = weight), position="fill") +
  theme(legend.position="none",
        axis.text.x = element_text(angle=30, hjust=1)) + 
  scale_fill_manual("Response", values=c("orange", "grey", "steelblue")) +
  xlab("") +
  ggtitle("Accuracy\nby gender") +
  scale_y_continuous("Percentage", breaks=seq(0,1, by=0.25), labels = seq(0,1, by=0.25)*100)

gender_ab <- round_123_long %>% 
  filter(task=="ab") %>%
  mutate(
    response3 = c("Wrong", "They are the same", "Correct")[correct+correct3+1],
    response3 = factor(response3, levels=rev(c("Correct", "They are the same", "Wrong"))),
    task = factor(task, levels=c("cd", "ab"), labels=c("Aligned", "Unaligned")),
    chart = factor(chart, levels=c("Vertical", "Horizontal", "Horizontal wide"))
  ) %>% 
  ggplot(aes(x = gender)) + 
  facet_grid(.~task) + 
  geom_bar(aes(fill = response3, weight = weight), position="fill") +
  theme(legend.position="none",
        axis.text.x = element_text(angle=30, hjust=1)) + 
  scale_fill_manual("Response", values=c("orange", "grey", "steelblue")) +
  xlab("") +
  ggtitle("Accuracy\nby gender") +
  scale_y_continuous("Percentage", breaks=seq(0,1, by=0.25), labels = seq(0,1, by=0.25)*100)

income_cd <- round_123_long %>% 
  filter(task=="cd") %>%
  mutate(
    response3 = c("Wrong", "They are the same", "Correct")[correct+correct3+1],
    response3 = factor(response3, levels=rev(c("Correct", "They are the same", "Wrong"))),
    task = factor(task, levels=c("cd", "ab"), labels=c("Aligned", "Unaligned")),
    chart = factor(chart, levels=c("Vertical", "Horizontal", "Horizontal wide"))
  ) %>% 
  ggplot(aes(x = income4)) + 
  facet_grid(.~task) + 
  geom_bar(aes(fill = response3, weight = weight), position="fill") +
  theme(legend.position="none",
        axis.text.x = element_text(angle=30, hjust=1)) + 
  scale_fill_manual("Response", values=c("orange", "grey", "steelblue")) +
  xlab("") +
    ggtitle("\nby income") +
  scale_y_continuous("Percentage", breaks=seq(0,1, by=0.25), labels = seq(0,1, by=0.25)*100)

income_ab <- round_123_long %>% 
  filter(task=="ab") %>%
  mutate(
    response3 = c("Wrong", "They are the same", "Correct")[correct+correct3+1],
    response3 = factor(response3, levels=rev(c("Correct", "They are the same", "Wrong"))),
    task = factor(task, levels=c("cd", "ab"), labels=c("Aligned", "Unaligned")),
    chart = factor(chart, levels=c("Vertical", "Horizontal", "Horizontal wide"))
  ) %>% 
  ggplot(aes(x = income4)) + 
  facet_grid(.~task) + 
  geom_bar(aes(fill = response3, weight = weight), position="fill") +
  theme(legend.position="none",
        axis.text.x = element_text(angle=30, hjust=1)) + 
  scale_fill_manual("Response", values=c("orange", "grey", "steelblue")) +
  xlab("") +
    ggtitle("\nby income") +
  scale_y_continuous("Percentage", breaks=seq(0,1, by=0.25), labels = seq(0,1, by=0.25)*100)

race <- round_123_long %>% 
  mutate(
    response3 = c("Wrong", "They are the same", "Correct")[correct+correct3+1],
    response3 = factor(response3, levels=rev(c("Correct", "They are the same", "Wrong"))),
    task = factor(task, levels=c("cd", "ab"), labels=c("Aligned", "Unaligned")),
    chart = factor(chart, levels=c("Vertical", "Horizontal", "Horizontal wide"))
  ) %>% 
  ggplot(aes(x = racethnicity)) + 
  facet_grid(.~task) + 
  geom_bar(aes(fill = response3, weight = weight), position="fill") +
  theme(legend.position="none",
        axis.text.x = element_text(angle=30, hjust=1)) + 
  scale_fill_manual("Response", values=c("orange", "grey", "steelblue")) +
  xlab("") +
    ggtitle("Accuracy by race, ethnicity and task") +
  scale_y_continuous("Percentage", breaks=seq(0,1, by=0.25), labels = seq(0,1, by=0.25)*100)

age_cd <- round_123_long %>% 
  filter(task=="cd") %>%
  mutate(
    response3 = c("Wrong", "They are the same", "Correct")[correct+correct3+1],
    response3 = factor(response3, levels=rev(c("Correct", "They are the same", "Wrong"))),
    task = factor(task, levels=c("cd", "ab"), labels=c("Aligned", "Unaligned")),
    chart = factor(chart, levels=c("Vertical", "Horizontal", "Horizontal wide"))
  ) %>% 
  ggplot(aes(x = age4)) + 
   facet_grid(.~task) + 
 geom_bar(aes(fill = response3, weight = weight), position="fill") +
  theme(legend.position="none",
        axis.text.x = element_text(angle=30, hjust=1)) + 
  scale_fill_manual("Response", values=c("orange", "grey", "steelblue")) +
  xlab("") +
    ggtitle("\nby age") +
  scale_y_continuous("Percentage", breaks=seq(0,1, by=0.25), labels = seq(0,1, by=0.25)*100)

age_ab <- round_123_long %>% 
  filter(task=="ab") %>%
  mutate(
    response3 = c("Wrong", "They are the same", "Correct")[correct+correct3+1],
    response3 = factor(response3, levels=rev(c("Correct", "They are the same", "Wrong"))),
    task = factor(task, levels=c("cd", "ab"), labels=c("Aligned", "Unaligned")),
    chart = factor(chart, levels=c("Vertical", "Horizontal", "Horizontal wide"))
  ) %>% 
  ggplot(aes(x = age4)) + 
   facet_grid(.~task) + 
 geom_bar(aes(fill = response3, weight = weight), position="fill") +
  theme(legend.position="none",
        axis.text.x = element_text(angle=30, hjust=1)) + 
  scale_fill_manual("Response", values=c("orange", "grey", "steelblue")) +
  xlab("") +
    ggtitle("\nby age") +
  scale_y_continuous("Percentage", breaks=seq(0,1, by=0.25), labels = seq(0,1, by=0.25)*100)
gender_cd + age_cd  + educ_cd +income_cd + plot_layout(nrow=1, guides = "collect")

gender_ab + age_ab  + educ_ab +income_ab + plot_layout(nrow=1, guides = "collect")
```

```{r demographics-model}
logit_demo_task_cd_correct <- svyglm(correct~gender+age4+educ5+income4, design=survey_123, family=quasibinomial(), subset = task=="cd")

logit_demo_task_cd_correct_same <- svyglm(correct3~gender+age4+educ5+income4, design=survey_123, family=quasibinomial(), subset = task=="cd")

logit_demo_task_ab_correct <- svyglm(correct~gender+age4+educ5+income4, design=survey_123, family=quasibinomial(), subset = task=="ab")
logit_demo_task_ab_correct_same <- svyglm(correct3~gender+age4+educ5+income4, design=survey_123, family=quasibinomial(), subset = task=="ab")

coefs_cd_correct <- broom::tidy(logit_demo_task_cd_correct)
coefs_cd_correct_same <- broom::tidy(logit_demo_task_cd_correct_same)

coefs_ab_correct <- broom::tidy(logit_demo_task_ab_correct)
coefs_ab_correct_same <- broom::tidy(logit_demo_task_ab_correct_same)
```

\blandscape

```{r demographics-table}
#| label: tbl-demographics
#| tbl-cap: Demographics matter for perception, particularly when the tasks get harder.

 coefs_clean <- function(dframe) {
  dframe %>% mutate(
    variable = c("Intercept", "Gender", rep("Age", 3), rep("Education", 4), rep("Income", 3)),
    term = ifelse(term=="(Intercept)", "", term),
    term = gsub("gender", "", term),
    term = gsub("educ5", "", term),
    term = gsub("age4", "", term),
    term = gsub("income4", "", term),
    ci_low = exp(estimate-1.96*std.error),
    ci_high = exp(estimate+1.96*std.error),
    estimate = exp(estimate)
  )
}

coefs_clean(coefs_cd_correct) %>% mutate(task = "Aligned", correct="correct") %>%
 rbind(
  coefs_clean(coefs_ab_correct) %>% mutate(task = "Unaligned", correct="correct")
 ) %>%
 rbind(
  coefs_clean(coefs_cd_correct_same) %>% mutate(task = "Aligned", correct="not wrong")
 ) %>%
 rbind(
  coefs_clean(coefs_ab_correct_same) %>% mutate(task = "Unaligned", correct="not wrong")
 ) %>%
 dplyr::select(-std.error, -statistic) %>%
 mutate(
  p.value = ifelse (p.value <  0.001, "***",
      ifelse (p.value <  0.01, "**",
             ifelse (p.value < 0.05, "*",
                    ifelse (p.value < 0.1, ".", ""))))
 ) %>%
 group_by(variable) %>%
 pivot_wider(
    id_cols = c(term, variable),
    names_from = c(task, correct),
    values_from = c(estimate, ci_low, ci_high, p.value),
  ) %>% 
 gt() %>%
  tab_header(title = "Odds of accuracy by task and demographics of respondents") %>%
  tab_footnote("Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1") %>%
 tab_options(
   column_labels.font.weight = "bold",
   row_group.font.weight = "bold",
   data_row.padding.horizontal = px(5),
   page.orientation = "landscape"
 ) %>%
  fmt_number(
    decimals=2
  ) %>%
  cols_width(
    1~px(50),
    contains("p.value") ~ px(10)
  ) %>%
  cols_align(
    align = "left",
    columns = contains("p.value")
  ) %>%
  cols_merge(
    columns = contains("ci") & contains("_Aligned_correct"),
    pattern = "<< [{1}, {2}]>>"
  ) %>%
  cols_merge(
    columns = contains("ci") & contains("_Aligned_not"),
    pattern = "<< [{1}, {2}]>>"
  ) %>%
  cols_move_to_end(contains("_Aligned_")) %>%
  tab_spanner(
    label = "correct | same or wrong",
    columns = contains("_Aligned_correct")
  )  %>%
  tab_spanner(
    label = "correct or same | wrong",
    columns = contains("_Aligned_not")
  ) %>%
  tab_spanner(
    label = "Aligned tiles",
    columns = contains("_Aligned_"),
    spanners = c("correct | same or wrong", "correct or same | wrong")
  ) %>%
  cols_move_to_end(contains("_Unaligned_")) %>%
  cols_merge(
    columns = contains("ci") & contains("_Unaligned_correct"),
    pattern = "<< [{1}, {2}]>>"
  ) %>%
  cols_merge(
    columns = contains("ci") & contains("_Unaligned_not"),
    pattern = "<< [{1}, {2}]>>"
  ) %>%
  tab_spanner(
    label = "correct | same or wrong ",
    columns = contains("_Unaligned_correct")
  )  %>%
  tab_spanner(
    label = "correct or same | wrong ",
    columns = contains("_Unaligned_not")
  ) %>%
  tab_spanner(
    label = "Unaligned tiles",
    columns = contains("_Unaligned")
  ) %>%
   cols_label(
    .list = c(
      "term" = " ",
      "estimate_Aligned_correct" = "Est.",
      "ci_low_Aligned_correct" = "          [95% CI]",
      "p.value_Aligned_correct" = " ",
      "estimate_Aligned_not wrong" = "Est.",
      "ci_low_Aligned_not wrong" = "          [95% CI]",
      "p.value_Aligned_not wrong" = " ",
      "estimate_Unaligned_correct" = "Est.",
      "ci_low_Unaligned_correct" = "          [95% CI]",
      "p.value_Unaligned_correct" = " ",
      "estimate_Unaligned_not wrong" = "Est.",
      "ci_low_Unaligned_not wrong" = "          [95% CI]",
      "p.value_Unaligned_not wrong" = " "
    )
  ) 

```

\elandscape

<!-- Results to discuss: -->

<!-- There are differences by alignment -->
<!-- There are not huge differences across structures -->
<!-- However, we see a difference in how people react across different structures (zooming behavior, certainty? time?) -->

<!-- - We can reproduce prior findings with nat rep sample  -->
<!-- - We can rank  -->
<!-- - We can rank better because we have a bigger sample size  -->
<!-- - We can find changes in viewer behavior  -->
<!-- - Demographics matter to accuracy and choices -->

# Conclusions

Through this work, we have tested the use of probability-based survey panels to ask perception questions; within a large survey covering a variety of topics, and with a limited number of questions for our specific tasks, we are able to measure viewer perception and produce results consistent with prior studies. Testing data visualization design structures and viewer behavior on a nationally-representative sample is of broad interest and applicability to the scientific communication community, and this study demonstrates a framework under which to complete further work in this area. 

With our survey framework, we have shown that we can rank structural variations on design and tasks by their relative accuracy; we identified significantly different rates of respondent accuracy under different designs presenting the same data. The size of our sample aids in identifying this signal.  

Further, we can also study respondent behavior and how viewers interact with the chart. Paradata on viewer interaction with the chart (e.g., zooming and time spent on each task) can be collected as well as asking directly asking respondents questions about their certainty. Not only can we collect this information, but we can also identify significant differences in this behavior across different designs. Understanding viewer interaction and engagement is a key component to designing effective data visualizations. If a particular design leads viewers to zoom in more to investigate it when determining a response, we might consider that design to be more frustrating to viewers 'in the wild' when viewing it outside of the context of our tests. \kr{I don't love the last sentence but I think we can clean it up}.

Perhaps most salient among our results are the patterns in response behavior across demographic groups. The large size of our sample, paired with the probability-based survey panel approach, afford us the ability to dive into response patterns among demographics across tasks and identify significant differences among them. In particular, the differences across educational attainment groups and income groups underscore the importance of utilizing a representative population when testing perception; prior results in this area may be absorbing bias in responses due to the larger representation of individuals with higher education levels among those study respondents. 


While our results are limited to mapping in a stacked bar chart with a specific topic, they demonstrate among them a wealth of insights about how respondents perceive and interact with data visualizations. More expansive work on this topic should be completed to understand how the general public interacts with and understands charts, including expanding to a wider set of structural variations, more aesthetic design variations, and considering differences in respondent behavior across different data topic areas. 

<!-- Key findings to discuss: -->

<!-- -   Surveys can be used to ask these types of questions -- in the midst of other topics and with a limited number of questions, we are able to ask perception questions and produce results consistent with prior studies -->

<!-- -   Reproduced (some) prior convenience sample results using a large, nationally-representative survey population: -->

<!--     -   Cleveland: position (aligned) versus unaligned -->
<!--     -   unaligned tiles within stacked bar are a hybrid between framed bar and floats -->
<!--     -   Talbot: pie charts - individual wedges vs piecharts -->

<!-- -   Design choices impact viewer accuracy: -->

<!--     -   new finding: pie charts accuracy in work better when framed (wedges) -->

<!-- -   Other important measures beyond accuracy: time to completion (automatic on web), certainty in response (asked). -->

<!-- -   Studying how viewers interact with charts can be done in a survey, and more expansive work on this topic should be completed to understand how the general public interacts with and understands charts. -->

<!--     -   Design choices impact viewer behavior: zooming -->

<!-- -   (some) Demographics matter: age and gender does not seem to matter for perception economic background and education does matter for perception: we need to know, we do not want to create a hurdle in communication -->

# Supplementary Material {.unnumbered}

-   **Participant Data (Linear):** Link to csv file with the data.
-   **Data Analysis Code:** Link to an html document with annotated code chunks.

# References
