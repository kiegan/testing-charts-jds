---
title: "Testing Charts: viewer's perceptual accuracy in surveys"
short-title: Testing charts in surveys # running title
author:
  - first: Kiegan                 # first name
    first-init: K.              # initial of first name
    last: Rice                  # last name
    marker: 1               # identifier for the affiliation
    footnote-id: 1              # optional identifier for the footnote
    email: foo@bar.com          # optional email address
  - first: Heike
    first-init: H.
    last: Hofmann
    marker: 2
  - first: Nola
    first-init: N.
    last: duToit
    marker: 1
  - first: Edward
    first-init: E.
    last: Mulrow
    marker: 1
footnote:
  - content: Corresponding author
    type: corresp               # set type for the corresponding author
    id: 1
affiliation:
  - institution: National Opinion Research Center (NORC)
    country: United States
    marker: 1
  - institution: Iowa State University
    department: Statistics
    country: United States
    marker: 2
bibliography: references.bib
abstract: "The use of visuals is a key component in scientific communication, and decisions about the design of a data visualization should be informed by what design elements best support the audience's ability to perceive and understand the components of the data visualization. We build on the foundations of Cleveland and McGill's work in graphical perception, employing a large, nationally-representative, probability-based panel of survey respondents to test perception in statistical charts. Our findings provide actionable guidance for data visualization practitioners to employ in their work."
keywords: 'list, keywords, here, 3-6, are, fine'
preamble: >
  \usepackage{amsfonts,amsmath,amssymb,amsthm}
  \usepackage{booktabs}
  \usepackage{graphicx}
  \usepackage{hyperref}
format:
  html: default
  pdf: 
    documentclass: jds
    include-in-header: authors.tex
    keep-tex: true
editor: 
  markdown: 
    wrap: sentence
---

```{=latex}
  \newcommand{\hh}[1]{{\textcolor{orange}{#1}}}
  \newcommand{\kr}[1]{{\textcolor{teal}{#1}}}
  \setlength{\parindent}{0pt}
```
<!-- fig-width: 6.5 -->

<!-- fig-height: 4.5 -->

<!-- template: template-jdsart.cls.tex -->

```{r run-to-update-bibfile, eval=FALSE, include=FALSE}
keys <- rbbt::bbt_detect_citations("testing-charts-accuracy.qmd")
citations_only <- grep(pattern="[fig|tbl|eq][-|:].*", keys, value=TRUE, invert=TRUE)
rbbt::bbt_write_bib(keys=citations_only, 'references.bib', overwrite = TRUE,
                    library_id=rbbt::bbt_library_id('Graphics Research'),
                    translator='bibtex')
```

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = F, 
                      eval = T, 
                      fig.path="./figures/", # figures are created on the fly, images are pictures like the stimuli
                      fig.env="figure*", 
                 #     fig.align = "center",
                      out.width = "\\textwidth",
                #      fig.height = 5,
                 #     fig.width = 5,
                      dev = "png", dev.args = list(type = "cairo-png")
#                      dpi = 72#, # we can set the resolution up to 300 later.
                      #cache = TRUE) 
)

knitr::opts_chunk$set(
  jds.rmd::pdf_article()$knitr
)


library(readr)
library(tidyverse)
library(knitr)
library(bibtex)
theme_set(theme_bw())

library(readxl)
library(lubridate)
library(patchwork)
library(survey)
#library(ggmosaic)
library(ggpcp)
library(broom)
library(latex2exp)
library(gt) # grammar of tables
library(here)

# Kish's Effective Sample Size
neff <- function(weight) {
  n <- length(weight)
  L <- var(weight)/mean(weight)^2
  n/(1+L)
}

# inverse of the logistic function
inv_logit <- function(x) {exp(x)/(1+ exp(x))}

# calculate lambda for combining weights
calculate_lambda <- function(weights1, weights2) {
  d1 <- 1+var(weights1)/mean(weights1)^2
  d2 <- 1+var(weights2)/mean(weights2)^2

  n1 <- length(weights1)
  n2 <- length(weights2)

  denom <- n1/d1 + n2/d2

  lambda <- n1/(d1*denom) 
  lambda
}

vs_repo_local <- here::here("data") # link to data folder
```

```{r data}
# other github repo. 
round1 <- readRDS(file.path(vs_repo_local, "round1.rds"))
round2 <- readRDS(file.path(vs_repo_local, "round2.rds"))

round1_adj <- round1 %>% 
            filter(ab != "SKIPPED ON WEB", cd != "SKIPPED ON WEB") %>%
            mutate(CorrectAB = ab=="B is bigger", CorrectCD = cd=="D is bigger",
                   CorrectABCD = paste(CorrectAB, CorrectCD),
                   one = 1)

round2_adj <- round2 %>% 
            filter(ab != "SKIPPED ON WEB", cd != "SKIPPED ON WEB") %>%
            mutate(CorrectAB = ab=="B is bigger", CorrectCD = cd=="D is bigger",
                   CorrectABCD = paste(CorrectAB, CorrectCD),
                   one = 1)

survey1 <- survey::svydesign(
  data = round1_adj, 
  ids = ~CaseId, weights = ~weight)

survey2 <- survey::svydesign(
  data = round2_adj,
  ids = ~CaseId, weights = ~weight)
```

## Introduction

<!-- What is our main objective? -->

\kr{Should the abstract match -- or be close to -- our SDSS short abstract?}

What do viewers see when we show them a data chart?
A data chart -- at its core -- maps quantitative values to graphical elements representing their relative values.
Modern data visualizations are much more than a simple, objective mapping of values to a plane; they contain contextual and design elements, and are often structured to support the viewer in understanding a particular view of a set of data or specific pattern underlying the values.
The design of a data visualization impacts a viewer's ability to achieve that understanding; a poorly designed data visualization may leave viewers struggling to understand the content or context, or make it difficult to complete accurate and useful comparison of values across groups or time points.
More broadly, the design of a data visualization can change how viewers interact with the chart.

A crucial step in the process of interacting with and understanding a chart is the viewer's employment of comparisons of the parts within.
@clevelandGraphicalPerceptionTheory1984 observed as such, and in their seminal study defined the better visual among a pair as the one that allows viewers to make more accurate comparisons.
Based on mappings of quantitative variables to different graphical elements, Cleveland and McGill's study resulted in a ranking of perceptual tasks from most accurate to least accurate, which was then extended by @mackinlayAutomatingDesignGraphical1986 to a theoretical framework ranking tasks' order along their ordinal and nominal scales, as shown in @fig-perceptual-tasks.

Cleveland and McGill's work -- while a foundational user study in graphical perception -- utilized a small convenience sample, consisting of only a few individuals recruited from among the authors' coworkers and their spouses.
@heerCrowdsourcingGraphicalPerception2010 reproduced Cleveland and McGill's rankings using a larger sample from a crowd sourcing platform.
A total of XXX Amazon Mechanical Turkers were involved.
Crowd sourced samples were shown by @borgoCrowdsourcingInformationVisualization2017 to be biased towards more male, younger, and relatively higher education relative to the adult U.S. population as a whole.
These study populations are thus not representative of the general population, a common target audience for data visualization and scientific communication work.
Further, the populations' emphasis on higher education individuals also leads to results which hold for groups of individuals who may be more likely to have prior exposure to data visualization in the context of scientific communication, or more exposure to data topics in higher education, but may not hold across other groups within the population.
\kr{I don't know if the prior statement is going too far? Maybe we can rephrase.}

<!-- Bias from crowd-sourcing? sample size?  -->

\kr{The mackinlay thing feels like maybe we are overemphasizing the ranking done by cleveland/mcgill and then mackinlay - I think we could emphasize less if we want}

Our work -- as that of Cleveland and McGill and Heer and Bostock -- centers around studying data visualization design choices and their impact on viewer behavior and accuracy of viewers' responses.
We seek to answer whether it is possible to reproduce some of their findings in the context of a survey with a large, nationally-representative set of respondents, and within that context we focus on the following research questions:

1.  How do structural design choices in a data visualization impact viewers' ability to identify the larger of two elements?

2.  How is viewer interaction with the task impacted by structural design choices in a data visualization?

We employ a probability-based survey panel and run a series of perception tests with nationally-representative samples of respondents from that panel.
The advantage of using a probability-based approach is two-fold.
First, we have access to a large sample of survey participants and thus have greater power in making inference about graphical perceptional abilities.
Second, the sample is representative of the general adult public in the U.S., which is an important target audience for scientific communication, and this allows us to test whether prior results from convenience samples hold with a nationally representative sample.
\kr{Do we want a stronger statement about this here?}

We present viewers with structural variations on bar charts and pie charts and ask them to answer questions comparing the size of elements within those charts.
In this work, we present the series of tests we completed and the resulting findings.
The remainder of the paper is organized as follows: first, we describe the design of the visual stimulus used in our perception tests.
We then describe the population of study respondents and obtained survey sample.
Subsequently, we share our analyses of the resulting survey responses across each of our tests, including analyses on accuracy of responses and response behavior.
Finally, we discuss implications of this work and next steps.

```{r}
#| label: fig-perceptual-tasks
#| fig-cap: Ranking of perceptual tasks, as given by @mackinlayAutomatingDesignGraphical1986. The ranking of tasks on the quantitative scale are empirically verified by @clevelandGraphicalPerceptionTheory1984.
knitr::include_graphics("images/mackinlay-tasks.png")
```

\kr{The figure below is still a bit of an orphan, but I think we can revisit the intro ad study setup after we get more of the results in the paper}

```{r}
#| label: fig-aligned-unaligned
#| fig-cap: "The only difference between the two pairs of rectangles A, B and C, D is their alignment, i.e. A and C are of identical size, as are B and D. When participants are asked to compare the size of these tiles in barcharts, the predominant response for the unaligned pair on the left is 'they are of the same size'. In contrast, more than half of the viewers respond with 'D is bigger' to the aligned pair of bars on the right."
#| fig-subcap:
#| - "**Unaligned bars**"
#| - "**Aligned bars**"
#| layout-nrow: 1
#| fig-align: "center"
#| out-width: 50%
knitr::include_graphics("images/round5-modified.PNG")
knitr::include_graphics("images/round3-modified.PNG")
```

## Study design -- stimulus

Each task is made up of two elements: a visual stimulus and a question about that image that the viewer is asked to respond to.
In our study, each visual stimulus is an image of a data visualization, while each question prompts viewers to identify which of two marked pieces in the data visualization is larger.

**What specifically is each task?** Each comparison between marked pairs is designed to be a difficult task, with the difference between the values represented in the two marked pieces being a just-noticeable difference.
The **Just-Noticeable Difference** (JND) is defined as the smallest difference that will be detected 50% of the time.
Prior results from studies on bar charts and pie charts [@luModelingJustNoticeable2022] inform the differences in charts shown to our survey panelists.

**Why do we utilize just-noticeable differences?** We employ comparisons at the JND in our tasks in order to maximize our ability to identify the impact of design changes on viewer accuracy and behavior.
Asking perception tasks in a survey differs from the controlled environment of a cognitive lab, where these kind of questions may usually be assessed.
Rather than asking the same (or similar) type of question with varied signal strength dozens or hundreds of times, we are limited to only a few questions at a time.
With a small set of tasks, we need to present tasks that are perceptually hard, and thus ask questions about stimuli that are close to our perceptual threshold.
Therefore, we focus on questions which vary the presented image, but ask viewers to compare the same values across those varied images.

**How do we vary the task?** We ask participants to determine which of two just-noticeably different marked pieces is larger within the data visualization image, and we vary the structural design of that data visualization image.
We focus on three main sets of structural variation in the design.
First, we vary the alignment of the pieces in question.
Viewers are presented with two marked pieces in a chart that do not share a common baseline, then two pieces that do share a common baseline.
Second, we vary the orientation of the chart -- a vertically oriented stacked bar chart, a horizontally oriented version of the same chart, and then a wider version of the horizontally oriented chart.
\kr{We need to finalize what we end up showing in the results and then revisit this section...} Third, we consider a facetted bar chart and facetted pie chart.

\hh{Not sure where should put this, parking it here for right now:}
\hh{expectations - start}
What we call 'aligned' and 'unaligned', here, is similar to Cleveland and McGill's set of rankings, but with some modifications:
both 'aligned' and 'unaligned' bars (in tests 1 and 2) or wedges (in test 3) share the same axis. Aligned tiles are additionally anchored in the same position in one dimension, i.e. the difference between their sizes can be reduced to a positional assessment. Unaligned tiles do not share this anchor, however, the context of the other tiles in the chart provide a frame, which *should* help with an assessment of the tiles' sizes beyond a comparison of (arc) lengths or areas.   
We would expect that comparing unaligned tiles is a harder task (with correspondingly lower levels of accuracy) than a comparison of aligned tiles, with the framing given by the context of the other tiles in the same column (or the same pie) mitigating some of this difficulty. @fig-tasks-explain gives an overview of the comparisons of tasks 1 through 3 and the closest corresponding tasks in Cleveland and McGill. 

```{r}
#| label: fig-tasks-explain
#| fig-cap: Comparisons made in charts in the Cleveland and McGill ranking

cat("still working on the chart")
```

\hh{expectations - end}


**How do we present the task?** The format of a survey guides the format and design of the questions asked and how they are presented to respondents.
First, participant instructions must be delivered in a very short and easily understandable format, because participants cannot ask clarifying questions about the task as they might be able to in a cognitive lab setting.
\kr{I'm not sure how important this second point is...} Second, participants should be given some context for the tasks they are being asked to complete; in a given round, our set of tasks appear as a group in the midst of other survey questions and topics, and without providing respondents some transition we risk a jarring shift and low respondent engagement in the task.
Finally, to prevent viewers from being exposed to slight variations of the same stimulus in a row (and risk unforeseen order effects or respondents using prior questions to inform their responses), we either split a survey sample in two and show each subsample a distinct version of the chart or test variations of a chart across distinct rounds of the survey.

\kr{Somewhere here we should also talk about how we measure viewer behavior/interaction with the chart: certainty, time spent, zooming, devices, etc. }

### Test 1: Alignment in stacked bar charts

\autoref{fig-tasks-12} shows the two stacked bar charts shown to participants in the first test.
The marked tiles in each plot are 155 pixels apart.
Based on @luModelingJustNoticeable2022's model, a difference of 155 pixels leads to a just noticeable difference of 3.5 pixels.
The heights of the bars are 205 (left) and 213 pixels (right), respectively, corresponding to about twice the JND.
This difference should lead to a relatively high accuracy rate for participants and simultaneously limit the amount of frustration resulting from a task that is perceived as 'too hard'.

<!-- Lu et al calculation: log_{10}(JND) = -.4653 + .0065 distance -->

<!-- describe stimulus -->

Both charts in \autoref{fig-tasks-12} show the same data with slight modifications to the order of the levels -- the first and second level in each of the bars are reversed between the left and the right chart.
Participants were asked to compare the relative sizes of the tiles marked A and B (C and D, respectively) and select the correct response out of the possible choices:

1.  A is bigger
2.  B is bigger
3.  They are the same

Answer 2 is the correct answer for both charts.
Both charts are shown at the same size, i.e. in both cases the difference in size between the bars is exactly the same, the vertical distance between the bars is the exact same amount.
This leaves the vertical positioning of the bars as the only difference between the charts.
Any differences in observed accuracy can therefore be attributed to this difference in presentation.

```{r}
#| label: fig-tasks-12
#| fig-cap: "Two stacked (vertical) barcharts. In each barchart, two tiles are marked. In both instances, the tile on the right is (very slightly) larger. "
#| fig-subcap:
#| - "**Unaligned bars**"
#| - "**Aligned bars**"
#| layout-nrow: 1
#| out-width: 100%
knitr::include_graphics("images/vertical-ab.PNG")
knitr::include_graphics("images/vertical-cd.PNG")
```

<!-- |                                   |                                   | -->

<!-- |-----------------------------------|-----------------------------------| -->

<!-- | **Unaligned positions**           | **Aligned positions**             | -->

<!-- | ![Unaligned positions marked A and B](../SDSS-2023/images/vertical-ab.PNG) | ![Aligned positions marked C and D](../SDSS-2023/images/vertical-cd.PNG) | -->

<!-- : The two stacked barcharts every participant got to see. In each barchart, the two marked tiles are to be compared for their size. in both instances, the tile on the right is (very slightly) larger. {#tbl-tasks-12 tbl-colwidths="\[50,50\]"} -->



### Test 2: Orientation of stacked bar charts

Let's talk about the stacked bar flippy setup here

```{r}
#| label: fig-tasks-3
#| fig-cap: "Changes to to the vertical design are made in two steps. First, the original chart is rotated. The areas of all tiles are kept the same. In a second step, the aspect ratio of tiles is changed while keeping the areas of tiles the same."
#| fig-subcap:
#| - "**Tall horizontal bars**, unaligned"
#| - "**Wide horizontal bars**, unaligned"
#| layout-nrow: 1
#| out-width: 100%
knitr::include_graphics("images/round3/horizontal-ab.PNG")
knitr::include_graphics("images/round3/hwide-ab.PNG")
```

### Test 3: Alignment and orientation in pie charts

Let's talk about the pies here

## Study design - survey population

Participants were recruited as part of NORC's AmeriSpeak panel, which utilizes a probability-based sampling methodology and samples U.S. households from NORC's National Sample Frame that provides coverage of over 97% of U.S. households.
The current panel size is 54,001 panel members aged 13 and over residing in over 43,000 households **CITE DENNIS 2022**.
Each test was conducted using the AmeriSpeak Omnibus survey, which runs biweekly and samples around 1,000 U.S. adults to answer questions on a variety of topics.

\hh{XXX I think what you are trying to say is that we do not know, if participants have seen the stimuli multiple times, but if they did, there was at least a month in between?} \kr{Yes that's what I was trying to say here!!}

Given the nature of pulling a sample from the panel for each Omnibus round, there is a possibility that some participants may have been included in multiple rounds; however, our collected data is not a longitudinal or panel study, so we do not have repeated responses from the same participants across each of our tests.
Each of the tests was run at least a month apart, so if respondents participated in multiple rounds, there was at least a one month gap between viewing each visual stimulus.

# Notes on Analysis

I need a better name for this section...

Paragraph on applying survey weights

-   All calculations in this paper are done in R [@RLanguage] using the `survey` package [@lumleyAnalysisComplexSurvey2004] version 4.0 [@survey] based on @lumleyComplexSurveysGuide2010.

Paragraph on combining samples across rounds

-   Pull from analysis doc

-   When combining responses from multiple rounds, we make weighting adjustments to ensure that weights in each sample are properly calibrated to the population total across both rounds so each round is correctly represented in the resulting model [@omuircheartaighCombiningSamplesVs2002].
    **NEED ED REVIEW OF THIS LANGUAGE.** This adjustment is completed as follows: \[insert Heike text from analysis doc here\].

# Results

## Respondents

Description of respondents

-   \# of respondents in each round

-   broad overview of demo characteristics(?)

-   raw sample size and effective sample size

### Test 1

-   Alignment in stacked bars

    -   Accuracy and responses

    -   Differences in accuracy across demographic groups

### Test 2

-   Structure of stacked bars -- vertical, horizontal, horizontal wide
    -   Accuracy and responses

    -   

### Test 3

-   Facetted bars and pies
    -   Accuracy and responses

    -   Timing of responses

Results to discuss:

-   Differences in accuracy between aligned and unaligned bar

-   Differences in accuracy between framed, floating, and general pie

-   Differences in timing and zooming behavior across all designs

```{r lambdas}
n1 <- nrow(round1_adj)
n2 <- nrow(round2_adj)
neff1 <- svytotal(~one, survey1, deff=TRUE)
neff2 <- svytotal(~one, survey2, deff=TRUE)
#abcd1 <- svytotal(~CorrectABCD, survey1, deff=TRUE)
#abcd2 <- svytotal(~CorrectABCD, survey2, deff=TRUE)

d1 <- 1 + cv(neff1)^2
d2 <- 1 + cv(neff2)^2

denom <- n1/d1 + n2/d2

lambda <- n1/(d1*denom) 
```

```{r combine_rounds_1_and_2}
round_12 <- round1_adj %>% 
            mutate(weight = lambda[1]*weight) %>% 
            select(CaseId, ab, cd, CorrectAB, CorrectCD, CorrectABCD, weight) %>%
    rbind(round2_adj %>% 
            mutate(weight = (1-lambda[1])*weight) %>% 
            select(CaseId, ab, cd, CorrectAB, CorrectCD, CorrectABCD, weight))
round_12_long <- round_12 %>% 
  pivot_longer(CorrectAB:CorrectCD, names_to = "Question", values_to = "Response") 

survey_12 <- 
  survey::svydesign(
  data = round_12_long, 
  ids = ~CaseId, weights = ~weight)
```

```{r ttest}
paired <- svyttest(Response~Question, survey_12)
abcd <- svyby(~Response, by=~Question, survey_12, svytotal)

marginal_results <- data.frame(abcd)
```

The data used for assessing the accuracy of comparisons in \autoref{tbl-tasks-12} is collected in two rounds of the NORC Omnibus survey.
Rounds 1 and 2 are combined by adjusting the weights with $\lambda$ = `r round(lambda[1],3)` for an effective sample size of `r round(neff(round_12$weight),1)`.
Figure \ref{fig-alignment}(a) shows that more than twice the number of responses is accurate, when the tiles are aligned along the same axis.
Because each participant was shown both versions of the chart, we can use a paired $t$-test to compare mean accuracy between the two charts.
The resulting $t$-statistic is highly significant ($t$ statistic: `r round(paired$statistic,1)`, df: `r paired$parameter`, $p$-value: \< 2.2e-16).

```{r accuracy,   warning=FALSE}
#| fig-cap: "On the left (a), a stacked barchart shows the number of respondents with correct (green) and incorrect (grey) responses to the two comparison questions. When tiles are aligned along the same axis, more than twice the number of responses is accurate. The shaded area along the top of the green tiles corresponds to  95\\% confidence intervals around (marginal) correct responses. On the right (b), a parallel coordinate plot shows all combinations of responses. There's a huge asymmetry in the number of responses, where participants answered only one of the questions correctly. A lot more responses are correct when comparing aligned tiles than unaligned tiles."
#| label: fig-alignment
#| out-width: 80%
#| fig-height: 5 
#| fig-width: 10

gg1 <- round_12_long %>%
  mutate(
    Response = ifelse(Response, "Correct", "Not correct"),
    Response = factor(Response, levels=c("Not correct", "Correct")),
    Question = ifelse(Question=="CorrectAB",
                      "Unaligned: A or B?", "Aligned: C or D?")
  ) %>%
#  filter(Response == "Correct") %>%
  ggplot(aes(x = Question, fill = Response, weight = weight)) +
  geom_bar() +
  ylab("Number of (effective) Respondents") +
  xlab("") +
  scale_fill_manual(values=c("grey", "forestgreen")) +
  scale_y_continuous(
      sec.axis = sec_axis(~./sum(round_12$weight)*100,
                          name="Percent responses")
  ) +
  theme(legend.position = "bottom") +
  ggtitle("(a) Accuracy of survey responses")  +
  geom_rect(
    aes(
      x = rev(c("Aligned: C or D?", "Unaligned: A or B?")),
      xmin = 2:1 - 0.45,
      ymin = ResponseTRUE - 1.96* se.ResponseTRUE,
      xmax = 2:1 + 0.45,
      ymax = ResponseTRUE + 1.96* se.ResponseTRUE,
      weight = 1),
    fill = "black", alpha = 0.25, #colour = "white", linewidth = 0.25,
    data = marginal_results
  )

gg2 <- round_12 %>%
  mutate(
    `Unaligned: A or B?` = ifelse(CorrectAB, "Correct", "Not correct"), 
    `Aligned: C or D?` = ifelse(CorrectCD, "Correct", "Not correct")
    ) %>%
  pcp_select(`Aligned: C or D?`, `Unaligned: A or B?`) %>%
  pcp_scale() %>%
  pcp_arrange() %>%
  ggplot(aes_pcp()) + 
    geom_pcp_axes() + 
    geom_pcp(aes(colour = CorrectABCD), alpha = 0.2)+
    geom_pcp_boxes(fill=NA, colour = "black", linewidth=0.25) + 
    geom_pcp_labels(colour = "black") +
  scale_y_continuous("Number responses", breaks=seq(0,1,by=0.25), 
                     labels = round(sum(round_12$weight)*seq(0,1,by=0.25),0),
                     sec.axis = sec_axis(~.*100,
                          name="Percent responses")) +
  xlab("") + 
  scale_colour_manual(values = c("grey", "#84AC84", "#70A570", "forestgreen")) +
  ggtitle("(b) Combinations of responses") +
  theme(legend.position = "none")

gg1 + gg2
```

ANALYSIS PLAN:

Models below structured as:

Response

Covariates to use in each model

STRUCTURAL VARIATION -- START HERE

Binary accuracy across structural choices

Model 1:

Alignment only, just vertical stacked bar

Using the AmeriSpeak survey tool, a total of 1902 participants were exposed to two barcharts each, as shown in \autoref{tbl-tasks-12}.

Model 2:

Alignment

Bar vs pie (comparable question for pie is A vs B)

Model 3:

Alignment

Vertical x horizontal x horizontal wide

Model 4:

Alignment

Every structure (vertical bar, horizontal bar, horizontal wide bar, facet bar, pie)

Visuals:

\% yes across each different structural condition

Facet by aligned/unaligned?

Model estimates + CIs

Ordinal response

Model 1:

Alignment only, just vertical stacked bar

Model 2:

Alignment

Bar vs pie (comparable question for pie is A vs B)

Model 3:

Alignment

Vertical x horizontal x horizontal wide

Model 4:

Alignment

Every structure (vertical bar, horizontal bar, horizontal wide bar, facet bar, pie)

Visuals:

All responses across each different structural condition

Facet by aligned/unaligned?

Model estimates + CIs

Zooming behavior (zoomed/did not zoom)

Model 1:

Device type

Alignment

Vertical x horizontal x horizontal wide

Visuals:

\% zoomed by device + alignment (already have this chart)

Model estimates + CIs

Time spent on question (in seconds)

Model 1:

Device type

Zoom

Alignment

Vertical x horizontal x horizontal wide

Model 2 (this may not be feasible for comparison depending on what level the 'TOTALTIME' is captured at):

Device type

Zoom

Alignment

Every structure (vertical bar, horizontal bar, horizontal wide bar, facet bar, pie)

Visuals:

Distribution of time spent variable

Facet by device type, zoom, structural condition, alignment?
Play around with it

Average time spent by each of the conditions

Certainty?

Same models as above, but I'm not sure how we want to do the response.
Ordinal response?
Binary (certain or very certain vs everybody else)?

AESTHETIC VARIATION -- ONLY IF TIME

Binary accuracy (correct/incorrect -- 'they are the same' is incorrect here) across structural choices

Model 1:

Dark grid vs no grid (only have for horizontal wide)

Response choice (ordinal response)

Model 1:

Dark grid vs no grid (only have for horizontal wide)

Zooming behavior (zoomed/did not zoom)

Model 1:

Device type

Dark grid vs no grid

Time spent on question (in seconds)

Model 1:

Device type

Zoom

Dark grid vs no grid

Certainty?

Same models as above, but I'm not sure how we want to do the response.
Ordinal response?
Binary (certain or very certain vs everybody else)?

# Conclusion

Key findings to discuss:

-   Surveys can be used to ask these types of questions -- in the midst of other topics and with a limited number of questions, we are able to ask perception questions and produce results consistent with prior studies

-   Reproduced (some) prior convenience sample results using a large, nationally-representative survey population:

    -   Cleveland: position (aligned) versus unaligned
    -   unaligned tiles within stacked bar are a hybrid between framed bar and floats
    -   Talbot: pie charts - individual wedges vs piecharts

-   Design choices impact viewer accuracy:

    -   new finding: pie charts accuracy in work better when framed (wedges)

-   Other important measures beyond accuracy: time to completion (automatic on web), certainty in response (asked).

-   Studying how viewers interact with charts can be done in a survey, and more expansive work on this topic should be completed to understand how the general public interacts with and understands charts.

    -   Design choices impact viewer behavior: zooming

-   (some) Demographics matter: age and gender does not seem to matter for perception economic background and education does matter for perception: we need to know, we do not want to create a hurdle in communication

# Supplementary Material {.unnumbered}

-   **Participant Data (Linear):** Link to csv file with the data.
-   **Data Analysis Code:** Link to an html document with annotated code chunks.

# References
