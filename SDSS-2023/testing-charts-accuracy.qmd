---
title: "Testing Charts: viewer's perceptual accuracy in surveys"
short-title: Testing charts in surveys # running title
author:
  - first: Kiegan                 # first name
    first-init: K.              # initial of first name
    last: Rice                  # last name
    marker: 1               # identifier for the affiliation
    footnote-id: 1              # optional identifier for the footnote
    email: foo@bar.com          # optional email address
  - first: Heike
    first-init: H.
    last: Hofmann
    marker: 2
  - first: Nola
    first-init: N.
    last: duToit
    marker: 1
  - first: Edward
    first-init: E.
    last: Mulrow
    marker: 1
footnote:
  - content: Corresponding author
    type: corresp               # set type for the corresponding author
    id: 1
affiliation:
  - institution: NORC at the University of Chicago
    country: United States
    marker: 1
  - institution: Iowa State University
    department: Statistics
    country: United States
    marker: 2
bibliography: references.bib
abstract: "The use of visuals is a key component in scientific communication, and decisions about the design of a data visualization should be informed by what design elements best support the audience's ability to perceive and understand the components of the data visualization. We build on the foundations of Cleveland and McGill's work in graphical perception, employing a large, nationally-representative, probability-based panel of survey respondents to test perception in statistical charts. Our findings provide actionable guidance for data visualization practitioners to employ in their work."
keywords: 'list, keywords, here, 3-6, are, fine'
preamble: >
  \usepackage{amsfonts,amsmath,amssymb,amsthm}
  \usepackage{booktabs}
  \usepackage{graphicx}
  \usepackage{hyperref}
  \usepackage{setspace}
documentclass: jds
format:
  pdf: 
    include-in-header: authors.tex
    keep-tex: true
  html: default
editor: 
  markdown: 
    wrap: sentence
---

```{=latex}
  \newcommand{\hh}[1]{{\textcolor{orange}{#1}}}
  \newcommand{\kr}[1]{{\textcolor{teal}{#1}}}
  \setlength{\parindent}{0pt}
  \singlespacing
```
<!-- fig-width: 6.5 -->

<!-- fig-height: 4.5 -->

<!-- template: template-jdsart.cls.tex -->

```{r run-to-update-bibfile, eval=FALSE, include=FALSE}
keys <- rbbt::bbt_detect_citations("testing-charts-accuracy.qmd")
citations_only <- grep(pattern="[fig|tbl|eq][-|:].*", keys, value=TRUE, invert=TRUE)
rbbt::bbt_write_bib(keys=citations_only, 'references.bib', overwrite = TRUE,
                    library_id=rbbt::bbt_library_id('Graphics Research'),
                    translator='bibtex')
```

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = F, 
                      eval = T, 
                      fig.path="./figures/", # figures are created on the fly, images are pictures like the stimuli
                      fig.env="figure*", 
                 #     fig.align = "center",
                      out.width = "\\textwidth",
                #      fig.height = 5,
                 #     fig.width = 5,
                      dev = "png", dev.args = list(type = "cairo-png")
#                      dpi = 72#, # we can set the resolution up to 300 later.
                      #cache = TRUE) 
)




library(readr)
library(tidyverse)
library(knitr)
library(bibtex)
theme_set(theme_bw())

library(readxl)
library(lubridate)
library(patchwork)
library(survey)
#library(ggmosaic)
library(ggpcp)
library(broom)
library(latex2exp)
library(gt) # grammar of tables
library(here)
library(multcomp)

# Kish's Effective Sample Size
neff <- function(weight) {
  n <- length(weight)
  L <- var(weight)/mean(weight)^2
  n/(1+L)
}

# inverse of the logistic function
inv_logit <- function(x) {exp(x)/(1+ exp(x))}

# calculate lambda for combining weights
calculate_lambda <- function(weights1, weights2) {
  d1 <- 1+var(weights1)/mean(weights1)^2
  d2 <- 1+var(weights2)/mean(weights2)^2

  n1 <- length(weights1)
  n2 <- length(weights2)

  denom <- n1/d1 + n2/d2

  lambda <- n1/(d1*denom) 
  lambda
}

vs_repo_local <- here::here("data") # link to data folder
```

```{r data}
# other github repo. 
round1 <- readRDS(file.path(vs_repo_local, "round1.rds"))

round1_adj <- round1 %>% 
            filter(ab != "SKIPPED ON WEB", cd != "SKIPPED ON WEB") %>%
            mutate(CorrectAB = ab=="B is bigger", CorrectCD = cd=="D is bigger",
                   CorrectABCD = paste(CorrectAB, CorrectCD),
                   one = 1)

round2 <- readRDS(file.path(vs_repo_local, "round2.rds"))

round2_adj <- round2 %>% 
            filter(ab != "SKIPPED ON WEB", cd != "SKIPPED ON WEB") %>%
            mutate(CorrectAB = ab=="B is bigger", CorrectCD = cd=="D is bigger",
                   CorrectABCD = paste(CorrectAB, CorrectCD),
                   one = 1)

round3 <- readRDS(file.path(vs_repo_local, "round3.rds"))

# filter out all of the 'skipped on web':
round3_adj <- round3 %>% mutate(
  weight = WEIGHT_NOLA
) %>% filter(
  ab != "SKIPPED ON WEB",
  cd != "SKIPPED ON WEB"
)

round6 <- readRDS(file.path(vs_repo_local, "round6.rds"))

round6_adj <- round6 %>% mutate(
  weight = WEIGHT_NOLA
) %>% filter(RND_01 == 0) # same type of questions as round 7

round7 <- readRDS(file.path(vs_repo_local, "round7.rds"))

# filter out all of the 'skipped on web':
round7_adj <- round7 %>% mutate(
  weight = WEIGHT_NOLA
)
```

```{r data-rnd3}
round3 <- readRDS(file.path(vs_repo_local, "round3.rds"))

# filter out all of the 'skipped on web':
round3_adj <- round3 %>% mutate(
  weight = WEIGHT_NOLA
) %>% filter(
  ab != "SKIPPED ON WEB",
  cd != "SKIPPED ON WEB"
)
```

```{r combine-123}
lambda_1_2 <- calculate_lambda(round1_adj$weight, round2_adj$weight)

round_12 <- round1_adj %>% 
            mutate(weight = lambda_1_2*weight/sum(weight),
                   round = "Round 1") %>% 
            dplyr::select(CaseId, ab, cd, CorrectAB, CorrectCD, CorrectABCD,
                   ab_certain, cd_certain, ab_zoom, cd_zoom, round, weight,
                   age4, gender, income4, racethnicity, educ5) %>%
    rbind(round2_adj %>% 
            mutate(weight = (1-lambda_1_2)*weight/sum(weight),
                   round = "Round 2") %>% 
            dplyr::select(CaseId, ab, cd, CorrectAB, CorrectCD, CorrectABCD, 
                   ab_certain, cd_certain, ab_zoom, cd_zoom, round, weight,
                   age4, gender, income4, racethnicity, educ5))


lambda_12_3 <- calculate_lambda(round_12$weight, round3_adj$weight)

lambdas <- c(lambda_1_2, 1-lambda_1_2)*lambda_12_3
lambdas <- c(lambdas, 1-sum(lambdas))

round_123 <- round_12 %>% 
              mutate(weight = lambda_12_3*weight/sum(weight),
                     chart = "Vertical") %>% 
  rbind(
    round3_adj %>% 
      mutate(
        weight = (1-lambda_12_3)*weight/sum(weight),
        round = "Round 3",
        CorrectAB = ab == "B is bigger",
        CorrectCD = cd == "D is bigger",
        CorrectABCD = paste(CorrectAB, CorrectCD),
        chart = ifelse(P_EXP==1, "Horizontal", "Horizontal wide")
       ) %>% 
        dplyr::select(CaseId, ab, cd, CorrectAB, CorrectCD, CorrectABCD,
                   ab_certain, cd_certain, ab_zoom, cd_zoom, round, chart, weight,
                   age4, gender, income4, racethnicity, educ5) 
  )
  
round_123$weight <- neff(round_123$weight)/sum(round_123$weight)*round_123$weight 
# does not affect the further analysis, but helps with the interpretation of results

round_123_long <- round_123 %>% 
  pivot_longer(ab:cd, names_to = "task", values_to = "Response") 

round_123_long <- round_123_long %>% 
  mutate(
    correct = Response %in% c("B is bigger", "D is bigger"),
    correct3 = correct | (Response == "They are the same"),
    task_chart = factor(paste(task, chart, sep="-")),
    zoom = ifelse(task=="ab", ab_zoom, cd_zoom),
    response3 = ifelse(correct, "Correct", ifelse(correct3, "They are the same", "Wrong"))
  )



survey_123 <- 
  survey::svydesign(
  data = round_123_long, 
  ids = ~CaseId, weights = ~weight)
```


## Introduction

<!-- What is our main objective? -->

\kr{Should the abstract match -- or be close to -- our SDSS short abstract?}
\hh{I don't think it has to}


What do viewers see when we show them a data chart?
A data chart -- at its core -- maps quantitative values to graphical elements representing their relative values.
Modern data visualizations are much more than a simple, objective mapping of values to a plane; they contain contextual and design elements, and are often structured to support the viewer in understanding a particular view of a set of data or specific pattern underlying the values.
The design of a data visualization impacts a viewer's ability to achieve that understanding; a poorly designed data visualization may leave viewers struggling to understand the content or context, or make it difficult to complete accurate and useful comparison of values across groups or time points.
More broadly, the design of a data visualization can change how viewers interact with the chart.

A crucial step in the process of interacting with and understanding a chart is the viewer's employment of comparisons of the parts within.
@clevelandGraphicalPerceptionTheory1984 observed as such, and in their seminal study defined the better visual among a pair as the one that allows viewers to make more accurate comparisons.
Based on mappings of quantitative variables to different graphical elements, Cleveland and McGill's study resulted in a ranking of perceptual tasks from most accurate to least accurate, which was then extended by @mackinlayAutomatingDesignGraphical1986 to a theoretical framework ranking tasks' order along their ordinal and nominal scales, as shown in @fig-perceptual-tasks.

Cleveland and McGill's work -- while a foundational user study in graphical perception -- utilized a small convenience sample, consisting of only a few individuals recruited from among the authors' coworkers and their spouses.
@heerCrowdsourcingGraphicalPerception2010 reproduced Cleveland and McGill's rankings using a larger sample from a crowd sourcing platform.
A total of XXX Amazon Mechanical Turkers were involved.
Crowd sourced samples were shown by @borgoCrowdsourcingInformationVisualization2017 to be biased towards more male, younger, and relatively higher education relative to the adult U.S. population as a whole.
These study populations are thus not representative of the general population, a common target audience for data visualization and scientific communication work.
Further, the populations' emphasis on higher education individuals also leads to results which hold for groups of individuals who may be more likely to have prior exposure to data visualization in the context of scientific communication, or more exposure to data topics in higher education, but may not hold across other groups within the population.
\kr{I don't know if the prior statement is going too far? Maybe we can rephrase.}
\hh{I think this is going to hold - the demographics are pretty significant}
<!-- Bias from crowd-sourcing? sample size?  -->

\kr{The mackinlay thing feels like maybe we are overemphasizing the ranking done by cleveland/mcgill and then mackinlay - I think we could emphasize less if we want}

Our work -- as that of Cleveland and McGill and Heer and Bostock -- centers around studying data visualization design choices and their impact on viewer behavior and accuracy of viewers' responses.
We seek to answer whether it is possible to reproduce some of their findings in the context of a survey with a large, nationally-representative set of respondents, and within that context we focus on the following research questions:

1.  How do structural design choices in a data visualization impact viewers' ability to identify the larger of two elements?

2.  How is viewer interaction with the task impacted by structural design choices in a data visualization?

3.  Are there differences in perception and interaction with the tasks across demographic groups?  

We employ a probability-based survey panel and run a series of perception tests with nationally-representative samples of respondents from that panel.
The advantage of using a probability-based approach is two-fold.
First, we have access to a large sample of survey participants and thus have greater power in making inference about graphical perceptional abilities.
Second, the sample is representative of the general adult public in the U.S., and this allows us to test whether prior results from convenience samples hold with a nationally representative sample and whether there are differences in those results across demographic subgroups.

We present viewers with structural variations of bar charts and ask them to answer questions comparing the size of elements within those charts.
In this work, we present the series of tests we completed and the resulting findings.
The remainder of the paper is organized as follows: first, we describe the design of the visual stimuli used in our perception tests.
We then describe the population of study respondents and obtained survey sample.
Subsequently, we share our analyses of the resulting survey responses across each of our tests, including analyses on accuracy of responses and response behavior.
Finally, we discuss implications of this work and next steps.

```{r}
#| label: fig-perceptual-tasks
#| fig-cap: Ranking of perceptual tasks, as given by @mackinlayAutomatingDesignGraphical1986. The ranking of tasks on the quantitative scale are empirically verified by @clevelandGraphicalPerceptionTheory1984.
knitr::include_graphics("images/mackinlay-tasks.png")
```

<!-- \kr{The figure below is still a bit of an orphan, but I think we can revisit the intro ad study setup after we get more of the results in the paper} -->

<!-- ```{r} -->
<!-- #| label: fig-aligned-unaligned -->
<!-- #| fig-cap: "The only difference between the two pairs of rectangles A, B and C, D is their alignment, i.e. A and C are of identical size, as are B and D. When participants are asked to compare the size of these tiles in barcharts, the predominant response for the unaligned pair on the left is 'they are of the same size'. In contrast, more than half of the viewers respond with 'D is bigger' to the aligned pair of bars on the right." -->
<!-- #| fig-subcap: -->
<!-- #| - "**Unaligned bars**" -->
<!-- #| - "**Aligned bars**" -->
<!-- #| layout-nrow: 1 -->
<!-- #| fig-align: "center" -->
<!-- #| out-width: 50% -->
<!-- knitr::include_graphics("images/round5-modified.PNG") -->
<!-- knitr::include_graphics("images/round3-modified.PNG") -->
<!-- ``` -->

## Study design -- stimulus

Each task is made up of two elements: a visual stimulus and a question about that stimulus that the viewer is asked to respond to.
In our study, each visual stimulus is an image of a data visualization, while each question prompts viewers to identify which of two marked pieces in the data visualization is larger.

**What specifically is each task?** Each comparison between marked pairs is designed to be a difficult task, with the difference between the values represented in the two marked pieces being a just-noticeable difference.
The **Just-Noticeable Difference** (JND) is defined as the smallest difference that will be detected 50% of the time.
Prior results from studies on bar charts and pie charts [@luModelingJustNoticeable2022] inform the differences in charts shown to our survey panelists.

**Why do we utilize just-noticeable differences?** We employ comparisons at the JND in our tasks in order to maximize our ability to identify the impact of design changes on viewer accuracy and behavior.
Asking perception tasks in a survey differs from the controlled environment of a cognitive lab, where these kind of questions may usually be assessed.
Rather than asking the same (or similar) type of question with varied signal strength dozens or hundreds of times, we are limited to only a few questions at a time.
With a small set of tasks, we need to present tasks that are perceptually hard, and thus ask questions about stimuli that are close to our perceptual threshold.
Therefore, we focus on questions which vary the presented image, but ask viewers to compare the same values across those varied images.

**How do we vary the task?** We ask participants to determine which of two just-noticeably different marked pieces is larger within the data visualization image, and we vary the structural design of that data visualization image.
We focus on three main sets of structural variation in the design.
First, we vary the alignment of the pieces in question.
Viewers are presented with two marked pieces in a chart that do not share a common baseline, then two pieces that do share a common baseline.
Second, we vary the orientation of the chart -- we rotate the vertical stacked bar chart, and present a horizontally oriented version of the same chart, with identically sized marked pieces.
Finally, we change the aspect ratio of the chart and present a wider version of the horizontally oriented chart which has longer, but thinner, marked pieces.

```{=tex}
\hh{Not sure where should put this, parking it here for right now:}
\hh{expectations - start}
```
What we call 'aligned' and 'unaligned', here, is similar to Cleveland and McGill's set of rankings, but with some modifications: both 'aligned' and 'unaligned' bars share the same axis.
Aligned tiles are additionally anchored in the same position in one dimension, i.e. the difference between their sizes can be reduced to a positional assessment.
Unaligned tiles do not share this anchor, however, the context of the other tiles in the chart provide a frame, which *should* help with an assessment of the tiles' sizes beyond a comparison of (arc) lengths or areas.\

We would expect that comparing unaligned tiles is a harder task (with correspondingly lower levels of accuracy) than a comparison of aligned tiles, with the framing given by the context of the other tiles in the same column (or the same pie) mitigating some of this difficulty.
@fig-tasks-explain gives an overview of the comparisons of tasks 1 through 3 and the closest corresponding tasks in Cleveland and McGill.

```{=tex}
\hh{I think we should include the floating bars here to demonstrate that they are worse than the wide horizontal bars. }
```

```{r}
#| label: fig-tasks-explain
#| fig-cap: Comparisons made in charts within the Cleveland and McGill ranking 
#| out-width: 60%

include_graphics("images/sketch-tasks.png")
```

\hh{expectations - end}

**How do we present the task?** Our use of a survey format guides the format and design of the questions asked and how they are presented to respondents.
First, participant instructions must be delivered in a very short and easily understandable format, because participants cannot ask clarifying questions about the task as they might be able to in a cognitive lab setting.
Second, we want to utilize content within the data visualization image which is not socially or politically charged for the average U.S. adult; this risks participants reacting to the subject matter within the chart rather than focusing on the presented task.
For this reason, we utilize data on living arrangements of older U.S. adults -- a topic which most U.S. adults will have some familiarity with, but is not inherently polarizing.
Finally, to prevent viewers from being exposed to slight variations of the same stimulus in a row (and risk unforeseen order effects or respondents using prior questions to inform their responses), we either split a survey sample in two and show each subsample a distinct version of the chart or test variations of a chart across distinct rounds of the survey.

When viewing each chart, participants were asked to compare the relative sizes of marked elements within the chart:

"There are many charts used in the news media to portray data visually. Looking at the chart below, which of the marked dark blue pieces is bigger, A or B? Just your best guess is fine.

-   A is bigger

-   B is bigger

-   They are the same"

When presented with the aligned version of each chart, pieces were marked with a C and D and the question text is updated accordingly.
In all scenarios, the second option (B \[D\] is bigger) is the correct response.

The time in seconds that each respondent spent on each task was recorded, as well as whether the participant zoomed in on each chart while answering the question.
Respondents were also asked to rate their certainty in their response to each question on a five-point scale.

```{r}
#| label: fig-tasks
#| fig-cap: "Each of the visual stimuli presented to viewers. In each bar chart, two pieces are marked. The larger piece in each chart are pieces B and D. "
#| fig-subcap:
#| - "Vertical stacked bar, unaligned"
#| - "Horizontal stacked bar, unaligned"
#| - "Wide horizontal stacked bar, unaligned"
#| - "Vertical stacked bar, aligned"
#| - "Horizontal stacked bar, aligned"
#| - "Wide horizontal stacked bar, aligned"
#| layout-nrow: 2
#| out-width: 90%
knitr::include_graphics("images/vertical-ab.PNG")
knitr::include_graphics("images/round3/horizontal-ab.PNG")
knitr::include_graphics("images/round3/hwide-ab.PNG")
knitr::include_graphics("images/vertical-cd.PNG")
knitr::include_graphics("images/round3/horizontal-cd.PNG")
knitr::include_graphics("images/round3/hwide-cd.PNG")
```


### Task 1: Vertical stacked bar

\autoref{fig-tasks}(a,d) shows the two stacked bar charts shown to participants in the first task.
The marked tiles in each plot are 155 pixels apart.
Based on @luModelingJustNoticeable2022's model, a difference of 155 pixels leads to a just noticeable difference of 3.5 pixels.
The heights of the bars are 205 (left) and 213 pixels (right), respectively, corresponding to about twice the JND.
This difference should lead to a relatively high accuracy rate for participants and simultaneously limit the amount of frustration resulting from a task that is perceived as 'too hard'.

<!-- Lu et al calculation: log_{10}(JND) = -.4653 + .0065 distance -->

<!-- describe stimulus -->

Both charts show the same data with slight modifications to the order of the levels -- the first and second level in each of the bars are reversed between the left and the right chart.
Both charts are shown at the same size, i.e. in both cases both the difference in size between the bars and the horizontal distance between the bars is the exact same amount.
This leaves the vertical positioning of the bars as the only difference between the charts.
Any differences in observed responses can therefore be attributed to this difference in presentation.

Task 1 was presented to viewers in one of two rounds (Rounds 1-2), with four color scheme variations each presented to 50% of respondents in each round.
There were no significant differences in respondent accuracy or behavior across color schemes, and we combine them here into one set of stimulus responses to compare structural differences.
\kr{Should we put the color variations and a test for significance in accuracy/a visual in the appendix/supplementary materials? }


### Task 2: Horizontal stacked bar

The visual mappings in the second task, shown in \autoref{fig-tasks-3a}, are identical to the first task, but the axes of the chart are rotated so that the stacked bar is represented in a horizontal format.
This represents a structural change in how the data are presented to the viewer while preserving the pixel size of the elements viewers must compare.
Tasks 2 and 3 were asked within the same survey round (Round 3) and the full sample was randomly split among respondents, with 50% of participants seeing the Task 2 stimuli and 50% of participants seeing the Task 3 stimuli.

### Task 3: Wide horizontal stacked bar

The images utilized in the third task again represent the same data, and the overall image has dimensions which are identical to the images in the first two tasks.
However, the aspect ratio of the plotting area is adjusted to increase the length in pixels of the stacked bar elements, and decrease the relative thickness \kr{wording?} of each bar. 

## Study design -- participants

Participants were recruited as part of NORC's AmeriSpeak panel, which utilizes a probability-based sampling methodology and samples U.S. households from NORC's National Sample Frame that provides coverage of over 97% of U.S. households.
The current panel size is 54,001 panel members aged 13 and over residing in over 43,000 households @dennis2019technical.
\hh{I don't think I've got the right citation} Each test was conducted using the AmeriSpeak Omnibus survey, which runs biweekly and samples around 1,000 U.S. adults to answer questions on a variety of topics.

\hh{XXX I think what you are trying to say is that we do not know, if participants have seen the stimuli multiple times, but if they did, there was at least a month in between?} \kr{Yes that's what I was trying to say here!!}

Given the nature of pulling a sample from the panel for each Omnibus round, there is a possibility that some participants may have been included in multiple rounds; however, our collected data is not a longitudinal or panel study, so we do not have repeated responses from the same participants across each of our tests.
Each of the tests was run at least a month apart, so if respondents participated in multiple rounds, there was at least a one month gap between viewing each visual stimulus.

## Study design -- survey weighting

Paragraph on how survey weights are derived?
-- **Question for Ed -- is this something AmeriSpeak can provide boilerplate language for?**

All calculations in this paper are done in R [@RLanguage], and weights are applied in analyses using the `survey` package [@lumleyAnalysisComplexSurvey2004] version 4.0 [@survey] based on @lumleyComplexSurveysGuide2010.

When combining responses that were gathered during distinct rounds of the Omnibus survey, we make a weighting adjustment to ensure that weights in each sample are properly calibrated to the population total across all rounds in the resulting model.

\kr{Do we need to update this description to just say how we did it for combining across all the rounds represented?}

We *combine* (rather than cumulate) surveys $S_1$ and $S_2$, as described in @omuircheartaighCombiningSamplesVs2002, by multiplying weights in $S_1$ and $S_2$ by $\lambda$ and $1 - \lambda$ respectively.
$$
\lambda = \frac{n_1/d_1}{n_1/d_1 + n_2/d_2},
$$ where $n_1$ and $n_2$ are the nominal sample sizes and $d_1$ and $d_2$ are the design effects for the estimators.
Here, $d_1$ and $d_2$ are estimated as $$
d_1 = 1 + CV(w_i \in S_1)^2 \ \ \ \text{ and } \ \ \ d_2 = 1 + CV(w_i \in S_2)^2
$$ where $CV$ is the coefficient of variation of the weights within each sample, and is estimated as in @kishSurveySampling1965 :

$$ 
CV(w \in S) = \frac{\widehat{Var(w)}}{\bar{w}^2}.
$$

Note that @omuircheartaighCombiningSamplesVs2002 estimate $\lambda$ separately for any combination of race/ethnicity by sex.
We employ that strategy whenever we include demographic variables in the analysis, otherwise we will use a single adjustment for the weights.

The data for this paper were collected in several rounds as part of the NORC Omnibus.

\hh{we might be able to include the lambda values for combining the tables here as well}

<!-- | Name    | Date       |                \# Participants |                effective sample size |         Sum of weights $\sum_i w_i$ | -->

<!-- |---------------|---------------|--------------:|--------------:|--------------:| -->

<!-- | Round 1 | April 2022 |           `r nrow(round1_adj)` | `r round(neff(round1_adj$weight),1)` | `r round(sum(round1_adj$weight),1)` | -->

<!-- | Round 2 | May 2022   |           `r nrow(round2_adj)` | `r round(neff(round2_adj$weight),1)` | `r round(sum(round2_adj$weight),1)` | -->

<!-- | Round 3 | Jun 2022   |           `r nrow(round3_adj)` | `r round(neff(round3_adj$weight),1)` | `r round(sum(round3_adj$weight),1)` | -->

<!-- | Round 6 | Sep 2022   | `r nrow(round6_adj)` \[Split\] | `r round(neff(round6_adj$weight),1)` | `r round(sum(round6_adj$weight),1)` | -->

<!-- | Round 7 | Oct 2022   |           `r nrow(round7_adj)` | `r round(neff(round7_adj$weight),1)` | `r round(sum(round7_adj$weight),1)` | -->

<!-- |         |            |                                |                                      |                                     | -->

<!-- : Survey rounds: dates, number of participants (nominal sample size), effective sample size, and sum of weights. {#tbl-rounds} -->

# Results

There are differences by alignment
There are not huge differences across structures
However, we see a difference in how people react across different structures (zooming behavior, certainty? time?)

- We can reproduce prior findings with nat rep sample 
- We can rank 
- We can rank better because we have a bigger sample size 
- We can find changes in viewer behavior 
- Demographics matter to accuracy and choices


### Respondents

A total of `r sum(c(nrow(round1_adj), nrow(round2_adj), nrow(round3_adj)))` respondents participated across the three rounds. The number of responses and corresponding effective sample sizes in each round are shown in \autoref{tbl-rounds}. 

\kr{We need to be careful about not making it seem like the rounds are the same as the tasks, since we have 3 rounds and 3 tasks. We describe it, but I feel like this table kind of muddies the waters. Could we instead do a table of respondents by task?} \hh{does this comment still stand after the re-organization? ... I would suggest to move it up to the survey methods and add the lambdas}


| Name    | Date       |      \# Participants |                effective sample size |         Sum of weights $\sum_i w_i$ | $\lambda_i$ |
|---------------|---------------|--------------:|--------------:|--------------:|--------------:|
| Round 1 | April 2022 | `r nrow(round1_adj)` | `r round(neff(round1_adj$weight),1)` | `r round(sum(round1_adj$weight),1)` | $\lambda_1$  =  `r round(lambdas[1],3)` |
| Round 2 | May 2022   | `r nrow(round2_adj)` | `r round(neff(round2_adj$weight),1)` | `r round(sum(round2_adj$weight),1)` | $\lambda_2$  = `r sprintf("%.3f",lambdas[2])` |
| Round 3 | Jun 2022   | `r nrow(round3_adj)` | `r round(neff(round3_adj$weight),1)` | `r round(sum(round3_adj$weight),1)` | $\lambda_3$  = `r round(lambdas[3],3)` |
| **Total**        |    ---        |    `r nrow(round_123)`   |  `r round(neff(round_123$weight),1)`   |    |  $\lambda_1w_1 + \lambda_2w_2 + \lambda_3w_3$ |

: Survey rounds: dates, number of participants (nominal sample size), effective sample size, sum of weights, and factors for the convex combination as discussed above. {#tbl-rounds}



All responses were combined into one set of survey responses, with adjusted combined sample weights and indicators for which task each respondent was exposed to. Across the distinct stimuli, we model the resulting accuracy of responses, metrics on viewer interaction with the chart, and differences across demographic groups. 

\kr{Insert figure here of demographic breakdown across tasks or rounds?}

```{r}

sum(round_123$weight)


```

### Accuracy of responses  

We first investigate respondent accuracy in selecting the correct response. As participants were able to select the option 'They are the same', there are several ways to model accuracy and response. The argument could be made that for the purposes of practical interpretation, 'they are the same' is a correct choice, as the options are visually very similar and not substantially different values within the context of the data shown in the chart. However, we are interested in understanding whether viewers *can* perceive the difference and correctly identify which piece is larger. We consider multiple ways of modeling response to investigate participant response patterns. First, we define a measure of binary 'correctness', for which all answers that are not the correct option (B, D is bigger) are 'incorrect', including the selection of 'they are the same'. 

\autoref{accuracy}(a) displays all responses along the binary correctness measure, separated by whether the stimulus was an aligned task or unaligned task. We can see that levels of accuracy for all responses are significantly higher for the 'easier' (aligned) task, with about twice as many respondents correctly selecting the larger of the two marked elements. 

\kr{Can we update the figure and t-test with combined data across all tasks/all rounds 1-3?}

```{r combine_rounds_1_and_2}

lambda <- calculate_lambda(round1_adj$weight, round2_adj$weight)
round_12 <- round1_adj %>%
            mutate(weight = lambda[1]*weight) %>%
            dplyr::select(CaseId, ab, cd, CorrectAB, CorrectCD, CorrectABCD, weight) %>%
    rbind(round2_adj %>%
            mutate(weight = (1-lambda[1])*weight) %>%
            dplyr::select(CaseId, ab, cd, CorrectAB, CorrectCD, CorrectABCD, weight))
round_12_long <- round_12 %>%
  pivot_longer(CorrectAB:CorrectCD, names_to = "Question", values_to = "Response")

survey_12 <-
  survey::svydesign(
  data = round_12_long,
  ids = ~CaseId, weights = ~weight)
```

```{r ttest}
paired <- svyttest(Response~Question, survey_12)
abcd <- svyby(~Response, by=~Question, survey_12, svytotal)

marginal_results <- data.frame(abcd)
```

\kr{ Can we update this to include the values for ROund 3 as well?}
Rounds 1 and 2 are combined by adjusting the weights with $\lambda$ = `r round(lambda[1],3)` for an effective sample size of `r round(neff(round_12$weight),1)`.
Figure \ref{fig-alignment}(a) shows that more than twice the number of responses are correct, when the tiles are aligned along the same axis.
Because each participant was shown both versions of the chart, we can use a paired $t$-test to compare mean accuracy between the two charts.
The resulting $t$-statistic is highly significant ($t$ statistic: `r round(paired$statistic,1)`, df: `r paired$parameter`, $p$-value: \< 2.2e-16).






\kr{We need to update this chart to include the data from all 3 rounds instead of just the first 2}

```{r accuracy,   warning=FALSE}
#| fig-cap: "On the left (a), a stacked barchart shows the number of respondents with correct (green) and incorrect (grey) responses to the two comparison questions. When tiles are aligned along the same axis, more than twice the number of responses are correct. The shaded area along the top of the green tiles corresponds to  95\\% confidence intervals around (marginal) correct responses. On the right (b), a parallel coordinate plot shows all combinations of responses. There's a huge asymmetry in the number of responses where participants answered only one of the questions correctly. A lot more responses are correct when comparing aligned tiles than unaligned tiles."
#| label: fig-alignment
#| out-width: 80%
#| fig-height: 5
#| fig-width: 10

gg1 <- round_12_long %>%
  mutate(
    Response = ifelse(Response, "Correct", "Not correct"),
    Response = factor(Response, levels=c("Not correct", "Correct")),
    Question = ifelse(Question=="CorrectAB",
                      "Unaligned: A or B?", "Aligned: C or D?")
  ) %>%
#  filter(Response == "Correct") %>%
  ggplot(aes(x = Question, fill = Response, weight = weight)) +
  geom_bar() +
  ylab("Number of (effective) Respondents") +
  xlab("") +
  scale_fill_manual(values=c("grey", "forestgreen")) +
  scale_y_continuous(
      sec.axis = sec_axis(~./sum(round_12$weight)*100,
                          name="Percent responses")
  ) +
  theme(legend.position = "bottom") +
  ggtitle("(a) Accuracy of survey responses")  +
  geom_rect(
    aes(
      x = rev(c("Aligned: C or D?", "Unaligned: A or B?")),
      xmin = 2:1 - 0.45,
      ymin = ResponseTRUE - 1.96* se.ResponseTRUE,
      xmax = 2:1 + 0.45,
      ymax = ResponseTRUE + 1.96* se.ResponseTRUE,
      weight = 1),
    fill = "black", alpha = 0.25, #colour = "white", linewidth = 0.25,
    data = marginal_results
  )

gg2 <- round_12 %>%
  mutate(
    `Unaligned: A or B?` = ifelse(CorrectAB, "Correct", "Not correct"),
    `Aligned: C or D?` = ifelse(CorrectCD, "Correct", "Not correct")
    ) %>%
  pcp_select(`Aligned: C or D?`, `Unaligned: A or B?`) %>%
  pcp_scale() %>%
  pcp_arrange() %>%
  ggplot(aes_pcp()) +
    geom_pcp_axes() +
    geom_pcp(aes(colour = CorrectABCD), alpha = 0.2)+
    geom_pcp_boxes(fill=NA, colour = "black", linewidth=0.25) +
    geom_pcp_labels(colour = "black") +
  scale_y_continuous("Number responses", breaks=seq(0,1,by=0.25),
                     labels = round(sum(round_12$weight)*seq(0,1,by=0.25),0),
                     sec.axis = sec_axis(~.*100,
                          name="Percent responses")) +
  xlab("") +
  scale_colour_manual(values = c("grey", "#84AC84", "#70A570", "forestgreen")) +
  ggtitle("(b) Combinations of responses") +
  theme(legend.position = "none")

gg1 + gg2
```

Next, we consider an ordinal model to investigate response behavior across all three options -- 'A (C) is bigger', 'B (D) is bigger', and 'they are the same', and consider these response patterns across each of the stimuli. 



```{r model-123, warning = FALSE, message = FALSE}

logit_123_correct <- svyglm(correct~task_chart-1, family=quasibinomial(), design=survey_123)
logit_123_correct3 <- svyglm(correct3~task_chart-1, family=quasibinomial(), design=survey_123)


ci_confint <- svyby(~correct, by=~task_chart, design=survey_123, svyciprop)
ci_confint3 <- svyby(~correct3, by=~task_chart, design=survey_123, svyciprop)


coefs_123 <- broom::tidy(logit_123_correct)
coefs_123_relaxed <- broom::tidy(logit_123_correct3)

# create all pairwise comparisons for tasks and questions.
cross_compare <- glht(logit_123_correct, mcp(task_chart="Tukey"))
cross_compare3 <- glht(logit_123_correct3, mcp(task_chart="Tukey"))

# get letters for each confidence 
ci_confint$letters <- cld(cross_compare)$mcletters$Letters
ci_confint3$letters <- cld(cross_compare3)$mcletters$Letters

coefs_123 <- coefs_123 %>% left_join(
  ci_confint %>%
    mutate(
      task_chart = paste0("task_chart",task_chart, sep="")
    ) %>% dplyr::select(task_chart, letters), by=c("term"= "task_chart"))

coefs_123_relaxed <- coefs_123_relaxed %>% left_join(
  ci_confint3 %>% 
    mutate(
      task_chart = paste0("task_chart",task_chart, sep="")
    ) %>% dplyr::select(task_chart, letters), by=c("term"= "task_chart"))


ci_confint <- ci_confint %>% 
  separate_wider_delim(
    task_chart, delim = "-", names =c("task", "chart")
    )  %>% mutate(
  task = factor(task, levels=c("cd", "ab"), labels=c("Aligned", "Unaligned")),
  chart = factor(chart, levels =c("Vertical", "Horizontal", "Horizontal wide"))
)

ci_confint3 <- ci_confint3 %>% 
  separate_wider_delim(
    task_chart, delim = "-", names =c("task", "chart")
    ) %>% mutate(
  task = factor(task, levels=c("cd", "ab"), labels=c("Aligned", "Unaligned")),
  chart = factor(chart, levels =c("Vertical", "Horizontal", "Horizontal wide"))
) 
```

```{r}
#| label: fig-response-123
#| fig-cap: "Responses for accuracy in the three designs. Responses to the same task are shown side-by-side for the three designs.  The overlaid rectangles represent 95% confidence intervals. The letters in blue and orange encode significances between pairwise proportions: two bars have a significantly different proportion (at a 5% significance level) if they do not share a letter. There is no significant difference between the three designs for wrong responses. When tiles are unaligned, the horizontal wide barchart is showing the highest accuracy. For aligned tile, the horizontal wide design and the vertical design do not show a significant difference in accuracy."
#| fig-height: 4.25
#| fig-width: 7
#| out-width: 80%

round_123_long %>% 
  mutate(
    response3 = c("Wrong", "They are the same", "Correct")[correct+correct3+1],
    response3 = factor(response3, levels=rev(c("Correct", "They are the same", "Wrong"))),
    task = factor(task, levels=c("cd", "ab"), labels=c("Aligned", "Unaligned")),
    chart = factor(chart, levels=c("Vertical", "Horizontal", "Horizontal wide"))
  ) %>% 
  ggplot(aes(x = chart)) + 
  geom_bar(aes(fill = response3, weight = weight), position="fill") +
  facet_grid(.~task) + 
  theme(legend.position="bottom") + 
  scale_fill_manual("Response", values=c("orange", "grey", "steelblue")) +
  xlab("") + 
  geom_rect(
    aes(
      xmin = as.numeric(chart) - 0.45,
      ymin = correct - 1.96* `se.as.numeric(correct)`,
      xmax = as.numeric(chart) + 0.45,
      ymax = correct + 1.96* `se.as.numeric(correct)`,
    ), 
    fill = "black", alpha = 0.25, #colour = "white", linewidth = 0.25,
    data = ci_confint
  ) +
  geom_segment(
    aes(
      x = as.numeric(chart) - 0.45,
      y = correct,
      xend = as.numeric(chart) + 0.45,
      yend = correct
    ), 
    alpha = 1, colour = "black", linewidth = 0.25,
    data = ci_confint
  ) +
  geom_rect(
    aes(
      xmin = as.numeric(chart) - 0.45,
      ymin = correct3 - 1.96* `se.as.numeric(correct3)`,
      xmax = as.numeric(chart) + 0.45,
      ymax = correct3 + 1.96* `se.as.numeric(correct3)`,
    ), 
    fill = "black", alpha = 0.25, #colour = "white", linewidth = 0.25,
    data = ci_confint3
  ) +
  geom_segment(
    aes(
      x = as.numeric(chart) - 0.45,
      y = correct3,
      xend = as.numeric(chart) + 0.45,
      yend = correct3
    ), 
    alpha = 1, colour = "black", linewidth = 0.25,
    data = ci_confint3
  ) +
  scale_y_continuous("Percent responses", breaks=seq(0,1,by=0.25), labels = 100*seq(0,1,by=0.25), limits = c(-0.1, 1.1)) + 
  geom_text(aes(label=letters), y = -0.05, data = ci_confint, colour = "steelblue") +
  geom_text(aes(label=letters), y = 1.05, data = ci_confint3, colour = "darkorange") 
```

@fig-response-123 shows the results of a cell-means model with ordinal response $Y_k$, where $Y_k$ is the $k$th participant's response, $Y_k \in \{1, 2, 3\}$, where 'correct' is encoded as 1, 'they are the same' is encoded as 2, and 'wrong' is encoded as 3:

$$
\text{logit }P(Y_k \le \ell) = \mu_{ij\ell(k)},
$$

where $\ell \in \{1, 2\}$; $i \in \{1, 2\}$ is the comparison type (1 = Aligned, 2 = Unaligned), and $j \in \{1, 2, 3\}$ is the chart design, with 1 = Vertical, 2 = Horizontal, and 3 = Horizontal wide.
The estimated values and 95% confidence intervals are shown in @tbl-rounds-123

```{r}
#| label: tbl-rounds-123
#| tbl-cap: Log-odds for the cell-means model, letters behind numbers indicate pairwise significances. Within the same **column** values are significantly different (at 5%) if they do not share the same letter.

dframe <- rbind(
  coefs_123 %>% mutate(level = c("correct | same or wrong")),
  coefs_123_relaxed %>% mutate(level = c("correct or same | wrong"))
) %>% mutate(
  term = gsub("task_chart", "", term),
) %>% separate(term, into=c("task", "chart"), sep="-")

dframe %>% 
  dplyr::select(level, task, chart, estimate, std.error, letters) %>%
  mutate(
    task = ifelse(task=="ab", "Unaligned", "Aligned")
  ) %>%
  group_by(task) %>%
  mutate(
    ci_low = exp(estimate-1.96*std.error),
    ci_high = exp(estimate+1.96*std.error),
    estimate = exp(estimate)
  ) %>%
  tidyr::pivot_wider(
        id_cols = c(task, chart),
        names_from = level,
        values_from = c(estimate, ci_low, ci_high, letters)
      ) %>%
  gt() %>%
  tab_header(title = "Log odds of accuracy by task and chart type") %>%
  fmt_number(
    everything(),
    decimals=2
  ) %>%
  cols_merge(
    columns = c("estimate_correct | same or wrong", "ci_low_correct | same or wrong",
                "ci_high_correct | same or wrong", "letters_correct | same or wrong"),
    pattern = "<<{1}>><< [{2}, {3}]>><< {4}>>"
  ) %>%
  cols_merge(
    columns = c("estimate_correct or same | wrong", "ci_low_correct or same | wrong",
                "ci_high_correct or same | wrong", "letters_correct or same | wrong"),
    pattern = "<<{1}>><< [{2}, {3}]>><< {4}>>"
  ) %>%
  tab_options(
    column_labels.font.weight = "bold",
    row_group.font.weight = "bold"
  ) %>%
  cols_label(
    .list = c(
      "chart" = "",
      "estimate_correct | same or wrong" = "correct | same or wrong",
      "estimate_correct or same | wrong" = "correct or same | wrong"
    )
  )
```

The same pattern in accuracy holds across each of the three structural variations; the aligned task has a higher level of accuracy than its unaligned counterpart. Interestingly, while we expect an improvement in accuracy when shifting from the horizontal to the horizontal wide design given the larger difference in pixel length between the two pieces, the resulting effects on the accuracy of the responses are not completely straightforward: The shift from a vertical to the (tall) horizontal design is detrimental to an accurate perception for both aligned and unaligned comparisons. The re-scaled design of the wide horizontal bars reclaims some of the loss for unaligned bars and outperforms the vertical design by a similar margin in aligned bars, but does not out-perform the vertical design when comparing unaligned tiles.



### Respondent behavior  


A contributing factor to this outcome might be the way that participants interact with the different designs.
Generally, about half of all participants make use of the option to zoom into charts - while zooming does help with the overall accuracy (which is in agreement with the findings by @luModelingJustNoticeable2022 about the physical size of stimuli), the increase is not significant.
However, different designs lead to different rates of zooming: when dealing with the vertical design, the rate of zooming is significantly higher than for the two horizontal designs.

<!-- The rate of zooming into charts is about twice as high for participants filling out the survey on their smart phones than on other, usually bigger devices such as tablets or desktops. -->

```{r zoom-123}
logit_zoom <- svyglm(I(zoom=="yes")~response3+task+chart, family=quasibinomial(), design=survey_123)

coefs_zoom <- broom::tidy(logit_zoom)
```

```{r}
#| label: fig-zoom-123
#| fig-cap: "Zooming - not significant for accuracy or task, but changes by the type of chart."
#| fig-height: 4.25
#| fig-width: 7
#| out-width: 80%

zt <- round_123_long %>% 
  mutate(
    response3 = c("Wrong", "They are the same", "Correct")[correct+correct3+1],
    response3 = factor(response3, levels=rev(c("Correct", "They are the same", "Wrong"))),
    task = factor(task, levels=c("cd", "ab"), labels=c("Aligned", "Unaligned")),
    chart = factor(chart, levels=c("Vertical", "Horizontal", "Horizontal wide")),
    zoom = factor(zoom, levels=c("no", "yes"))
  ) %>% 
  ggplot(aes(x = response3)) + 
  geom_bar(aes(fill = zoom, weight = weight), position="fill") +
  facet_grid(.~task) + 
  theme(axis.text.x = element_text(angle=30, hjust = 1)) + 
  scale_fill_manual("Zoom", values=c("grey", "#1274F3")) +
  xlab("") + 
  ggtitle("Zooming behavior by task")

zd <- round_123_long %>% 
  mutate(
    response3 = c("Wrong", "They are the same", "Correct")[correct+correct3+1],
    response3 = factor(response3, levels=rev(c("Correct", "They are the same", "Wrong"))),
    task = factor(task, levels=c("cd", "ab"), labels=c("Aligned", "Unaligned")),
    chart = factor(chart, levels=c("Vertical", "Horizontal", "Horizontal wide")),
    zoom = factor(zoom, levels=c("no", "yes"))
  ) %>% 
  ggplot(aes(x = response3)) + 
  geom_bar(aes(fill = zoom, weight = weight), position="fill") +
  facet_grid(.~chart) + 
  scale_fill_manual("Zoom", values=c("grey", "#1274F3")) +
  xlab("") + 
  ggtitle("Zooming behavior by chart") +
  theme(axis.text.x = element_text(angle=30, hjust = 1))

zt + zd + plot_layout(widths=c(5, 7.5), guides='collect') 
  
```

Let $Y_{jk}$ describe the zooming behavior of panelist $k$ on task $j$.
We model zooming behavior (no = 0, yes = 1) as a logistic regression by correctness of response ($\rho$), task ($\tau$), and design ($\delta$) of the chart: $$
\text{logit } P(Y_k \le 1) = \mu + \rho_{i(k)} + \tau_{j(k)} + \delta_{\ell(k)},
$$

I'll pretty up @tbl-zoom-123 if we are going to keep it.

```{r}
#| label: tbl-zoom-123
#| tbl-cap: Coefficients for logistic regression of zooming by task

coefs_zoom %>%
  mutate(
    term = ifelse(term=="(Intercept)", "Intercept", term),
    p.value = ifelse(p.value<0.0001, "< 0.0001", sprintf("%.4f", p.value))
  ) %>%
  gt() %>%
  tab_header("Estimates for logistic regression on zooming behavior") %>%
  cols_label(
    std.error = "SE",
    statistic = "t-statistic",
    p.value = "p-value"
  ) %>%
  fmt_number(
    columns = c(estimate, std.error),
    decimals = 2
  ) %>%
  fmt_number(
    columns = statistic,
    decimals = 1
  ) %>%
  cols_align(
    columns = p.value,
    align = "right"
  )

```

### Differences across demographic groups

Let $Y_k$ be the response of participant $k$, on a scale from 1 = 'wrong', 2 = 'they are the same' to 3 = 'correct'.
We use a generalized cumulative logistic regression, where $\mu_i$ are intercepts $1 \le i < 3$, $X_k$ are demographics of the $k$th participant (in form of the model matrix), and $\beta_{i}$ are the coefficients.
$$
\text{ logit } P(Y_k \le i \mid X_k) = \mu_i + X_{k}'\beta_{i}
$$

```{r}
#| label: fig-demographics
#| fig-cap: "Panelist's demographics matter, particularly, when the task difficulty increases. For aligned tiles, gender, age, and education are not significant factors. However, income levels do have - a small - effect. When income levels increase, the percentage of wrong answers (orange) decreases, while the percentage of correct answers (blue) increases slightly (significant at below 0.05). The more difficult task of comparing unaligned tiles, demographics are more significant.  XXX With increasing levels in education, we see a small drop in correct answers, as well as a small drop in wrong answers, for a significant increase in 'they are the same'."
#| fig-subcap: 
#|   - Comparisons of aligned tiles
#|   - Comparisons of unaligned tiles
#| fig-height: 5
#| fig-width: 7
#| layout-nrow: 2
#| out-width: 100%

educ_cd <- round_123_long %>% 
  filter(task=="cd") %>%
  mutate(
    response3 = c("Wrong", "They are the same", "Correct")[correct+correct3+1],
    response3 = factor(response3, levels=rev(c("Correct", "They are the same", "Wrong"))),
    task = factor(task, levels=c("cd", "ab"), labels=c("Aligned", "Unaligned")),
    chart = factor(chart, levels=c("Vertical", "Horizontal", "Horizontal wide"))
  ) %>% 
  ggplot(aes(x = educ5)) + 
  facet_grid(~task) + 
  geom_bar(aes(fill = response3, weight = weight), position="fill") +
  theme(legend.position="none",
        axis.text.x = element_text(angle=30, hjust=1)) + 
  scale_fill_manual("Response", values=c("orange", "grey", "steelblue")) +
  xlab("") +
    ggtitle("\nby education") +
  scale_y_continuous("Percentage", breaks=seq(0,1, by=0.25), labels = seq(0,1, by=0.25)*100)

educ_ab <- round_123_long %>% 
  filter(task=="ab") %>%
  mutate(
    response3 = c("Wrong", "They are the same", "Correct")[correct+correct3+1],
    response3 = factor(response3, levels=rev(c("Correct", "They are the same", "Wrong"))),
    task = factor(task, levels=c("cd", "ab"), labels=c("Aligned", "Unaligned")),
    chart = factor(chart, levels=c("Vertical", "Horizontal", "Horizontal wide"))
  ) %>% 
  ggplot(aes(x = educ5)) + 
  facet_grid(.~task) + 
  geom_bar(aes(fill = response3, weight = weight), position="fill") +
  theme(legend.position="none",
        axis.text.x = element_text(angle=30, hjust=1)) + 
  scale_fill_manual("Response", values=c("orange", "grey", "steelblue")) +
  xlab("") +
    ggtitle("\nby education") +
  scale_y_continuous("Percentage", breaks=seq(0,1, by=0.25), labels = seq(0,1, by=0.25)*100)

gender_cd <- round_123_long %>% 
  filter(task=="cd") %>%
  mutate(
    response3 = c("Wrong", "They are the same", "Correct")[correct+correct3+1],
    response3 = factor(response3, levels=rev(c("Correct", "They are the same", "Wrong"))),
    task = factor(task, levels=c("cd", "ab"), labels=c("Aligned", "Unaligned")),
    chart = factor(chart, levels=c("Vertical", "Horizontal", "Horizontal wide"))
  ) %>% 
  ggplot(aes(x = gender)) + 
  facet_grid(.~task) + 
  geom_bar(aes(fill = response3, weight = weight), position="fill") +
  theme(legend.position="none",
        axis.text.x = element_text(angle=30, hjust=1)) + 
  scale_fill_manual("Response", values=c("orange", "grey", "steelblue")) +
  xlab("") +
  ggtitle("Accuracy\nby gender") +
  scale_y_continuous("Percentage", breaks=seq(0,1, by=0.25), labels = seq(0,1, by=0.25)*100)

gender_ab <- round_123_long %>% 
  filter(task=="ab") %>%
  mutate(
    response3 = c("Wrong", "They are the same", "Correct")[correct+correct3+1],
    response3 = factor(response3, levels=rev(c("Correct", "They are the same", "Wrong"))),
    task = factor(task, levels=c("cd", "ab"), labels=c("Aligned", "Unaligned")),
    chart = factor(chart, levels=c("Vertical", "Horizontal", "Horizontal wide"))
  ) %>% 
  ggplot(aes(x = gender)) + 
  facet_grid(.~task) + 
  geom_bar(aes(fill = response3, weight = weight), position="fill") +
  theme(legend.position="none",
        axis.text.x = element_text(angle=30, hjust=1)) + 
  scale_fill_manual("Response", values=c("orange", "grey", "steelblue")) +
  xlab("") +
  ggtitle("Accuracy\nby gender") +
  scale_y_continuous("Percentage", breaks=seq(0,1, by=0.25), labels = seq(0,1, by=0.25)*100)

income_cd <- round_123_long %>% 
  filter(task=="cd") %>%
  mutate(
    response3 = c("Wrong", "They are the same", "Correct")[correct+correct3+1],
    response3 = factor(response3, levels=rev(c("Correct", "They are the same", "Wrong"))),
    task = factor(task, levels=c("cd", "ab"), labels=c("Aligned", "Unaligned")),
    chart = factor(chart, levels=c("Vertical", "Horizontal", "Horizontal wide"))
  ) %>% 
  ggplot(aes(x = income4)) + 
  facet_grid(.~task) + 
  geom_bar(aes(fill = response3, weight = weight), position="fill") +
  theme(legend.position="none",
        axis.text.x = element_text(angle=30, hjust=1)) + 
  scale_fill_manual("Response", values=c("orange", "grey", "steelblue")) +
  xlab("") +
    ggtitle("\nby income") +
  scale_y_continuous("Percentage", breaks=seq(0,1, by=0.25), labels = seq(0,1, by=0.25)*100)

income_ab <- round_123_long %>% 
  filter(task=="ab") %>%
  mutate(
    response3 = c("Wrong", "They are the same", "Correct")[correct+correct3+1],
    response3 = factor(response3, levels=rev(c("Correct", "They are the same", "Wrong"))),
    task = factor(task, levels=c("cd", "ab"), labels=c("Aligned", "Unaligned")),
    chart = factor(chart, levels=c("Vertical", "Horizontal", "Horizontal wide"))
  ) %>% 
  ggplot(aes(x = income4)) + 
  facet_grid(.~task) + 
  geom_bar(aes(fill = response3, weight = weight), position="fill") +
  theme(legend.position="none",
        axis.text.x = element_text(angle=30, hjust=1)) + 
  scale_fill_manual("Response", values=c("orange", "grey", "steelblue")) +
  xlab("") +
    ggtitle("\nby income") +
  scale_y_continuous("Percentage", breaks=seq(0,1, by=0.25), labels = seq(0,1, by=0.25)*100)

race <- round_123_long %>% 
  mutate(
    response3 = c("Wrong", "They are the same", "Correct")[correct+correct3+1],
    response3 = factor(response3, levels=rev(c("Correct", "They are the same", "Wrong"))),
    task = factor(task, levels=c("cd", "ab"), labels=c("Aligned", "Unaligned")),
    chart = factor(chart, levels=c("Vertical", "Horizontal", "Horizontal wide"))
  ) %>% 
  ggplot(aes(x = racethnicity)) + 
  facet_grid(.~task) + 
  geom_bar(aes(fill = response3, weight = weight), position="fill") +
  theme(legend.position="none",
        axis.text.x = element_text(angle=30, hjust=1)) + 
  scale_fill_manual("Response", values=c("orange", "grey", "steelblue")) +
  xlab("") +
    ggtitle("Accuracy by race, ethnicity and task") +
  scale_y_continuous("Percentage", breaks=seq(0,1, by=0.25), labels = seq(0,1, by=0.25)*100)

age_cd <- round_123_long %>% 
  filter(task=="cd") %>%
  mutate(
    response3 = c("Wrong", "They are the same", "Correct")[correct+correct3+1],
    response3 = factor(response3, levels=rev(c("Correct", "They are the same", "Wrong"))),
    task = factor(task, levels=c("cd", "ab"), labels=c("Aligned", "Unaligned")),
    chart = factor(chart, levels=c("Vertical", "Horizontal", "Horizontal wide"))
  ) %>% 
  ggplot(aes(x = age4)) + 
   facet_grid(.~task) + 
 geom_bar(aes(fill = response3, weight = weight), position="fill") +
  theme(legend.position="none",
        axis.text.x = element_text(angle=30, hjust=1)) + 
  scale_fill_manual("Response", values=c("orange", "grey", "steelblue")) +
  xlab("") +
    ggtitle("\nby age") +
  scale_y_continuous("Percentage", breaks=seq(0,1, by=0.25), labels = seq(0,1, by=0.25)*100)

age_ab <- round_123_long %>% 
  filter(task=="ab") %>%
  mutate(
    response3 = c("Wrong", "They are the same", "Correct")[correct+correct3+1],
    response3 = factor(response3, levels=rev(c("Correct", "They are the same", "Wrong"))),
    task = factor(task, levels=c("cd", "ab"), labels=c("Aligned", "Unaligned")),
    chart = factor(chart, levels=c("Vertical", "Horizontal", "Horizontal wide"))
  ) %>% 
  ggplot(aes(x = age4)) + 
   facet_grid(.~task) + 
 geom_bar(aes(fill = response3, weight = weight), position="fill") +
  theme(legend.position="none",
        axis.text.x = element_text(angle=30, hjust=1)) + 
  scale_fill_manual("Response", values=c("orange", "grey", "steelblue")) +
  xlab("") +
    ggtitle("\nby age") +
  scale_y_continuous("Percentage", breaks=seq(0,1, by=0.25), labels = seq(0,1, by=0.25)*100)
gender_cd + age_cd  + educ_cd +income_cd + plot_layout(nrow=1, guides = "collect")

gender_ab + age_ab  + educ_ab +income_ab + plot_layout(nrow=1, guides = "collect")
```

```{r demographics-model}
logit_demo_task_cd_correct <- svyglm(correct~gender+age4+educ5+income4, design=survey_123, family=quasibinomial(), subset = task=="cd")

logit_demo_task_cd_correct_same <- svyglm(correct3~gender+age4+educ5+income4, design=survey_123, family=quasibinomial(), subset = task=="cd")

logit_demo_task_ab_correct <- svyglm(correct~gender+age4+educ5+income4, design=survey_123, family=quasibinomial(), subset = task=="ab")
logit_demo_task_ab_correct_same <- svyglm(correct3~gender+age4+educ5+income4, design=survey_123, family=quasibinomial(), subset = task=="ab")

coefs_cd_correct <- broom::tidy(logit_demo_task_cd_correct)
coefs_cd_correct_same <- broom::tidy(logit_demo_task_cd_correct_same)

coefs_ab_correct <- broom::tidy(logit_demo_task_ab_correct)
coefs_ab_correct_same <- broom::tidy(logit_demo_task_ab_correct_same)
```

@tbl-demographics doesn't survive the formatting to pdf very well - \hh{need to shorten the row names or put into multiple lines}

```{r demographics-table}
#| label: tbl-demographics
#| tbl-cap: Demographics matter for perception, particularly when the tasks get harder.
 coefs_clean <- function(dframe) {
  dframe %>% mutate(
    variable = c("Intercept", "Gender", rep("Age", 3), rep("Education", 4), rep("Income", 3)),
    term = ifelse(term=="(Intercept)", "", term),
    term = gsub("gender", "", term),
    term = gsub("educ5", "", term),
    term = gsub("age4", "", term),
    term = gsub("income4", "", term),
    ci_low = exp(estimate-1.96*std.error),
    ci_high = exp(estimate+1.96*std.error),
    estimate = exp(estimate)
  )
}

coefs_clean(coefs_cd_correct) %>% mutate(task = "Aligned", correct="correct") %>%
 rbind(
  coefs_clean(coefs_ab_correct) %>% mutate(task = "Unaligned", correct="correct")
 ) %>%
 rbind(
  coefs_clean(coefs_cd_correct_same) %>% mutate(task = "Aligned", correct="not wrong")
 ) %>%
 rbind(
  coefs_clean(coefs_ab_correct_same) %>% mutate(task = "Unaligned", correct="not wrong")
 ) %>%
 dplyr::select(-std.error, -statistic) %>%
 mutate(
  p.value = ifelse (p.value <  0.001, "***",
      ifelse (p.value <  0.01, "**",
             ifelse (p.value < 0.05, "*",
                    ifelse (p.value < 0.1, ".", ""))))
 ) %>%
 group_by(variable) %>%
 pivot_wider(
    id_cols = c(term, variable),
    names_from = c(task, correct),
    values_from = c(estimate, ci_low, ci_high, p.value),
  ) %>% 
 gt() %>%
  tab_header(title = "Log odds of accuracy by task and demographics of respondents") %>%
 tab_options(
   column_labels.font.weight = "bold",
   row_group.font.weight = "bold"
 ) %>%
  fmt_number(
    decimals=2
  ) %>%
  cols_width(
    1~px(50),
    contains("p.value") ~ px(20)
  ) %>%
  cols_merge(
    columns = contains("ci") & contains("_Aligned_correct"),
    pattern = "<< [{1}, {2}]>>"
  ) %>%
  cols_merge(
    columns = contains("ci") & contains("_Aligned_not"),
    pattern = "<< [{1}, {2}]>>"
  ) %>%
  cols_move_to_end(contains("_Aligned_")) %>%
  tab_spanner(
    label = "correct | same or wrong",
    columns = contains("_Aligned_correct")
  )  %>%
  tab_spanner(
    label = "correct or same | wrong",
    columns = contains("_Aligned_not")
  ) %>%
  tab_spanner(
    label = "Aligned tiles",
    columns = contains("_Aligned_"),
    spanners = c("correct | same or wrong", "correct or same | wrong")
  ) %>%
  cols_move_to_end(contains("_Unaligned_")) %>%
  cols_merge(
    columns = contains("ci") & contains("_Unaligned_correct"),
    pattern = "<< [{1}, {2}]>>"
  ) %>%
  cols_merge(
    columns = contains("ci") & contains("_Unaligned_not"),
    pattern = "<< [{1}, {2}]>>"
  ) %>%
  tab_spanner(
    label = "correct | same or wrong ",
    columns = contains("_Unaligned_correct")
  )  %>%
  tab_spanner(
    label = "correct or same | wrong ",
    columns = contains("_Unaligned_not")
  ) %>%
  tab_spanner(
    label = "Unaligned tiles",
    columns = contains("_Unaligned")
  ) %>%
   cols_label(
    .list = c(
      "estimate_Aligned_correct" = "Est.",
      "ci_low_Aligned_correct" = "          [95% CI]",
      "p.value_Aligned_correct" = " ",
      "estimate_Aligned_not wrong" = "Est.",
      "ci_low_Aligned_not wrong" = "          [95% CI]",
      "p.value_Aligned_not wrong" = " ",
      "estimate_Unaligned_correct" = "Est.",
      "ci_low_Unaligned_correct" = "          [95% CI]",
      "p.value_Unaligned_correct" = " ",
      "estimate_Unaligned_not wrong" = "Est.",
      "ci_low_Unaligned_not wrong" = "          [95% CI]",
      "p.value_Unaligned_not wrong" = " "
    )
  ) 

```

Results to discuss:

-   Differences in accuracy between aligned and unaligned bar

-   Differences across structures

-   Differences in timing and zooming behavior across all designs

-   Demographic differences and their interaction with alignment

# Conclusion

Key findings to discuss:

-   Surveys can be used to ask these types of questions -- in the midst of other topics and with a limited number of questions, we are able to ask perception questions and produce results consistent with prior studies

-   Reproduced (some) prior convenience sample results using a large, nationally-representative survey population:

    -   Cleveland: position (aligned) versus unaligned
    -   unaligned tiles within stacked bar are a hybrid between framed bar and floats
    -   Talbot: pie charts - individual wedges vs piecharts

-   Design choices impact viewer accuracy:

    -   new finding: pie charts accuracy in work better when framed (wedges)

-   Other important measures beyond accuracy: time to completion (automatic on web), certainty in response (asked).

-   Studying how viewers interact with charts can be done in a survey, and more expansive work on this topic should be completed to understand how the general public interacts with and understands charts.

    -   Design choices impact viewer behavior: zooming

-   (some) Demographics matter: age and gender does not seem to matter for perception economic background and education does matter for perception: we need to know, we do not want to create a hurdle in communication

# Supplementary Material {.unnumbered}

-   **Participant Data (Linear):** Link to csv file with the data.
-   **Data Analysis Code:** Link to an html document with annotated code chunks.

# References
