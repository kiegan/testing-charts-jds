---
title: "Supplement: Analysis and code for 'Testing Perceptual Accuracy in Charts using Surveys'"
format:
  jasa-html: default
  jasa-pdf:
    keep-tex: true  
    journal:
      blinded: false
date: last-modified
bibliography: ../SDSS\-2023/references.bib
---


```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = F, 
                      eval = T, 
                      fig.path="./figures/", # figures are created on the fly, images are pictures like the stimuli
                      fig.env="figure*", 
                      fig.align = "center",
                      out.width = "50%",
                      dpi = 72, # we can set the resolution up to 300 later.
                      cache = TRUE) 
library(readr)
library(tidyverse)
library(knitr)
library(bibtex)
theme_set(theme_bw())

library(readxl)
library(lubridate)
library(patchwork)
library(survey)
#library(ggmosaic)
library(ggpcp)

# Kish's Effective Sample Size
neff <- function(weight) {
  n <- length(weight)
  L <- var(weight)/mean(weight)^2
  n/(1+L)
}
```

## Survey rounds

```{r data}
# other github repo. 
round1 <- readRDS("~/Documents/projects/NORC/visual-studies/data/round1.rds")
round2 <- readRDS("~/Documents/projects/NORC/visual-studies/data/round2.rds")

round1_adj <- round1 %>% 
            filter(ab != "SKIPPED ON WEB", cd != "SKIPPED ON WEB") %>%
            mutate(CorrectAB = ab=="B is bigger", CorrectCD = cd=="D is bigger",
                   CorrectABCD = paste(CorrectAB, CorrectCD))

round2_adj <- round2 %>% 
            filter(ab != "SKIPPED ON WEB", cd != "SKIPPED ON WEB") %>%
            mutate(CorrectAB = ab=="B is bigger", CorrectCD = cd=="D is bigger",
                   CorrectABCD = paste(CorrectAB, CorrectCD))

survey1 <- survey::svydesign(
  data = round1_adj, 
  ids = ~CaseId, weights = ~weight)

survey2 <- survey::svydesign(
  data = round2_adj,
  ids = ~CaseId, weights = ~weight)
```



The data for this paper were collected in several rounds as part of the NORC Omnibus. 

| Name    | Date  | # Participants | effective sample size | Sum of weights $\sum_i w_i$ |
|---------|-------|--------:|----------:|----------:|
| Round 1 | April 2022 |  `r nrow(round1_adj)`    |    `r round(neff(round1_adj$weight),1)`       | `r round(sum(round1_adj$weight),1)`       |
| Round 2 | May 2022 |  `r nrow(round2_adj)`   |    `r round(neff(round2_adj$weight),1)`       | `r round(sum(round2_adj$weight),1)`       |
|         |       |         |           |           |   

: Survey rounds: dates, number of participants (nominal sample size), effective sample size, and sum of weights. {#tbl-rounds}



We are using a strategy of *combining* (rather than cumulating) surveys $S_1$ and $S_2$, as described in @omuircheartaighCombiningSamplesVs2002,  by 
multiplying weights in $S_1$ and $S_2$ by $\lambda$ and $1 - \lambda$, respectively.
$$
\lambda = \frac{n_1/d_1}{n_1/d_1 + n_2/d_2},
$$
where $n_1$ and $n_2$ are the nominal sample sizes and $d_1$ and $d_2$ are the design effects for the estimators. 
Instead of using design effects itself, $d_1$ and $d_2$ are estimated as 
$$
d_1 = 1 + CV(w_i \in S_1)^2 \ \ \ \text{ and } \ \ \ d_2 = 1 + CV(w_i \in S_2)^2
$$
$CV$ is the coefficient of variation of the weights within each sample, and can be estimated as

$$ 
CV(w \in S) = \frac{\widehat{Var(w)}}{\bar{w}^2}.
$$

@omuircheartaighCombiningSamplesVs2002 calculated
$\lambda$ separately for any combination of race/ethnicity by sex. 

We will calculate $\lambda$ separately whenever we include demographic variables in the analysis, otherwise we will use a single adjustment for the weights.

All calculations are done in R [@R2022] using the `survey` package [@lumleyAnalysisComplexSurvey2004] version 4.0 [@survey] based on @lumleyComplexSurveysGuide2010.



## Model 1: Comparing Aligned and Unaligned Tiles {#sec-model1}


```{r lambdas}

d1 <- 1+var(round1_adj$weight)/mean(round1_adj$weight)^2
d2 <- 1+var(round2_adj$weight)/mean(round2_adj$weight)^2

n1 <- nrow(round1_adj)
n2 <- nrow(round2_adj)

neff1 <- n1/d1
neff2 <- n2/d2

denom <- n1/d1 + n2/d2

lambda <- n1/(d1*denom) 
```


```{r combine_rounds_1_and_2}
round_12 <- round1_adj %>% 
            mutate(weight = lambda*weight) %>% 
            select(CaseId, ab, cd, CorrectAB, CorrectCD, CorrectABCD, weight) %>%
    rbind(round2_adj %>% 
            mutate(weight = (1-lambda)*weight) %>% 
            select(CaseId, ab, cd, CorrectAB, CorrectCD, CorrectABCD, weight))

wt_scalar <- neff(round_12$weight)/sum(round_12$weight)
round_12$weight <- wt_scalar*round_12$weight 
# should not affect the further analysis, but helps with the interpretation of results

round_12_long <- round_12 %>% 
  pivot_longer(CorrectAB:CorrectCD, names_to = "Question", values_to = "Response") 


survey_12 <- 
  survey::svydesign(
  data = round_12_long, 
  ids = ~CaseId, weights = ~weight)
```

```{r ttest}
paired <- svyttest(Response~Question, survey_12)
abcd <- svyby(~Response, by=~Question, survey_12, svytotal)

marginal_results <- data.frame(abcd)
```


The data used for this is a combination of rounds 1 and 2, with $\lambda$ = `r round(lambda[1],3)` for an effective sample size of `r round(neff(round_12$weight),1)`. 

### Binary response

Defining an accurate response as "B is bigger" in the chart of unaligned tiles, and "D is bigger" in the aligned case, while calling the other two responses as incorrect, leads to the data shown in figure \ref{fig-alignment}(a): we see
 that more than twice the number of responses is accurate, when the tiles are aligned along the same axis. 
Because each particpant was shown both versions of the chart, we can use a paired $t$-test to compare mean accuracy between the two charts. The resulting $t$-statistic is highly significant ($t$ statistic: `r round(paired$statistic,1)`, df: `r paired$parameter`, $p$-value: < 2.2e-16). 


```{r accuracy, fig.height = 5, fig.width = 10, warning=FALSE}
#| fig-cap: "On the left (a), a stacked barchart shows the number of respondents with correct (green) and incorrect (grey) responses to the two comparison questions. When tiles are aligned along the same axis, more than twice the number of responses is accurate. The shaded area along the top of the green tiles corresponds to  95\\% confidence intervals around (marginal) correct responses. On the right (b), a parallel coordinate plot shows all combinations of responses. There's a huge asymmetry in the number of responses, where participants answered only one of the questions correctly. A lot more responses are correct when comparing aligned tiles than unaligned tiles."
#| label: fig-alignment
#| out-width: 80%
#| fig-env: figure



gg1 <- round_12_long %>%
  mutate(
    Response = ifelse(Response, "Correct", "Not correct"),
    Response = factor(Response, levels=c("Not correct", "Correct")),
    Question = ifelse(Question=="CorrectAB",
                      "Unaligned: A or B?", "Aligned: C or D?")
  ) %>%
#  filter(Response == "Correct") %>%
  ggplot(aes(x = Question, fill = Response, weight = weight)) +
  geom_bar() +
  ylab("Number of (effective) Respondents") +
  xlab("") +
  scale_fill_manual(values=c("grey", "forestgreen")) +
  scale_y_continuous(
      sec.axis = sec_axis(~./sum(round_12$weight)*100,
                          name="Percent responses")
  ) +
  theme(legend.position = "bottom") +
  ggtitle("(a) Accuracy of survey responses")  +
  geom_rect(
    aes(
      x = rev(c("Aligned: C or D?", "Unaligned: A or B?")),
      xmin = 2:1 - 0.45,
      ymin = ResponseTRUE - 1.96* se.ResponseTRUE,
      xmax = 2:1 + 0.45,
      ymax = ResponseTRUE + 1.96* se.ResponseTRUE,
      weight = 1),
    fill = "black", alpha = 0.25, #colour = "white", linewidth = 0.25,
    data = marginal_results
  )

gg2 <- round_12 %>%
  mutate(
    `Unaligned: A or B?` = ifelse(CorrectAB, "Correct", "Not correct"), 
    `Aligned: C or D?` = ifelse(CorrectCD, "Correct", "Not correct")
    ) %>%
  pcp_select(`Aligned: C or D?`, `Unaligned: A or B?`) %>%
  pcp_scale() %>%
  pcp_arrange() %>%
  ggplot(aes_pcp()) + 
    geom_pcp_axes() + 
    geom_pcp(aes(colour = CorrectABCD), alpha = 0.2)+
    geom_pcp_boxes(fill=NA, colour = "black", linewidth=0.25) + 
    geom_pcp_labels(colour = "black") +
  scale_y_continuous("Number responses", breaks=seq(0,1,by=0.25), 
                     labels = round(sum(round_12$weight*wt_scalar)*seq(0,1,by=0.25),0),
                     sec.axis = sec_axis(~.*100,
                          name="Percent responses")) +
  xlab("") + 
  scale_colour_manual(values = c("grey", "#84AC84", "#70A570", "forestgreen")) +
  ggtitle("(b) Combinations of responses") +
  theme(legend.position = "none")

gg1 + gg2
```

### Ordinal response


```{r}
round_12_ord_long <- round_12 %>% 
  pivot_longer(ab:cd, names_to = "Question", values_to = "Response") 

round_12_ord_long <- round_12_ord_long %>% 
  mutate(
    Response = ifelse(Response %in% c("B is bigger", "D is bigger"), "Y is bigger (Correct)", 
                      ifelse(Response == "They are the same", "They are the same", "X is bigger (Wrong)"))
  )

survey_12_ord <- 
  survey::svydesign(
  data = round_12_ord_long, 
  ids = ~CaseId, weights = ~weight)



model1_ord <- svyloglin(~Response*Question, survey_12_ord)

# two logisting regressions
model1_logist1 <- svyglm(I(Response == "Y is bigger (Correct)")~Question, survey_12_ord)
# parameter Questioncd is effect of increase in accuracy due to alignment

model1_logist2 <- svyglm(I(Response %in% c("Y is bigger (Correct)", "They are the same"))~Question, survey_12_ord)
# For aligned tiles, 4.55 percentage points more respondents correctly pick D as the correct response or said the two tiles were the same size than picked B (or said they were the same size) in the unaligned tiles. 

summary(model1_logist2)
```


## References{-}