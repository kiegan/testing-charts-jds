---
title: "Supplement: Analysis and code for 'Testing Perceptual Accuracy in Charts using Surveys'"
format:
  jasa-html: default
  jasa-pdf:
    journal:
      blinded: false
date: last-modified
bibliography: references.bib 
---

```{r run-to-update-bibfile, eval=FALSE, include=FALSE}
keys <- rbbt::bbt_detect_citations("analysis.qmd")
citations_only <- grep(pattern="[fig|tbl]-.*", keys, value=TRUE, invert=TRUE)
rbbt::bbt_write_bib(keys=citations_only, 'references.bib', overwrite = TRUE,
                    library_id=rbbt::bbt_library_id('Graphics Research'),
                    translator='bibtex')
```



```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = F, 
                      eval = T, 
                      fig.path="./figures/", # figures are created on the fly, images are pictures like the stimuli
                      fig.env="figure*", 
                      fig.align = "center",
                      out.width = "50%",
                      dpi = 72, # we can set the resolution up to 300 later.
                      cache = FALSE) 
library(readr)
library(tidyverse)
library(knitr)
library(bibtex)
theme_set(theme_bw())

library(readxl)
library(lubridate)
library(patchwork)
library(survey)
#library(ggmosaic)
library(ggpcp)
library(broom)
library(latex2exp)
library(gt) # grammar of tables
library(multcomp)
library(here)

# Kish's Effective Sample Size
neff <- function(weight) {
  n <- length(weight)
  L <- var(weight)/mean(weight)^2
  n/(1+L)
}

# inverse of the logistic function
inv_logit <- function(x) {exp(x)/(1+ exp(x))}

# calculate lambda for combining weights
calculate_lambda <- function(weights1, weights2) {
  d1 <- 1+var(weights1)/mean(weights1)^2
  d2 <- 1+var(weights2)/mean(weights2)^2

  n1 <- length(weights1)
  n2 <- length(weights2)

  denom <- n1/d1 + n2/d2

  lambda <- n1/(d1*denom) 
  lambda
}

vs_repo_local <- here::here("data") # link to the data folder (of the visual_studies repo)
```

## Survey rounds

```{r data-1}
# other github repo. 
round1 <- readRDS(file.path(vs_repo_local, "round1.rds"))
round2 <- readRDS(file.path(vs_repo_local, "round2.rds"))

round1_adj <- round1 %>% 
            filter(ab != "SKIPPED ON WEB", cd != "SKIPPED ON WEB") %>%
            mutate(CorrectAB = ab=="B is bigger", CorrectCD = cd=="D is bigger",
                   CorrectABCD = paste(CorrectAB, CorrectCD))

round2_adj <- round2 %>% 
            filter(ab != "SKIPPED ON WEB", cd != "SKIPPED ON WEB") %>%
            mutate(CorrectAB = ab=="B is bigger", CorrectCD = cd=="D is bigger",
                   CorrectABCD = paste(CorrectAB, CorrectCD))

survey1 <- survey::svydesign(
  data = round1_adj, 
  ids = ~CaseId, weights = ~weight)

survey2 <- survey::svydesign(
  data = round2_adj,
  ids = ~CaseId, weights = ~weight)
```

```{r data-rnd6}
# other github repo. 
round6 <- readRDS(file.path(vs_repo_local, "round6.rds"))

round6_adj <- round6 %>% mutate(
  weight = WEIGHT_NOLA
) %>% filter(RND_01 == 0) # same type of questions as round 7
```



```{r data-rnd7}
# other github repo. 
round7 <- readRDS(file.path(vs_repo_local, "round7.rds"))

# filter out all of the 'skipped on web':
round7_adj <- round7 %>% mutate(
  weight = WEIGHT_NOLA
)
```


```{r data-rnd3}
round3 <- readRDS(file.path(vs_repo_local, "round3.rds"))

# filter out all of the 'skipped on web':
round3_adj <- round3 %>% mutate(
  weight = WEIGHT_NOLA
) %>% filter(
  ab != "SKIPPED ON WEB",
  cd != "SKIPPED ON WEB"
)
```


The data for this paper were collected in several rounds as part of the NORC Omnibus. 

| Name    | Date  | # Participants | effective sample size | Sum of weights $\sum_i w_i$ |
|---------|-------|--------:|----------:|----------:|
| Round 1 | April 2022 |  `r nrow(round1_adj)`    |    `r round(neff(round1_adj$weight),1)`       | `r round(sum(round1_adj$weight),1)`       |
| Round 2 | May 2022 |  `r nrow(round2_adj)`   |    `r round(neff(round2_adj$weight),1)`       | `r round(sum(round2_adj$weight),1)`       |
| Round 3 | Jun 2022 |  `r nrow(round3_adj)`  |    `r round(neff(round3_adj$weight),1)`       | `r round(sum(round3_adj$weight),1)`       |
| Round 6 | Sep 2022 |  `r nrow(round6_adj)` [Split]  |    `r round(neff(round6_adj$weight),1)`       | `r round(sum(round6_adj$weight),1)`       |
| Round 7 | Oct 2022 |  `r nrow(round7_adj)`   |    `r round(neff(round7_adj$weight),1)`       | `r round(sum(round7_adj$weight),1)`       |
|         |       |         |           |           |   

: Survey rounds: dates, number of participants (nominal sample size), effective sample size, and sum of weights. {#tbl-rounds}



We are using a strategy of *combining* (rather than cumulating) surveys $S_1$ and $S_2$, as described in @omuircheartaighCombiningSamplesVs2002,  by 
multiplying weights in $S_1$ and $S_2$ by $\lambda$ and $1 - \lambda$, respectively.
$$
\lambda = \frac{n_1/d_1}{n_1/d_1 + n_2/d_2},
$$
where $n_1$ and $n_2$ are the nominal sample sizes and $d_1$ and $d_2$ are the design effects for the estimators. 
Instead of using design effects itself, $d_1$ and $d_2$ are estimated as 
$$
d_1 = 1 + CV(w_i \in S_1)^2 \ \ \ \text{ and } \ \ \ d_2 = 1 + CV(w_i \in S_2)^2
$$
$CV$ is the coefficient of variation of the weights within each sample, and is estimated as @kishSurveySampling1965

$$ 
CV(w \in S) = \frac{\widehat{Var(w)}}{\bar{w}^2}.
$$

@omuircheartaighCombiningSamplesVs2002 estimate
$\lambda$ separately for any combination of race/ethnicity by sex. 
We will use that strategy whenever we include demographic variables in the analysis, otherwise we will use a single adjustment for the weights.

All calculations are done in R [@RLanguage] using the `survey` package [@lumleyAnalysisComplexSurvey2004] version 4.0 [@survey] based on @lumleyComplexSurveysGuide2010.



## Model 1: Comparing Aligned and Unaligned Tiles in Vertical Stacked Barcharts {#sec-model1}

```{r lambdas}

d1 <- 1+var(round1_adj$weight)/mean(round1_adj$weight)^2
d2 <- 1+var(round2_adj$weight)/mean(round2_adj$weight)^2

n1 <- nrow(round1_adj)
n2 <- nrow(round2_adj)

neff1 <- n1/d1 # same as neff(round1_adj$weight)
neff2 <- n2/d2

denom <- n1/d1 + n2/d2

lambda <- n1/(d1*denom) # same as calculate_lambda(round1_adj$weight, round2_adj$weight)
```


```{r combine_rounds_1_and_2}
# all weights are scaled to add to 1 first, then lambda is used to 
# get the convex weight assignment, 
# then the weights are scaled to the effective sample size
round_12 <- round1_adj %>% 
            mutate(weight = lambda*weight/sum(weight)) %>% 
            dplyr::select(CaseId, ab, cd, CorrectAB, CorrectCD, CorrectABCD,
                   ab_certain, cd_certain, weight) %>%
    rbind(round2_adj %>% 
            mutate(weight = (1-lambda)*weight/sum(weight)) %>% 
            dplyr::select(CaseId, ab, cd, CorrectAB, CorrectCD, CorrectABCD, 
                   ab_certain, cd_certain, weight))

round_12$weight <- neff(round_12$weight)/sum(round_12$weight)*round_12$weight 
# does not affect the further analysis, but helps with the interpretation of results

round_12_long <- round_12 %>% 
  pivot_longer(CorrectAB:CorrectCD, names_to = "Question", values_to = "Response") 


survey_12 <- 
  survey::svydesign(
  data = round_12_long, 
  ids = ~CaseId, weights = ~weight)
```

The data used for this is a combination of rounds 1 and 2, with $\lambda$ = `r round(lambda[1],3)` for an effective sample size of `r round(neff(round_12$weight),1)`. @fig-tasks-12 shows the two stacked barcharts shown to all panelists.

```{r}
#| label: fig-tasks-12
#| fig-cap: "The two stacked barcharts every participant got to see. In each barchart, the two marked tiles are to be compared for their size. in both instances, the tile on the right is (very slightly) larger."
#| fig-height: 3
#| fig-width: 4
#| out-width: 50%
#| layout-nrow: 1
#| fig-subcap:
#| - "**Unaligned tiles:** Is A bigger than B?"
#| - "**Aligned tiles:** Is C bigger than D?"
knitr::include_graphics("images/vertical-ab.PNG")
knitr::include_graphics("images/vertical-cd.PNG")
```



```{r ttest}
paired <- svyttest(Response~Question, survey_12)
logit_12 <- svyglm(Response ~ Question, survey_12, family = quasibinomial())
coefs_12 <- broom::tidy(logit_12)
abcd <- svyby(~Response, by=~Question, survey_12, svytotal)

marginal_results <- data.frame(abcd)
```


### Binary response

Defining an accurate response as "B is bigger" in the chart of unaligned tiles, and "D is bigger" in the aligned case, while calling the other two responses as incorrect, leads to the data shown in @fig-alignment(a): we see
 that more than twice the number of responses is accurate, when the tiles are aligned along the same axis. 
Because each particpant was shown both versions of the chart, we can use a paired $t$-test to compare mean accuracy between the two charts. The difference in mean accuracy is `r round(paired$estimate,2)`. This difference is highly significant ($t$ statistic: `r round(paired$statistic,2)`, df: `r paired$parameter`, $p$-value: < 2.2e-16). 



```{r accuracy, fig.height = 5, fig.width = 10, warning=FALSE}
#| fig-cap: "On the left (a), a stacked barchart shows the number of respondents with correct (blue) and incorrect (orange) responses to the two comparison questions. When tiles are aligned along the same axis, more than twice the number of responses is accurate. The shaded area along the top of the blue tiles corresponds to  95\\% confidence intervals around (marginal) correct responses. On the right (b), a parallel coordinate plot shows all combinations of responses. There's a huge asymmetry in the number of responses, where participants answered only one of the questions correctly. A lot more responses are correct when comparing aligned tiles than unaligned tiles."
#| label: fig-alignment
#| out-width: 90%
#| fig-env: figure

gg1 <- round_12_long %>%
  mutate(
    Response = ifelse(Response, "Correct", "Not correct"),
    Response = factor(Response, levels=c("Not correct", "Correct")),
    Question = ifelse(Question=="CorrectAB",
                      "Unaligned: A or B?", "Aligned: C or D?")
  ) %>%
#  filter(Response == "Correct") %>%
  ggplot(aes(x = Question, fill = Response, weight = weight)) +
  geom_bar() +
  ylab("Number of (effective) Respondents") +
  xlab("") +
  scale_fill_manual(values=c("orange", "steelblue")) +
  scale_y_continuous(
      sec.axis = sec_axis(~./sum(round_12$weight)*100,
                          name="Percent responses")
  ) +
  theme(legend.position = "bottom") +
  ggtitle("(a) Accuracy of survey responses")  +
  geom_rect(
    aes(
      x = rev(c("Aligned: C or D?", "Unaligned: A or B?")),
      xmin = 2:1 - 0.45,
      ymin = ResponseTRUE - 1.96* se.ResponseTRUE,
      xmax = 2:1 + 0.45,
      ymax = ResponseTRUE + 1.96* se.ResponseTRUE,
      weight = 1),
    fill = "black", alpha = 0.25, #colour = "white", linewidth = 0.25,
    data = marginal_results
  )

gg2 <- round_12 %>%
  mutate(
    `Unaligned: A or B?` = ifelse(CorrectAB, "Correct", "Not correct"), 
    `Aligned: C or D?` = ifelse(CorrectCD, "Correct", "Not correct"),
    changed = `Unaligned: A or B?` != `Aligned: C or D?`,
    changed = ifelse(changed, "changed", "unchanged")
    ) %>%
  pcp_select(`Aligned: C or D?`, `Unaligned: A or B?`) %>%
  pcp_scale() %>%
  pcp_arrange() %>%
  ggplot(aes_pcp()) + 
    geom_pcp_axes() + 
    geom_pcp(aes(colour = changed), alpha = 0.2)+
    geom_pcp_boxes(fill=NA, colour = "black", linewidth=0.25) + 
    geom_pcp_labels(colour = "black") +
  scale_y_continuous("Number responses", breaks=seq(0,1,by=0.25), 
                     labels = round(sum(round_12$weight)*seq(0,1,by=0.25),0),
                     sec.axis = sec_axis(~.*100,
                          name="Percent responses")) +
  xlab("") + 
  scale_colour_manual("Response", values = c("purple", "grey")) +
  ggtitle("(b) Combinations of responses") +
  theme(legend.position = "bottom") +
  guides(colour=guide_legend(override.aes = list(alpha = 1)))

gg1 + gg2
```

This test is equivalent to a logistic regression with response $Y_{ij}$, the response of participant $i$  to question $Q_j$ $j = 1/2$ is correct (=1) or incorrect (=0):
$$
\text{logit } P \left(Y = 1 \mid Q_j \right) = \alpha + \beta_j, 
$$
We assume that $Y \mid Q_j$ has a Bernoulli distribution with success probability $p_j$, $j = 1, 2$. 
For the purpose of estimability, we will assume that $\beta_1 = 0$, i.e. $\text{logit} (p_1) = \alpha$ and $\beta_2 = \text{logit}(p_2) - \text{logit}(p_1)$. The odds of an accurate answer with aligned bars is therefore  exp (`r round(coefs_12$estimate[2],1)`) =  `r round(exp(coefs_12$estimate[2]),1)` times higher than for the unaligned bars of Question 1. Estimates for this model fit are shown in @tbl-coefs-12.

```{r}
#| label: tbl-coefs-12
#| tbl-cap: Estimates for the logit model of accuracy by task
coefs_12$term <- c("$$\\widehat{\\alpha}$$", "$$\\widehat{\\beta}_2$$")

coefs_12 %>% 
  mutate(
    p.value = ifelse(p.value<0.0001, "< 0.0001", sprintf("%.4f", p.value)),
        term = ifelse(term=="(Intercept)", "Intercept", term)
  ) %>%
  gt() %>%
  tab_header("Estimates for Model 1",
             " accuracy of responses for unaligned (Question 1) and aligned bars (Question 2)") %>%
  cols_label(
    std.error = "SE",
    statistic = "t-statistic",
    p.value = "p-value"
  ) %>%
  fmt_number(
    columns = c(estimate, std.error),
    decimals = 2
  ) %>%
  fmt_number(
    columns = statistic,
    decimals = 1
  ) %>%
  cols_align(
    columns = p.value,
    align = "right"
  )
```


### Ordinal response

```{r data-ordinal}
round_12_ord_long <- round_12 %>% 
  pivot_longer(ab:cd, names_to = "Question", values_to = "Response") 

round_12_ord_long <- round_12_ord_long %>% 
  mutate(
    Response3 = ifelse(Response %in% c("B is bigger", "D is bigger"), "Y is bigger (Correct)", 
                      ifelse(Response == "They are the same", "They are the same", "X is bigger (Wrong)")),
    Response3 = factor(Response3, levels = c("X is bigger (Wrong)", "They are the same", "Y is bigger (Correct)")),
    Question = ifelse(Question=="ab",
                      "Unaligned: A or B?", "Aligned: C or D?")
  )

survey_12_ord <- 
  survey::svydesign(
  data = round_12_ord_long, 
  ids = ~CaseId, weights = ~weight)
```

```{r}
paired2 <- svyttest(I(Response3=="X is bigger (Wrong)")~Question, survey_12_ord)
```


When moving beyond the binary accuracy measure of the response and using all three levels of the response as dependent variable, we see that by far the largest change in responses comes from a change from 'they are the same' to the (perceptually) correct response of 'D is bigger' when the tiles are aligned along the same axis for a comparison (see @fig-ordinal). Even though the number of wrong responses (orange) is only slightly different (`r round(paired2$estimate,3)`) between aligned and unaligned comparisons, it is still highly significant ($t$ statistic: `r round(paired2$statistic,1)`, df: `r paired2$parameter`, $p$-value: < 2.2e-16). 
 

```{r fig-ordinal, fig.height = 5, fig.width = 10, warning=FALSE}
#| fig-cap: "On the left (a), a stacked barchart shows the number of respondents for each level of response: by aligning tiles, the number of responses of 'they are the same' is cut in half.  On the right (b), a parallel coordinate plot shows all combinations of responses. The largest nuber of changes are from 'D is bigger' to 'they are the same'. "
#| label: fig-ordinal
#| out-width: 80%
#| fig-env: figure

gg1 <- round_12_ord_long %>%
#  filter(Response == "Correct") %>%
  ggplot(aes(x = Question, fill = Response3, weight = weight)) +
  geom_bar() +
  ylab("Number of (effective) Respondents") +
  xlab("") +
  scale_fill_manual(values=c("darkorange", "grey", "steelblue")) +
  scale_y_continuous(
      sec.axis = sec_axis(~./sum(round_12$weight)*100,
                          name="Percent responses")
  ) +
  theme(legend.position = "none")

gg1_build <- gg1 %>% ggplot2::ggplot_build()
gg1_labels <- gg1_build$data[[1]] %>% mutate(
  x = (xmin+xmax)/2,
  y = (ymin+ymax)/2,
  label = c("C is bigger (Wrong)", "They are the same", "D is bigger (Correct)",
            "A is bigger (Wrong)", "They are the same", "B is bigger (Correct)"),
  Response3 = rep(c("X is bigger (Wrong)", "They are the same", "Y is bigger (Correct)"),2)
)

gg1 <- gg1 +
  ggtitle("(a) Three levels of survey responses")  +
  geom_label(aes(x = x, y = y, label=label, weight=NULL), 
             fill="white", alpha = 0.5, colour="black", data = gg1_labels)
  

gg2 <- round_12 %>%
  mutate(
    `Unaligned: A or B?` = factor(ab, levels=c("B is bigger", "They are the same", "A is bigger")), 
    `Aligned: C or D?` = factor(cd, levels=c("D is bigger", "They are the same", "C is bigger")), 
    changed = (as.numeric(ab) != as.numeric(cd)),
    changed = ifelse(changed, "changed", "unchanged")
    ) %>%
  pcp_select(`Aligned: C or D?`, `Unaligned: A or B?`) %>%
  pcp_scale() %>%
  pcp_arrange() %>%
  ggplot(aes_pcp()) + 
    geom_pcp_axes() + 
    geom_pcp(aes(colour = changed), alpha = 0.2)+
    geom_pcp_boxes(fill=NA, colour = "black", linewidth=0.25) + 
    geom_pcp_labels(colour = "black") +
  scale_y_continuous("Number responses", breaks=seq(0,1,by=0.25), 
                     labels = round(sum(round_12$weight)*seq(0,1,by=0.25),0),
                     sec.axis = sec_axis(~.*100,
                          name="Percent responses")) +
  xlab("") + 
  scale_colour_manual("Response", values = c("purple", "grey")) +
  ggtitle("(b) Combinations of responses") +
  theme(legend.position = "bottom") + 
  guides(colour=guide_legend(override.aes = list(alpha = 1)))

gg1 + gg2
```


```{r model-ord}
#| label: tbl-logodds
#| tbl-cap: A change from aligned to unaligned tiles is detrimental to the level of correct (or correct and same) responses.

#model1_ord <- svyloglin(~Response3*Question, survey_12_ord)

# two logisting regressions
model1_logist1 <- svyglm(I(Response3 == "Y is bigger (Correct)")~Question, survey_12_ord, family = quasibinomial())
# parameter Questioncd is effect of increase in accuracy due to alignment

model1_logist2 <- svyglm(I(Response3 %in% c("Y is bigger (Correct)", "They are the same"))~Question, survey_12_ord, family = quasibinomial())
# For aligned tiles, 4.55 percentage points more respondents correctly pick D as the correct response or said the two tiles were the same size than picked B (or said they were the same size) in the unaligned tiles. 
# exp(1.93092+0.49588)/(1 + exp(1.93092+0.49588)) - exp(1.93092)/(1 + exp(1.93092))

contrasts1 <- as.data.frame(summary(model1_logist1)$coefficients)
contrasts2 <- as.data.frame(summary(model1_logist2)$coefficients)
contrasts <- data.frame(c("correct | same or wrong", "correct or same | wrong"), rbind(contrasts1[2,], contrasts2[2,]), 
percent = 100*c(inv_logit(sum(model1_logist1$coefficients)) - inv_logit(model1_logist1$coefficients[1]),
inv_logit(sum(model1_logist2$coefficients)) - inv_logit(model1_logist2$coefficients[1]))
)
names(contrasts) <- c("level","log odds", "SE", "t-statistic", "p-value", "percent change")
knitr::kable(contrasts, digits = c(NA, 1, 2, 1, 4, 1), row.names = FALSE)
```

### How certain are participants?

```{r certainty-data}
round_12_certainty_long <- round_12 %>% 
  pivot_longer(ab_certain:cd_certain, names_to = "Question", values_to = "Response") %>%
  filter(Response != "SKIPPED ON WEB") %>%  # 13 participants
  mutate(
    Correct = ifelse(Question == "ab_certain", CorrectAB, CorrectCD),
    Question = ifelse(Question=="ab_certain", "A vs B (Unaligned)", "C vs D (Aligned)"),
# ordering of levels in Response is from extremely=6 to not at all=2 - not very intuitive
    Response = factor(Response, levels = c("not at all", "slightly", "moderately", "very", "extremely")) # gets rid of the 'SKIPPED ON WEB' and makes the ordering go from 1 to 5
  )

survey_12_crt <- 
  survey::svydesign(
  data = round_12_certainty_long, 
  ids = ~CaseId, weights = ~weight)
```

```{r}
#| label: fig-certainty
#| out-width: 45%
#| fig-height: 5
#| fig-width: 5
#| fig-cap: "Two views of certainty: The barcharts in (a) show that aligning bars along the same access is associated with more participants who are very certain of their response compared to the unaligned comparison of A vs B, where the majority of participants resports 'moderate' certainty. In the barcharts in (b) we see that participants are not quite as certain of their responses when their answers are correct as when their answers are not correct. Both of these are significant (see model 2.3)."
#| layout-nrow: 1
#| fig-subcap: 
#|   - Certainty by Task
#|   - Certainty by Accuracy of Response
round_12_certainty_long %>% 
  ggplot(aes(x = Response, weight=weight)) + geom_bar() + 
  ylab("Number of (weighted) responses") + 
  facet_grid(Question~.) +
  ggtitle("Certainty by Task")


round_12_certainty_long %>% 
  ggplot(aes(x = Response, weight=weight)) + geom_bar() + 
  ylab("Number of (weighted) responses") + 
  facet_grid(Correct~.) +
  ggtitle("Certainty by Correctness")

```

Using linear scores for the response of `Certainty`, with `not certain at all` assigned a score of 1 and `extremely certain` assigned a score of 5, we can estimate the effects of task and correctness on certainty  by using  a cell-means model of the form:
$$
Y_{k} = \mu_{ij(k)} + \epsilon_{k},
$$
where $k = 1, ..., N$, $\mu_{ij(k)}$ is average certainty (measured on a scale from 1 to 5) of the four combinations of task and correctness, where $i = 1, 2$ encodes unaligned/aligned, and $j=1,2$ encodes wrong, correct, respectively. We also assume that errors are normally distributed, i.e. $\epsilon_k \stackrel{i.i.d}{\sim} N(0, \sigma^2)$ for all $k = 1, ..., N$. What we find, is that we see the lowest score of certainty for correct responses to the unaligned comparison task. For aligned comparisons, the correctness of the response does not matter for the associated certainty.

```{r model-certainty}
model_certain <- svyglm(as.numeric(Response)~Question:Correct-1, design = survey_12_crt)

coefs_certainty <- broom::tidy(model_certain)
terms <- coefs_certainty$term

coefs_certainty$term <- c("$$\\widehat{\\mu}_{11}$$", "$$\\widehat{\\mu}_{21}$$", "$$\\widehat{\\mu}_{12}$$", "$$\\widehat{\\mu}_{22}$$")

coefs_certainty %>% 
  mutate(
    p.value = ifelse(p.value<0.0001, "< 0.0001", sprintf("%.4f", p.value)),
        term = ifelse(term=="(Intercept)", "Intercept", term)
  ) %>%
  gt() %>%
  tab_header("Estimates for Model 2.3",
             " certainty of responses by task and correctness of response") %>%
  cols_label(
    std.error = "SE",
    statistic = "t-statistic",
    p.value = "p-value"
  ) %>%
  fmt_number(
    columns = c(estimate, std.error),
    decimals = 2
  ) %>%
  fmt_number(
    columns = statistic,
    decimals = 1
  ) %>%
  cols_align(
    columns = p.value,
    align = "right"
  )
```


```{r}
#| out-width: 80%
#| fig-cap: "Certainty by task and correctness. Correct answers in the unaligned task have the least certainty associated with them. An incoorect response on the unaligned task shows a significant boost in certainty. "
round_12_certainty_long %>% 
  ggplot(aes(x = Response, weight=weight)) + geom_bar() + 
  ylab("Number of (weighted) responses") + 
  facet_grid(Correct~Question) +
  geom_vline(
    aes(xintercept = estimate),
    colour = "darkorange",
    data = data.frame(
      coefs_certainty, 
      Correct= as.logical(gsub(".*(FALSE|TRUE)", "\\1", terms)),
      Question = gsub("Question(.*):Correct.*", "\\1", terms))
  )
```


## Model 2: Aligned and unaligned comparisons between facetted barcharts and facetted piecharts {#sec-model2}

The data this section is based on comes from (a part) of Round 6 and all responses from Round 7. 

```{r combine_rnds67}
lambda6 <- calculate_lambda(round6_adj$weight, round7_adj$weight)
round6_adj <- round6_adj %>% 
  mutate(
    weight = lambda6*weight/sum(weight)
  )
round7_adj <- round7_adj %>% 
  mutate(
    weight = (1-lambda6)*weight/sum(weight)
  )

response_6 <- round6_adj %>%
  pivot_longer(
    ab1:cd1, # barcharts 
    names_to = "taskquestion", values_to = "responses") %>%
  filter(!is.na(responses), responses!="SKIPPED ON WEB") %>%
  mutate(
    labelled = toupper(gsub("1", "", taskquestion)),
    task = "Bar"
  )


response_7 <- round7_adj %>%
  pivot_longer(
    matches("NOLA[1-2]_[1-6]"), # only pie charts
    names_to = "taskquestion", values_to = "responses") %>%
  filter(!is.na(responses), responses!="SKIPPED ON WEB") %>%
  separate(col="taskquestion", into=c("task", "question"), sep="_")  %>%
  mutate(
    question = parse_number(question),
    labelled = c("AB", "AC", "AD", "BD", "BC", "CD")[question],
    time_spent = ifelse(parse_number(task) == 1, NOLA1_TOTALTIME, NOLA2_TOTALTIME),
    task = c("Pie\nUn-", "Pie\nA-")[parse_number(task)]
  )

# D < C < A < B.
response_67 <- rbind(
    response_6 %>% dplyr::select(CaseId, labelled, task, responses, 
                                 time_spent = NOLA1_TOTALTIME, weight),
    response_7 %>% dplyr::select(CaseId, labelled, task, responses, 
                                 time_spent, weight)
  ) %>% mutate(
  correct = ifelse(labelled %in% c("AB", "BC", "BD"), responses == "B is bigger", 
              ifelse(labelled %in% c("AC", "AD"), responses == "A is bigger",
                     responses == "C is bigger")),
  correct3 = correct | responses == "They are the same",
  time_spent = parse_number(time_spent),
  response3 = factor(correct+correct3, labels = c("Wrong", "Same", "Correct")),
  labelled = factor(labelled, levels=c("BD", "BC", "AB", "AD", "AC", "CD")),
  signal = ifelse(labelled=="BD", 4, 
             ifelse(labelled=="BC", 3,
               ifelse(labelled %in% c("AB", "AD"), 2, 1))),
  task_labelled = factor(paste(task, labelled, sep="_"))
)


survey_67 <- svydesign(~CaseId, data = response_67, weights = ~weight)
```

A total of three different charts were shown to (a part) of the panelists from Round 6 and all panelists of Round 7. 
Each of the charts has four labelled pieces A, B, C, and D.

```{r}
#| label: fig-tasks-67
#| fig-cap: "Stimuli shown to panelists. For each of the charts, all marked pieces were evaluated pairwise for their size difference. "
#| fig-height: 3
#| fig-width: 4
#| layout-nrow: 1
#| fig-subcap:
#| - "**Round 6**: Facetted barcharts"
#| - "**Round 7**: Facetted piecharts with unaligned wedges"
#| - "**Round 7**: Facetted piecharts with aligned wedges"
knitr::include_graphics("images/Round 6 - Sept 2022.png")
knitr::include_graphics("images/round7/unaligned pie - all.png")
knitr::include_graphics("images/round7/aligned pie - all.png")
```

The questions asked for each of these charts were of the form: "which of the marked pieces is bigger? Just your best guess is fine". 

A.	A or B?

RESPONSE OPTIONS:

1.	A is bigger
2.	B is bigger
3.	They are the same

Each participant was asked a total of six comparisons of this type: A vs B, A vs C, A vs D, B vs C, B vs C, and C vs D. The number of valid responses and the resulting effective sample sizes are shown in @tbl-combine-67


```{r tbl-combine-67}
#| label: tbl-combine-67
#| tbl-cap: Table of the number of effective responses by task 
dframe <- data.frame(
  round = c("Round 6", "Round 7", "Round 7"),
  task = c("Bar", "Pie-Aligned", "Pie-Unaligned"),
  responses = c(nrow(response_6), nrow(response_7 %>% filter(task=="Pie\nA-")), nrow(response_7 %>% filter(task=="Pie\nUn-"))),
  effective = c(neff(response_6$weight), neff((response_7 %>% filter(task=="Pie\nA-"))$weight),
                neff((response_7 %>% filter(task=="Pie\nUn-"))$weight)),
  participants = c(length(unique(response_6$CaseId)),
                   nrow(unique(response_7 %>% filter(task=="Pie\nA-") %>% dplyr::select(CaseId))),
                   nrow(unique(response_7 %>% filter(task=="Pie\nUn-") %>% dplyr::select(CaseId)))),
  lambda = c(calculate_lambda(response_6$weight, response_7$weight), 
             calculate_lambda((response_7 %>% filter(task=="Pie\nA-"))$weight, 
                               c(response_6$weight,(response_7 %>% filter(task=="Pie\nUn-"))$weight)),
             calculate_lambda((response_7 %>% filter(task=="Pie\nUn-"))$weight, 
                               c(response_6$weight,(response_7 %>% filter(task=="Pie\nA-"))$weight)))
)

dframe %>% 
  group_by(round) %>%
  gt() %>%
  tab_header("Number of (effective) responses by task") %>%
  cols_label(
    responses = md("number of responses:<br>nominal"),
    effective = md("number of responses:<br>effective"),
    participants = md("number of participants"),
    lambda = md("λ"),
  ) %>%
  fmt_number(
    everything(), 
    decimals = 0,
    use_seps=TRUE
  ) %>%
  fmt_number(
    columns = effective,
    decimals = 1
  ) %>%
  fmt_number(
    columns = c(lambda),
    decimals = 4
  ) %>%
  tab_options(
    column_labels.font.weight = "bold",
    row_group.font.weight = "bold"
  )
```

The four labelled pieces A, B, C, and D correspond to values 46, 47.6, 45.2, and 44.4, respectively, i.e underlying each evaluation is the order D < C < A < B. Nominally, the differences between the size of the marked elements can be expressed as multiples in $\delta = 0.8$, with $B = A + 2\delta = C + 3 \delta = D + 4 \delta$. A difference of $\delta = 0.8$ corresponds to about a  3 degree difference for the angle of a wedge. We would expect that an increase in the difference in the values increases the accuracy in the responses. This is mitigated by the distance between wedges: more distance (should) lead to a decrease in accuracy. Similarly, if pieces have an axis or a line of reference in common, we would expect an increase in accuracy. 


### Binary and Ordinal Correctness as Response

@fig-response-67 gives an overview of all responses to size assessments of the marked tiles. The panels are ordered according to the difference in signal and hypothesized difficulty from easiest (left) to hardest (right) comparison. We see, that in general the percentage of correct responses decreases from left to right. 
Design does not seem to matter for easy comparisons (BD, BC) or the hard comparison (CD). 
The three comparisons in the middle show interesting patterns, highlighted in the reordered @fig-response-67b. Colored letters below and above the bars encode significances between pairs of bars of the same color: two proportions are significantly different at a 5% significance level, if they do not have a letter in common [@piephoAlgorithmLetterBasedRepresentation2004]. Significances are adjusted for multiple comparisons as implemented in the `multcomp` package [@hothornSimultaneousInferenceGeneral2008]. 

```{r model-67, warning = FALSE, cache=TRUE}
# same model as for plot 
logit_67_correct <- svyglm(correct~task_labelled-1, family=quasibinomial(), design=survey_67)

logit_67_correct3 <- svyglm(correct3~task_labelled-1, family=quasibinomial(), design=survey_67)

ci_confint <- svyby(~correct, by=~task_labelled, design=survey_67, svyciprop)
ci_confint3 <- svyby(~correct3, by=~task_labelled, design=survey_67, svyciprop)


coefs_67 <- broom::tidy(logit_67_correct)
coefs_67_relaxed <- broom::tidy(logit_67_correct3)

# create all pairwise comparisons for tasks and questions.
cross_compare <- glht(logit_67_correct, mcp(task_labelled="Tukey"))
cross_compare3 <- glht(logit_67_correct3, mcp(task_labelled="Tukey"))

# get letters for each confidence 
ci_confint$letters <- cld(cross_compare)$mcletters$Letters
ci_confint3$letters <- cld(cross_compare3)$mcletters$Letters

ci_confint <- ci_confint %>% 
separate_wider_delim(
  task_labelled, delim = "_", names =c("task", "labelled")
)  %>% mutate(
  task = factor(task, levels=c("Bar", "Pie\nA-", "Pie\nUn-")),
  labelled = factor(labelled, levels =c("BD", "BC", "AB", "AD", "AC", "CD"))
)

ci_confint3 <- ci_confint3 %>% 
separate_wider_delim(
  task_labelled, delim = "_", names =c("task", "labelled")
)  %>% mutate(
  task = factor(task, levels=c("Bar", "Pie\nA-", "Pie\nUn-")),
  labelled = factor(labelled, levels =c("BD", "BC", "AB", "AD", "AC", "CD"))
)
```


```{r}
#| label: fig-response-67
#| fig-cap: "Responses for comparisons AB through CD on the three different facetted designs. Responses to the same comparison are shown side-by-side for the three designs. Comparisons are ordered from least difficult (BD) to most difficult (CD). The increasing difficulty of comparisons is reflected in the overall decreasing percentage of correct responses (height of blue tiles). The overlaid rectangles represent 95% confidence intervals. The letters in blue and orange encode significances between pairwise proportions: two bars have a significantly different proportion (at a 5% significance level) if they do not share a letter."
#| fig-height: 4.25
#| fig-width: 7
#| out-width: 80%


props_ci <- as.data.frame(svyby(~response3, by=~interaction(labelled, task), svymean, design = survey_67))
props_ci <- props_ci %>% 
  mutate(
    labels = row.names(props_ci),
  ) 
props_ci <- props_ci %>% separate_wider_delim(
    labels, delim = ".", names =c("labelled", "task")
  ) %>% mutate(
    task = factor(task, levels=c("Bar", "Pie\nA-", "Pie\nUn-")),
    labelled = factor(labelled, levels =c("BD", "BC", "AB", "AD", "AC", "CD"))
  )


response_67 %>% 
  mutate(
    labelled = factor(labelled, levels=c("BD", "BC", "AB", "AD", "AC", "CD"))
  ) %>% 
  ggplot(aes(x = task)) + 
  geom_bar(aes(fill = response3, weight = weight), position="fill") +
  facet_grid(.~labelled) + 
  theme(legend.position="bottom") + 
  scale_fill_manual("Response", values=c("orange", "grey", "steelblue")) +
  xlab("") + 
  geom_rect(
    aes(
      xmin = as.numeric(task) - 0.45,
      ymin = (response3Correct - 1.96* se.response3Correct),
      xmax = as.numeric(task) + 0.45,
      ymax = (response3Correct + 1.96* se.response3Correct),
    ), 
    fill = "black", alpha = 0.25, #colour = "white", linewidth = 0.25,
    data = props_ci
  ) +
  geom_segment(
    aes(
      x = as.numeric(task) - 0.45,
      y = (response3Correct ),
      xend = as.numeric(task) + 0.45,
      yend = (response3Correct )
    ), 
    alpha = 1, colour = "black", linewidth = 0.25,
    data = props_ci
  ) +
  geom_rect(
    aes(
      xmin = as.numeric(task) - 0.45,
      ymin = (1- response3Wrong - 1.96* se.response3Wrong),
      xmax = as.numeric(task) + 0.45,
      ymax = (1-response3Wrong + 1.96* se.response3Wrong),
    ), 
    fill = "black", alpha = 0.25, #colour = "white", linewidth = 0.25,
    data = props_ci
  ) +
  geom_segment(
    aes(
      x = as.numeric(task) - 0.45,
      y = (1- response3Wrong ),
      xend = as.numeric(task) + 0.45,
      yend = (1-response3Wrong )
    ), 
    alpha = 1, colour = "black", linewidth = 0.25,
    data = props_ci
  ) +
  scale_y_continuous("Percent responses", breaks=seq(0,1,by=0.25), labels = 100*seq(0,1,by=0.25), limits = c(-0.1, 1.1)) + 
  geom_text(aes(label=letters), y = -0.05, data = ci_confint, colour = "steelblue") +
  geom_text(aes(label=letters), y = 1.05, data = ci_confint3, colour = "darkorange") 
```

For the facetteed barcharts, there is an abrupt drop in accuracy of responses between 'easy' and 'hard' comparisons. (HH: All of these comparisons could be affected by the context of their light blue neighbors. 'B' is bigger than its light blue neighbor, and also the overall biggest. 'A' is only a bit smaller than 'B', but quite a bit smaller than the light blue bar next to it. This seems to make comparisons with A harder.)  For pie charts the accuracy in responses is reacting a lot more gentle to the increasing difficulty. There is no apparent effect in pie charts due to alignment. However, all comparisons are very close to 50% - the intrinsic reference lines [@lipkusVisualCommunicationRisk1999] in pie charts (at 180 degrees, and to a lesser degree at 90 degree angles) might be helping with this particular set of comparisons.




```{r}
#| label: fig-response-67b
#| fig-cap: "Responses for comparisons AB through CD on the three different facetted designs, reordered. Now, responses to the same design are shown side-by-side across comparisons. Comparisons are ordered from least difficult (BD) to most difficult (CD). The three different designs result in different response patterns for increasing comparison difficulty.  "
#| fig-height: 4.25
#| fig-width: 7
#| out-width: 80%

props_ci <- as.data.frame(svyby(~response3, by=~interaction(labelled, task), svymean, design = survey_67))
props_ci <- props_ci %>% 
  mutate(
    labels = row.names(props_ci),
  ) 
props_ci <- props_ci %>% separate_wider_delim(
  labels, delim = ".", names =c("labelled", "task")
) %>% mutate(
  task = factor(task, levels=c("Bar", "Pie\nA-", "Pie\nUn-")),
  labelled = factor(labelled, levels =c("BD", "BC", "AB", "AD", "AC", "CD"))
)

response_67 %>% 
  mutate(
    labelled = factor(labelled, levels=c("BD", "BC", "AB", "AD", "AC", "CD"))
  ) %>% 
  ggplot(aes(x = labelled)) + 
  geom_bar(aes(fill = response3, weight = weight), position="fill") +
  facet_grid(.~task) + 
  theme(legend.position="bottom") + 
  scale_fill_manual("Response", values=c("orange", "grey", "steelblue")) +
  xlab("") + 
  geom_rect(
    aes(
      xmin = as.numeric(labelled) - 0.45,
      ymin = (response3Correct - 1.96* se.response3Correct),
      xmax = as.numeric(labelled) + 0.45,
      ymax = (response3Correct + 1.96* se.response3Correct),
    ), 
    fill = "black", alpha = 0.25, #colour = "white", linewidth = 0.25,
    data = props_ci
  ) +
  geom_segment(
    aes(
      x = as.numeric(labelled) - 0.45,
      y = (response3Correct ),
      xend = as.numeric(labelled) + 0.45,
      yend = (response3Correct )
    ), 
    alpha = 1, colour = "black", linewidth = 0.25,
    data = props_ci
  ) +
  geom_rect(
    aes(
      xmin = as.numeric(labelled) - 0.45,
      ymin = (1- response3Wrong - 1.96* se.response3Wrong),
      xmax = as.numeric(labelled) + 0.45,
      ymax = (1-response3Wrong + 1.96* se.response3Wrong),
    ), 
    fill = "black", alpha = 0.25, #colour = "white", linewidth = 0.25,
    data = props_ci
  ) +
  geom_segment(
    aes(
      x = as.numeric(labelled) - 0.45,
      y = (1- response3Wrong ),
      xend = as.numeric(labelled) + 0.45,
      yend = (1-response3Wrong )
    ), 
    alpha = 1, colour = "black", linewidth = 0.25,
    data = props_ci
  ) +
  scale_y_continuous("Percent responses", breaks=seq(0,1,by=0.25), labels = 100*seq(0,1,by=0.25), limits = c(-0.1, 1.1)) + 
  geom_text(aes(label=letters), y = -0.05, data = ci_confint, colour = "steelblue") +
  geom_text(aes(label=letters), y = 1.05, data = ci_confint3, colour = "darkorange") 
```


```{r estimates, eval=FALSE}
coefs_67 %>% 
  mutate(
    term = ifelse(term=="(Intercept)", "Intercept", term),
    p.value = ifelse(p.value<0.0001, "< 0.0001", sprintf("%.4f", p.value))) %>%
  gt() %>%
  tab_header("Estimates for Model 3.2",
             " accuracy of responses across pairwise evalulations") %>%
  cols_label(
    estimate = "log odds",
    std.error = "SE",
    statistic = "t-statistic",
    p.value = "p-value"
  ) %>%
  fmt_number(
    columns = c(estimate, std.error),
    decimals = 2
  ) %>%
  fmt_number(
    columns = statistic,
    decimals = 1
  ) %>%
  cols_align(
    columns = p.value,
    align = "right"
  )
```

Besides intrinsic reference lines, the overall size of the stimulus might be another factor contributing to the difference in accuracy between the tiles and the wedges. In the shown example, the marked tiles have an  area between $133 \times 55 = 7315$  pixels (for D) to  $144 \times 55 = 7920$ pixels (for B), while the pie wedges  are based on circles with a diameter of 105 pixels, and therefore cover visually more than twice the area of the tiles in the barchart. The area of D in form of a pie-wedge is $\frac{44.4}{100} \times 105^2 \pi  \approx$ 15,378 pixels and B has an area of $\frac{47.6}{100} \times 105^2 \pi  \approx$ 16,487 pixels.

There are different possible strategies for evaluating the size of a wedge in a pie chart. Panelists can use the **angle** of the wedge (or a suitable derivative of it, such as the complementary angle or the remainder to 180 degrees), its **area** (as part of the circle or some derivative) or the **arc length** of the wedge. Studies by  @kosaraJudgmentErrorPie2016 and @kosaraEvidenceAreaPrimary2019 show that the area of a piece in a pie chart is the primary evaluation method.


### How much time did participants spend on each task?

Note: we do not have certainty for round 7. We also do not have timing by individual comparisons.

```{r vis-time-pie}
#| label: fig-time-67
#| fig-cap: "Comparisons for aligned pie slices were on average the fastest. Comparisons in the facetted barcharts took the longest. The difference in average times are significant for aligned pies versus either of the other designs, but not different between barcharts and unaligned pie charts."
response_67 %>%
  mutate(task = reorder(task, time_spent)) %>%
  ggplot(aes(y = task, x = time_spent)) + 
  geom_boxplot() +
  scale_x_log10(breaks=c(1, 5, 10, 20, 30, 60, 120, 300, 600), labels = c("1 sec", "5 secs", "10", "20", "30", "1 min", "2", "5", "10 mins")) + 
  ylab("") + xlab("") + 
  ggtitle("Time spent on comparing all six pairs by design") 
```

```{r model-time}
time_mod <- svyglm(time_spent~task, design = survey_67)
coef_time <- broom::tidy(time_mod)

coef_time %>% mutate(
    p.value = ifelse(p.value<0.0001, "< 0.0001", sprintf("%.4f", p.value)),
    term = ifelse(term=="(Intercept)", "Intercept", term)
  ) %>%
  gt() %>%
  tab_header("Estimates for Model 3.2",
             " time spent (in seconds) on assessing the six pairs") %>%
  cols_label(
    std.error = "SE",
    statistic = "t-statistic",
    p.value = "p-value"
  ) %>%
  fmt_number(
    columns = c(std.error),
    decimals = 2
  ) %>%
  fmt_number(
    columns = c(estimate, statistic),
    decimals = 1
  ) %>%
  cols_align(
    columns = p.value,
    align = "right"
  )

```
## Model 3: Aligned and unaligned comparisons between horizontal and vertical barcharts {#sec-model3}

We are mostly interested in a comparison between this vertical design and the **wide horizontal** design favored by many NORC designers (XXX can I say that this way?). This design *should* help two-fold with decision making, because 

  a. the bars are overall wider, increasing the physical representation of the stimulus, 
  b. the bars are closer together. 
    
Both of these factors have been shown [@luModelingJustNoticeable2022] to be beneficial for an accurate assessment of small differences between bars.

In order to separate these two effects from the effect of horizontal vs vertical assessment of bars, we introduce the **horizontal** design as a transition, in which the bars are kept to the exact same size as in the vertical design. Similarly, the spacing of the bars is identical between these two designs.

```{r round3-participants}
# P_EXP is 1 or 2:
# panelists in 1 see HZ1-AB.png and HZ1-CD.png (horizontal tall)
# panelists in 2 see HZ2-AB.png and HZ2-CD.png (horizontal wide)
n_tall <- round3_adj %>% filter(P_EXP == 1) %>% nrow()
n_tall_eff <- round3_adj %>% filter(P_EXP == 1) %>% pull(weight) %>% neff()

n_wide <- round3_adj %>% filter(P_EXP == 2) %>% nrow()
n_wide_eff <- round3_adj %>% filter(P_EXP == 2) %>% pull(weight) %>% neff()
```

We use the results from rounds 1 and 2 to assess perceptual accuracy in the vertical stacked barchart (as shown in @fig-tasks-12). 
@fig-designs shows the two designs shown to participants of Round 3. The survey was administered using a split design on the type of visualization, but all participants were asked to compare the size of two unaligned tiles (A and B) and two aligned tiles (C and D). Roughly half ($n_1 = $ `r n_tall` for an efficient sample size of `r round(n_tall_eff,1)`) of the round 6 panelists were shown the horizontal tall barchart on the left, the other participants ($n_2 =$   `r n_wide`; efficient $n_2^{(eff)} =$    `r round(n_wide_eff,1)`) saw the horizontal wide barchart.

```{r}
#| label: fig-designs
#| fig-cap: Design and stimulus of round 3
#| fig-subcap:
#|   - Horizontal tall, stacked bars, unaligned
#|   - Horizontal wide, stacked bars, unaligned
#|   - Horizontal tall, stacked bars, aligned
#|   - Horizontal wide, stacked bars, aligned
#| layout-nrow: 2   

knitr::include_graphics("images/round3/horizontal-ab.PNG")
knitr::include_graphics("images/round3/hwide-ab.PNG")
knitr::include_graphics("images/round3/horizontal-cd.PNG")
knitr::include_graphics("images/round3/hwide-cd.PNG")
```

```{r combine-123}
lambda_1_2 <- calculate_lambda(round1_adj$weight, round2_adj$weight)

round_12 <- round1_adj %>% 
            mutate(weight = lambda_1_2*weight/sum(weight),
                   round = "Round 1") %>% 
            dplyr::select(CaseId, ab, cd, CorrectAB, CorrectCD, CorrectABCD,
                   ab_certain, cd_certain, ab_zoom, cd_zoom, round, weight) %>%
    rbind(round2_adj %>% 
            mutate(weight = (1-lambda_1_2)*weight/sum(weight),
                   round = "Round 2") %>% 
            dplyr::select(CaseId, ab, cd, CorrectAB, CorrectCD, CorrectABCD, 
                   ab_certain, cd_certain, ab_zoom, cd_zoom, round, weight))

lambda_12_3 <- calculate_lambda(round_12$weight, round3_adj$weight)

round_123 <- round_12 %>% 
              mutate(weight = lambda_12_3*weight/sum(weight),
                     chart = "Vertical") %>% 
  rbind(
    round3_adj %>% 
      mutate(
        weight = (1-lambda_12_3)*weight/sum(weight),
        round = "Round 3",
        CorrectAB = ab == "B is bigger",
        CorrectCD = cd == "D is bigger",
        CorrectABCD = paste(CorrectAB, CorrectCD),
        chart = ifelse(P_EXP==1, "Horizontal", "Horizontal wide")
       ) %>% 
        dplyr::select(CaseId, ab, cd, CorrectAB, CorrectCD, CorrectABCD,
                   ab_certain, cd_certain, ab_zoom, cd_zoom, round, chart, weight) 
  )
  
round_123$weight <- neff(round_123$weight)/sum(round_123$weight)*round_123$weight 
# does not affect the further analysis, but helps with the interpretation of results

round_123_long <- round_123 %>% 
  pivot_longer(ab:cd, names_to = "task", values_to = "Response") 

round_123_long <- round_123_long %>% 
  mutate(
    correct = Response %in% c("B is bigger", "D is bigger"),
    correct3 = correct | (Response == "They are the same"),
    task_chart = factor(paste(task, chart, sep="-"))
  )

survey_123 <- 
  survey::svydesign(
  data = round_123_long, 
  ids = ~CaseId, weights = ~weight)
```


We combine rounds 1, 2 and 3 as shown in @tbl-combine-123 to incorporate all relevant responses to evaluate the three different designs of charts in terms of their accuracy in assessing small differences in tile sizes. 



```{r tbl-combine-123, warning = FALSE, message = FALSE}
#| label: tbl-combine-123
#| tbl-cap: Table of the number of effective responses by type of visualization and round

lambdas <- round_123_long %>% group_by(round, chart) %>% 
  summarize(
    lambda = calculate_lambda(
      round_123_long$weight[round_123_long$chart==chart[1] & round_123_long$round==round[1]],
      round_123_long$weight[!(round_123_long$chart==chart[1] & round_123_long$round==round[1])])
  )

dframe <- round_123_long %>% group_by(round, chart) %>% 
  summarize(
    nominal = n(),
    effective = round(neff(weight),1),
    participants = length(unique(CaseId))
  ) %>% left_join(
    lambdas, by = c("round", "chart")
  )


dframe %>% 
  group_by(round) %>%
  gt() %>%
  tab_header("Number of (effective) responses by task") %>%
  cols_label(
    nominal = md("number of responses:<br>nominal"),
    effective = md("number of responses:<br>effective"),
    participants = md("number of participants"),
    lambda = md("λ"),
  ) %>%
  fmt_number(
    everything(), 
    decimals = 0,
    use_seps=TRUE
  ) %>%
  fmt_number(
    columns = effective,
    decimals = 1
  ) %>%
  fmt_number(
    columns = c(lambda),
    decimals = 4
  ) %>%
  tab_options(
    column_labels.font.weight = "bold",
    row_group.font.weight = "bold"
  )
```
```{r model-123, warning = FALSE, message = FALSE}

logit_123_correct <- svyglm(correct~task_chart-1, family=quasibinomial(), design=survey_123)
logit_123_correct3 <- svyglm(correct3~task_chart-1, family=quasibinomial(), design=survey_123)


ci_confint <- svyby(~correct, by=~task_chart, design=survey_123, svyciprop)
ci_confint3 <- svyby(~correct3, by=~task_chart, design=survey_123, svyciprop)


coefs_123 <- broom::tidy(logit_123_correct)
coefs_123_relaxed <- broom::tidy(logit_123_correct3)

# create all pairwise comparisons for tasks and questions.
cross_compare <- glht(logit_123_correct, mcp(task_chart="Tukey"))
cross_compare3 <- glht(logit_123_correct3, mcp(task_chart="Tukey"))

# get letters for each confidence 
ci_confint$letters <- cld(cross_compare)$mcletters$Letters
ci_confint3$letters <- cld(cross_compare3)$mcletters$Letters

ci_confint <- ci_confint %>% 
  separate_wider_delim(
    task_chart, delim = "-", names =c("task", "chart")
    )  %>% mutate(
  task = factor(task, levels=c("cd", "ab"), labels=c("Aligned", "Unaligned")),
  chart = factor(chart, levels =c("Vertical", "Horizontal", "Horizontal wide"))
)

ci_confint3 <- ci_confint3 %>% 
  separate_wider_delim(
    task_chart, delim = "-", names =c("task", "chart")
    ) %>% mutate(
  task = factor(task, levels=c("cd", "ab"), labels=c("Aligned", "Unaligned")),
  chart = factor(chart, levels =c("Vertical", "Horizontal", "Horizontal wide"))
) 
```


```{r}
#| label: fig-response-123
#| fig-cap: "Responses for accuracy in the three designs. Responses to the same task are shown side-by-side for the three designs.  The overlaid rectangles represent 95% confidence intervals. The letters in blue and orange encode significances between pairwise proportions: two bars have a significantly different proportion (at a 5% significance level) if they do not share a letter. There is no significant difference between the three designs for wrong responses. When tiles are unaligned, the horizontal wide barchart is showing the highest accuracy. For aligned tile, the horizontal wide design and the vertical design do not show a significant difference in accuracy."
#| fig-height: 4.25
#| fig-width: 7
#| out-width: 80%

round_123_long %>% 
  mutate(
    response3 = c("Wrong", "They are the same", "Correct")[correct+correct3+1],
    response3 = factor(response3, levels=rev(c("Correct", "They are the same", "Wrong"))),
    task = factor(task, levels=c("cd", "ab"), labels=c("Aligned", "Unaligned")),
    chart = factor(chart, levels=c("Vertical", "Horizontal", "Horizontal wide"))
  ) %>% 
  ggplot(aes(x = chart)) + 
  geom_bar(aes(fill = response3, weight = weight), position="fill") +
  facet_grid(.~task) + 
  theme(legend.position="bottom") + 
  scale_fill_manual("Response", values=c("orange", "grey", "steelblue")) +
  xlab("") + 
  geom_rect(
    aes(
      xmin = as.numeric(chart) - 0.45,
      ymin = correct - 1.96* `se.as.numeric(correct)`,
      xmax = as.numeric(chart) + 0.45,
      ymax = correct + 1.96* `se.as.numeric(correct)`,
    ), 
    fill = "black", alpha = 0.25, #colour = "white", linewidth = 0.25,
    data = ci_confint
  ) +
  geom_segment(
    aes(
      x = as.numeric(chart) - 0.45,
      y = correct,
      xend = as.numeric(chart) + 0.45,
      yend = correct
    ), 
    alpha = 1, colour = "black", linewidth = 0.25,
    data = ci_confint
  ) +
  geom_rect(
    aes(
      xmin = as.numeric(chart) - 0.45,
      ymin = correct3 - 1.96* `se.as.numeric(correct3)`,
      xmax = as.numeric(chart) + 0.45,
      ymax = correct3 + 1.96* `se.as.numeric(correct3)`,
    ), 
    fill = "black", alpha = 0.25, #colour = "white", linewidth = 0.25,
    data = ci_confint3
  ) +
  geom_segment(
    aes(
      x = as.numeric(chart) - 0.45,
      y = correct3,
      xend = as.numeric(chart) + 0.45,
      yend = correct3
    ), 
    alpha = 1, colour = "black", linewidth = 0.25,
    data = ci_confint3
  ) +
  scale_y_continuous("Percent responses", breaks=seq(0,1,by=0.25), labels = 100*seq(0,1,by=0.25), limits = c(-0.1, 1.1)) + 
  geom_text(aes(label=letters), y = -0.05, data = ci_confint, colour = "steelblue") +
  geom_text(aes(label=letters), y = 1.05, data = ci_confint3, colour = "darkorange") 
```


### Accuracy in different barchart designs

@fig-response-123 shows the results of a cell-means model with ordinal response $Y_k$, where $Y_k$ is the $k$th participant's response, $Y_k \in \{1, 2, 3\}$, where 'correct' is encoded as 1, 'they are the same' is encoded as 2, and 'wrong' is encoded as 3:

$$
\text{logit }P(Y_k \le \ell) = \mu_{ij\ell(k)},
$$
where  $\ell \in \{1, 2\}$; $i \in \{1, 2\}$ is the comparison type (1 = Aligned, 2 = Unaligned),  and $j \in \{1, 2, 3\}$ is the chart design, with 1 = Vertical, 2 = Horizontal, and 3 = Horizontal wide. 



Interestingly, while we should, theoretically, see an improvement in accuracy when shifting from the vertical to the horizontal wide design, the resulting effects on the accuracy of the responses are not completely straightforward: 
The shift from a vertical to the (tall) horizontal design is detrimental to an accurate perception for both aligned and unaligned comparisons.  
The re-scaled design of the wide horizontal bars reclaims some of the loss for unaligned bars and outperforms the vertical design by a similar margin in aligned bars, but does not out-perform the vertical design when comparing unaligned tiles. 


### Zooming

A contributing factor to this outcome might be the way that participants interact with the different designs. Generally, about half of all participants make use of the option to zoom into charts - and it can be shown that zooming into charts helps with the overall accuracy (which is in agreement with the findings by @luModelingJustNoticeable2022 about the physical size of stimuli). The rate of zooming into charts is about twice as high for participants filling out the survey on their smart phones than on other, usually bigger devices such as tablets or desktops. However, different designs lead to different rates of zooming: when dealing with the vertical design, the rate of zooming is higher than for the two horizontal designs.


### Certainty


## Model X: Comparing Aligned and Unaligned Elements Facets {#sec-modelX}

The survey setup and all stimuli are shown in @tbl-tasks-7. A total of `r nrow(round7_adj)` panelists  (Effective sample size: `r round(neff(round7_adj$weight), 1)`) were shown four sets of facetted charts each. For questions 3 and 4, the panel was split into two (roughly) halves. One set of panelists was shown the floating wedges, while the other half was shown the framed wedges on the right. 

For each chart, panelists were asked to compare all marked tiles against each other.  

|                              |                                |                                |
|------------------------------|--------------------------------|--------------------------------|
|   **Everybody:**       |  **Question 1:** Facetted Pie, unaligned | **Question 2:** Facetted Pie, aligned |
|   | ![Facetted pie charts, unaligned](images/round7/unaligned pie - all.png)   |   ![Facetted pie charts, aligned](images/round7/aligned pie - all.png)  |
|   **Split Sample**      | Effective sample size:  `r round(neff(filter(round7_adj, RND_05 == 0)$weight), 1)`                   |   Effective sample size: `r round(neff(filter(round7_adj, RND_05 == 1)$weight), 1)`          |
|   **Question 3:**  unaligned  |    Floating pie wedges | Framed pie wedges |
|            | ![Question 3](images/round7/unaligned pie floating - sample 1.png)                    |            ![Question 3](images/round7/unaligned pie framed - sample 2.png)          |
|  **Question 4:**  aligned     |     ![Question 4](images/round7/aligned pie floating - sample 1.png)                     |               ![Question 4](images/round7/aligned pie framed - sample 2.png)                    |

: Setup of questions and stimuli for Round 7. {#tbl-tasks-7 tbl-colwidths="\[20,40,40\]"}



For a comparison of wedges marked X and Y, there were three possible options for the response: (1) X is bigger, (2) Y is bigger or (3) They are the same. 



```{r eval=TRUE}
widths <- c(28, 16, 31, 38)
heights <- c(206, 209, 206, 203)

tan_est <- (pi-atan(widths/(heights-105)))/pi*180
cos_est <- (pi/2+acos(widths/105))/pi*180
alpha_est <- round((tan_est+cos_est)/2,1)

values <- c(46, 47.6, 45.2,  44.4)
names(values) <- LETTERS[1:4]
```


```{r survey-round7}
response_type <- round7_adj %>%
  pivot_longer(
    matches("NOLA[1-6]_[1-6]"), 
    names_to = "taskquestion", values_to = "responses") 

response_type <- response_type %>% filter(!is.na(responses), responses!="SKIPPED ON WEB")

response_type <- response_type %>% separate(col="taskquestion", into=c("task", "question"), sep="_")  

response_type <- response_type %>% mutate(
  question = parse_number(question),
  labelled = c("AB", "AC", "AD", "BD", "BC", "CD")[question]
)

# D < C < A < B.
response_type <- response_type %>% mutate(
  correct = ifelse(labelled %in% c("AB", "BC", "BD"), responses == "B is bigger", 
              ifelse(labelled %in% c("AC", "AD"), responses == "A is bigger",
                     responses == "C is bigger")),
  correct3 = correct | responses == "They are the same"
)


  
response_type <- response_type %>% 
  mutate(
    responses = factor(responses, levels = c("B is bigger", "A is bigger", "C is bigger", "D is bigger", "They are the same")),
    taskname = c("Pie", "Pie", "Float", "Float", "Frame", "Frame")[parse_number(gsub("NOLA_","",task))],
    taskname = factor(taskname, levels=c("Frame", "Float", "Pie")),
    concept = c("Un-", "A-", "Un-", "A-", "Un-", "A-")[parse_number(gsub("NOLA_","",task))],
    signal = ifelse(labelled %in% "BD", 4, 
              ifelse(labelled %in% "BC", 3, 
                ifelse(labelled %in% c("AB", "AD"), 2, 1)))
  )

survey_7 <- svydesign(~CaseId, data = response_type, weights = ~weight)
```


```{r fig-pies, eval=TRUE, fig.width = 10, fig.height = 5, fig.cap="Panelist responses by task and comparison. On average, the accuracy for aligned pieces is higher than for unaligned pieces. Similarly, the accuracy for framed pieces is higher than for floating pieces, which in turn has a higher accuracy than the full pie chart.", out.width="100%", warning =FALSE}
# D < C < A < B.
#   1   1.  2

correct_ci <- as.data.frame(svyby(~correct, by=~interaction(labelled, concept,taskname), svymean, design = survey_7))
correct_ci <- correct_ci %>% 
  mutate(
    labels = row.names(correct_ci),
  ) 
correct_ci <- correct_ci %>% separate_wider_delim(
    labels, delim = ".", names =c("labelled", "concept", "taskname")
  ) %>% mutate(
    taskname = factor(taskname, levels=c("Frame", "Float", "Pie")),
    labelled = factor(labelled, levels =c("BD", "BC", "AB", "AD", "AC", "CD")),
    ct = interaction(concept, taskname)
  )

correct3_ci <- as.data.frame(svyby(~correct3, by=~interaction(labelled, concept,taskname), svymean, design = survey_7))
correct3_ci <- correct3_ci %>% 
  mutate(
    labels = row.names(correct3_ci),
  ) 
correct3_ci <- correct3_ci %>% separate_wider_delim(
    labels, delim = ".", names =c("labelled", "concept", "taskname")
  ) %>% mutate(
    taskname = factor(taskname, levels=c("Frame", "Float", "Pie")),
    labelled = factor(labelled, levels =c("BD", "BC", "AB", "AD", "AC", "CD")),
    ct = interaction(concept, taskname)
  )



gg2b <- response_type %>% 
  mutate(
    labelled = factor(labelled, levels =c("BD", "BC", "AB", "AD", "AC", "CD")),
    ct = interaction(concept, taskname)
  ) %>% 
  ggplot(aes(x = ct, fill=factor(as.numeric(correct)+as.numeric(correct3)), weight = weight)) +
  geom_bar(position="fill") +
  facet_grid(~labelled, space = "free") +
  theme_bw() +
  ggtitle("Which piece is bigger?") +
  scale_fill_manual(values=rev(c("steelblue", "grey", "orange"))) + 
  scale_x_discrete(labels=c("Frame\nA", "\nN", "Float\nA", "\nU", "Pie\nA", "\nU")) +
  ylab("") +
  geom_rect(
    aes(
      xmin = as.numeric(ct) - 0.45,
      ymin = (correctTRUE - 1.96* se.correctTRUE),
      xmax = as.numeric(ct) + 0.45,
      ymax = (correctTRUE + 1.96* se.correctTRUE),
      weight = 1), 
    fill = "black", alpha = 0.25, #colour = "white", linewidth = 0.25,
    data = correct_ci
  ) +
  geom_rect(
    aes(
      xmin = as.numeric(ct) - 0.45,
      ymin = (correct3TRUE - 1.96* se.correct3TRUE),
      xmax = as.numeric(ct) + 0.45,
      ymax = (correct3TRUE + 1.96* se.correct3TRUE),
      weight = 1), 
    fill = "black", alpha = 0.25, #colour = "white", linewidth = 0.25,
    data = correct3_ci
  ) +
  geom_hline(yintercept=c(.25,.50,.75), colour = "grey50", linewidth=0.35) +
  geom_hline(yintercept=c(.25,.50,.75), colour = "grey90", linewidth=0.35,
             linetype="dashed") +
  xlab("") +
  theme(legend.position = "none") 
  
gg2b
```

@fig-pies shows a summary of panelists' accuracy of their responses. The panels are sorted according to the level of difficulty of a comparison as hypothesized above from lowest difficulty (BD) to highest (CD). We see that the percentage of correct responses follows (on average) this hypothesized order. Using the comparison of A versus B as the baseline, we see from @tbl-model7, that comparisons BC and BD have a significantly higher accuracy (in that order), while the accuracy of comparisons AD, AC, and CD are significantly worse (HH: should we do pairwise comparisons for that?).
Comparisons involving aligned pieces have (with two exceptions) a higher rate of accuracy than a  comparison of the same-size, unaliged pieces.  
Generally, framed pieces have the highest rate of accuracy, while the comparisons within the full context of the pie chart fared the worst, both in the percent of 'the same' responses as well as in identifying the smaller piece as the bigger.

Using a frame as an additional visual cue was shown by @clevelandGraphicalPerceptionTheory1984 to significantly improve the accuracy of perceiving the height of bars (cf. Fig. 12, p. 538; difference between 'length' and 'position along an unaligned identical axis'). Here, we find that framed pieces show the same trend in accuracy, however, this effect fails to become significant in the model (tasknameFloat).

The fact that the full pie chart sees the lowest accuracy rates is similar to the finding in @talbotFourExperimentsPerception2014 for bars. @talbotFourExperimentsPerception2014 report that other bars serve as distractors when comparing two other bars.

```{r}
#| label: tbl-model7
#| tbl-cap: Estimates for the cell means model
logit_7 <- svyglm(correct ~ labelled + concept + taskname, family = quasibinomial(), design = survey_7)
coefs_7 <- broom::tidy(logit_7)

#logit_7b <- svyglm(correct ~ factor(signal) + concept + taskname, family = quasibinomial(), design = survey_7)
#logit_7c <- svyglm(correct ~ signal + concept + taskname, family = quasibinomial(), design = survey_7)

coefs_7 %>% 
  mutate(
    term = ifelse(term=="(Intercept)", "Intercept", term),
    p.value = ifelse(p.value<0.0001, "< 0.0001", sprintf("%.4f", p.value))) %>%
  gt() %>%
  tab_header("Estimates for Model 2",
             " accuracy of responses across conditions of Round 7") %>%
  cols_label(
    std.error = "SE",
    statistic = "t-statistic",
    p.value = "p-value"
  ) %>%
  fmt_number(
    columns = c(estimate, std.error),
    decimals = 2
  ) %>%
  fmt_number(
    columns = statistic,
    decimals = 1
  ) %>%
  cols_align(
    columns = p.value,
    align = "right"
  )
```


## References{-}